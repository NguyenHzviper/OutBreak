{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import pickle\n",
    "import torch\n",
    "from xgboost import XGBRegressor\n",
    "from darts import TimeSeries\n",
    "from darts.models import  RandomForest, LinearRegressionModel,  \\\n",
    "                        LightGBMModel, CatBoostModel, XGBModel,  \\\n",
    "                        BlockRNNModel, NBEATSModel, NHiTSModel, \\\n",
    "                        TCNModel, TFTModel\n",
    "\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import PoissonRegressor\n",
    "\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from darts.utils.likelihood_models import GaussianLikelihood\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "prj_path = '../'\n",
    "data_path = prj_path + \"data/new_data/DH/squeezed/\"\n",
    "prj_path_opt= prj_path + \"optimize_hyperparam/opt_results/\"\n",
    "output_process = prj_path + \"data/new_data/DH/processed_data/\"\n",
    "output_featureselection = prj_path + \"data/new_data/DH/feature_selection/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cities = [\n",
    "        'An Giang', 'BR Vũng Tàu', 'Bình Phước', 'Bình Thuận', 'Bình Định',\n",
    "        'Bạc Liêu', 'Bắc Kạn', 'Bắc Giang', 'Cao Bằng', 'Cà Mau',\n",
    "        'Cần Thơ', 'Gia Lai', 'Hà Giang', 'Hà Nội', 'Hà Tĩnh',\n",
    "        'Hòa Bình','Hưng Yên', 'Hải Dương', 'Hải Phòng', 'Khánh Hòa', 'Kiên Giang',\n",
    "        'Kon Tum', 'Lai Châu', 'Long An', 'Lào Cai', 'Lâm Đồng',\n",
    "        'Lạng Sơn','Nam Định', 'Nghệ An', 'Ninh Bình', 'Ninh Thuận',\n",
    "        'Phú Thọ', 'Phú Yên', 'Quảng Bình', 'Quảng Nam', 'Quảng Ngãi',\n",
    "        'Quảng Ninh', 'Quảng Trị', 'Sóc Trăng', 'Sơn La', 'TT Huế',\n",
    "        'Thanh Hóa', 'Thái Bình', 'Thái Nguyên', 'Tiền Giang', 'Trà Vinh',\n",
    "        'Tuyên Quang', 'Tây Ninh', 'Vĩnh Phúc', 'Yên Bái', 'Điện Biên',\n",
    "        'Đà Nẵng', 'Đắk Nông', 'Đắk Lắk', 'Đồng Tháp'\n",
    "]\n",
    "# cities = ['Hà Nội','Hải Phòng','Quảng Ninh','Nam Định','Thái Bình','Quảng Nam','Quảng Ngãi', 'Phú Yên',\n",
    "#           'Ninh Thuận', 'Bình Thuận', 'Tây Ninh', 'Bình Phước', 'An Giang', 'Tiền Giang','Cần Thơ', 'Trà Vinh']\n",
    "\n",
    "cities = [ 'Bình Phước', 'An Giang','Quảng Ninh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters as args using the Configuration class\n",
    "class Configuration():\n",
    "    def __init__(self):\n",
    "        # lấy bộ test dài 36 tháng = 3 năm\n",
    "        self.test_size = 36\n",
    "        # là nhìn vào dữ liệu trước 3 tháng và dự phóng        \n",
    "        self.look_back = 3\n",
    "        # dự phóng n-step trong 6 tháng\n",
    "        self.n_predicted_period_months = 6\n",
    "        self.n_features = 3\n",
    "        self.seed = 42\n",
    "        # mỗi phần tử x trong tập suppervise có độ lớn là 16 = 16 tháng\n",
    "        self.batch_size = 16\n",
    "        self.device = torch.device(\"cuda\")\n",
    "        self.epochs = 300\n",
    "        #others\n",
    "        self.labels = \"Dengue_fever_rates\"\n",
    "        # Input param for Optimize Run\n",
    "        self.ntry = 1\n",
    "        self.njob = 1\n",
    "\n",
    "args = Configuration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    import random\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    \n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "seed_everything(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supporting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def send_to_telegram(message):\n",
    "\n",
    "    apiToken = '5908735099:AAGVSLrW62aXPBP-GrMvxoVgMsuJxXJpP1Q'\n",
    "    chatID = '@ptn_announcement'\n",
    "    apiURL = f'https://api.telegram.org/bot{apiToken}/sendMessage'\n",
    "\n",
    "    try:\n",
    "        response = requests.post(apiURL, json={'chat_id': chatID, 'text': message})\n",
    "        print(response.text)\n",
    "    except Exception as e:\n",
    "        message_error = \"Bị lỗi rùi: \"+str(e)\n",
    "        response = requests.post(apiURL, json={'chat_id': chatID, 'text': message_error})\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict_all_city_data():\n",
    "  cities_data = {}  \n",
    "  for city in cities:\n",
    "    city_result = pd.read_excel(prj_path+'data/new_data/DH/squeezed/squeezed_'+city+'.xlsx')  \n",
    "    \"\"\"Get all data from all city in 1997 - 2016\"\"\" \n",
    "    city_result = city_result.loc[city_result['year_month'] < '2017-1-1'] \n",
    "    cities_data[city] = city_result\n",
    "  return cities_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data (pre-)processing functions\n",
    "# modification\n",
    "def get_city_data(city_name, dict_full_data):\n",
    "    \"\"\"Returns Diarrhoea rate and climate data\"\"\" \n",
    "    city_data = dict_full_data[city_name].drop(columns=['Diarrhoea_cases','Diarrhoea_rates', 'province',\n",
    "                                                        'Influenza_rates','Influenza_cases',\n",
    "                                                        'Dengue_fever_cases', 'year', 'month'], \n",
    "                                                                  axis=1, \n",
    "                                                                  inplace=False)    \n",
    "    return city_data\n",
    "\n",
    "def convert_to_stationary(city_data):\n",
    "    \"\"\"Subtracts previous value for all cols except disease rates\"\"\"\n",
    "    for col_name in city_data.columns:\n",
    "        if col_name != 'Diarrhoea_rates':\n",
    "            try:\n",
    "                city_data[col_name] = city_data[col_name] - city_data[col_name].shift()\n",
    "            except:\n",
    "                print(col_name)\n",
    "    return city_data\n",
    "\n",
    "def impute_missing_value(city_data):\n",
    "    \"\"\"\n",
    "    Imputes 0 for first 12 months, \n",
    "    last year's value for months 12-24, \n",
    "    and minimum value of last two years for months 25+\n",
    "    \"\"\"\n",
    "    for col in city_data.columns:\n",
    "        for index in range(len(city_data[col])):\n",
    "            if np.isnan(city_data[col].iloc[index]):\n",
    "                if index < 12:\n",
    "                    city_data[col].iloc[index] = 0\n",
    "                elif index >= 12 and index <= 24:\n",
    "                    city_data[col].iloc[index] = city_data[col].iloc[index - 12]\n",
    "                else:\n",
    "                    city_data[col].iloc[index] = min(city_data[col].iloc[index - 12], city_data[col].iloc[index - 24])\n",
    "    return city_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_full_data(dict_full_data):\n",
    "    climate_and_disease_feats = ['Total_Evaporation',\n",
    "       'Total_Rainfall', 'Max_Daily_Rainfall', 'n_raining_days',\n",
    "       'Average_temperature', 'Max_Average_Temperature',\n",
    "       'Min_Average_Temperature', 'Max_Absolute_Temperature',\n",
    "       'Min_Absolute_Temperature', 'Average_Humidity', 'Min_Humidity',\n",
    "       'n_hours_sunshine', 'Dengue_fever_rates']\n",
    "    for city in cities:\n",
    "        city_data = get_city_data(city_name=city,dict_full_data = dict_full_data)\n",
    "        city_data_features = city_data[climate_and_disease_feats]\n",
    "        city_data_features = impute_missing_value(city_data_features)\n",
    "        city_data_features = convert_to_stationary(city_data_features)\n",
    "        city_data_features.dropna(inplace=True)\n",
    "        city_data_features.loc[:, \"year_month\"] = city_data[\"year_month\"]\n",
    "        dict_full_data[city] = city_data_features\n",
    "    return dict_full_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_supervised(data,  d_out, d_in, features_list=[]):\n",
    "    \"\"\"\n",
    "    Frames time-series as supervised learning dataset.\n",
    "    \n",
    "    Args:\n",
    "      d_in: lookback window\n",
    "      d_out: number of predicted months\n",
    "      features_list: list of all features **where last col is the disease incidence**\n",
    "\n",
    "    Returns:\n",
    "      Numpy arrays of disease incidence (y) and other predictors (X)\n",
    "    \"\"\"\n",
    "    X, y = list(), list()\n",
    "    for index, _ in enumerate(data):\n",
    "        in_end = index + d_in\n",
    "        out_end = in_end + d_out\n",
    "        if out_end <= len(data):\n",
    "            if len(features_list) == 0 :\n",
    "                X.append(data[index: in_end, :-1])\n",
    "            else:\n",
    "                X.append(data[index: in_end, features_list])\n",
    "            y.append(data[out_end-1: out_end, -1])\n",
    "    return np.array(X), np.array(y).reshape(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(df_train, df_eval, model, feature_list , labels, scaler, is_dl_algo,is_sklearn_model,nstep):\n",
    "  \"\"\"\n",
    "  $df: pandas.DataFrame object containing data for training and testing model:\n",
    "  $model: darts model object\n",
    "  $feature_list: Names of the features used as model input\n",
    "  $label: the value the model will be trained to predict\n",
    "  $scaler: scaler object. Note: the scaler will be fitted on training data and applied to test data\n",
    "  $lags: how much to look back into the past to output prediction\n",
    "  $split_index: the point at which to divide train and test_data\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  if is_dl_algo == 1:\n",
    "    print(\"🍋\")\n",
    "  else:\n",
    "      if is_sklearn_model == 1:\n",
    "        train = df_train[feature_list+[args.labels]].iloc[:,:].to_numpy()\n",
    "        df_eval = df_eval[-args.test_size - args.look_back-(nstep - 1): ] #Fit size of the test by step\n",
    "        test = df_eval[feature_list+[args.labels]].iloc[:,:].to_numpy()\n",
    "        x_train,y_train = to_supervised(train, d_out=nstep, d_in=args.look_back )\n",
    "        x_train = x_train.reshape(len(x_train),x_train.shape[1]*x_train.shape[2])\n",
    "        x_test,y_test = to_supervised(test, d_out=nstep, d_in=args.look_back )\n",
    "        x_test = x_test.reshape(len(x_test),x_test.shape[1]*x_test.shape[2])\n",
    "\n",
    "        model = model.fit(x_train,y_train)\n",
    "        prediction = model.predict(x_test)\n",
    "        \n",
    "        print(\"🐹Len(prediction):\",len(prediction))\n",
    "        df_eval = df_eval[-args.test_size:]\n",
    "        y_true = df_eval[labels].values\n",
    "\n",
    "        df_eval_pred_inverse = df_eval[-args.test_size:]\n",
    "        df_eval_pred_inverse[labels]= prediction\n",
    "        y_pred = scaler.inverse_transform(df_eval_pred_inverse.iloc[:,:-1])[:,[-1]].reshape(len(prediction))  \n",
    "      else:\n",
    "        x_train = TimeSeries.from_dataframe(df_train, \"year_month\", feature_list)\n",
    "        y_train = TimeSeries.from_dataframe(df_train, \"year_month\", labels)\n",
    "\n",
    "        df_eval = df_eval[-args.test_size - args.look_back-(nstep - 1): ]\n",
    "\n",
    "        x_test = TimeSeries.from_dataframe(df_eval, \"year_month\", feature_list)\n",
    "        y_test = TimeSeries.from_dataframe(df_eval, \"year_month\", labels)\n",
    "\n",
    "        model.fit(y_train, past_covariates = x_train)\n",
    "\n",
    "        prediction = model.predict(args.test_size, past_covariates = x_test, num_samples=1)\n",
    "\n",
    "        df_eval = df_eval[-args.test_size:]\n",
    "        y_true = df_eval[labels].values\n",
    "        df_eval[labels]= np.array(prediction._xa).squeeze()\n",
    "        y_pred = scaler.inverse_transform(df_eval.iloc[:,:-1])[:,[-1]].reshape(len(prediction))\n",
    "\n",
    "\n",
    "  mse = mean_squared_error(y_true, y_pred)\n",
    "  mae = mean_absolute_error(y_true, y_pred)\n",
    "  rmse = mse**0.5\n",
    "  mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "  print(f\"mean_squared_error: {mse:.4f}\")\n",
    "  print(f\"rmse: {rmse}\")\n",
    "  print(f\"mape: {mape}\")\n",
    "  return model, y_true, y_pred, mse, mae, rmse, mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_prediction_for_location(df_train, df_eval, model, location, feature_list, \n",
    "                                                labels, scaler, is_dl_algo, is_sklearn_model ,nstep, model_name):\n",
    "    \"\"\"train and generate prediction for a province\n",
    "    df: DataFrame object containing features and label(s) for training model\n",
    "    localtion: location_name\n",
    "    feature_list: list of features used as model input,  must be among the column names of df\n",
    "    labels: the values model will be trained to predict\n",
    "    scaler: sklearn scaler object\n",
    "    lags: how long into the past to look back when making prediction\n",
    "    split_index: the point at which to divide data into the train and test subsets.\n",
    "    \"\"\"\n",
    "    model, y_true, prediction_inverse, mse, mae, rmse, mape = train_and_evaluate(df_train, df_eval, model, feature_list, labels, scaler,is_dl_algo, is_sklearn_model,nstep)\n",
    "    df_prediction = pd.DataFrame({\"Date\": df_eval[\"year_month\"][-len(prediction_inverse):],\n",
    "                                  \"Observed\": y_true[-len(prediction_inverse):],\n",
    "                                  f\"{nstep}-month\": prediction_inverse})\n",
    "    \n",
    "\n",
    "    df_prediction[\"City\"] = location\n",
    "    df_prediction[f\"RMSE_{nstep}-month\"] = rmse\n",
    "    df_prediction[f\"MAE_{nstep}-month\"] = mae\n",
    "    df_prediction[f\"MAPE_{nstep}-month\"] = mape\n",
    "    df_prediction[f\"MSE_{nstep}-month\"] = mse\n",
    "\n",
    "    df_compare_test_predict = pd.DataFrame({'y_true':y_true, 'y_pred':prediction_inverse})\n",
    "    df_compare_test_predict.plot()\n",
    "    plt.legend()\n",
    "    plt.title(f\"{nstep}step_{type(model).__name__}_DF_{location}\")\n",
    "    plt.savefig(f\"./predict_results/{model_name}/picture/{nstep}step_{type(model).__name__}_DF_{location}_tkde.png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    temp_rs = f\"{rmse},{mae},{mape},{mse}\"\n",
    "    print(\"rmse,mae,mape,mse\")\n",
    "    print(temp_rs)\n",
    "    \n",
    "    return df_prediction, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataWithSelectedFeature(city, next_predicted_month):\n",
    "  selected_feature = []\n",
    "  df = pd.read_csv(output_featureselection+str(next_predicted_month)+\"step_feature_selection_3_most.csv\")\n",
    "  for row in range(len(df)):\n",
    "    if (df[\"City\"][row] == city):\n",
    "      selected_feature.append(df[\"1st_Feature\"][row])\n",
    "      selected_feature.append(df[\"2nd_Feature\"][row])\n",
    "      selected_feature.append(df[\"3rd_Feature\"][row])\n",
    "  return selected_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHyperParams(model_name, city, nstep):\n",
    "  folder_path = f'../optimize_hyperparam/opt_results/opt_res_ml_26102023/{model_name}/'\n",
    "  file_path = folder_path+ f'261023_DF_opt_hyperparam_{model_name}_{nstep}-nstep.xlsx'\n",
    "  df_optimized = pd.read_excel(file_path)\n",
    "  # display(df_optimized)\n",
    "  df_optimized_params = df_optimized.loc[(df_optimized['City'] == city)]\n",
    "  return df_optimized_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_list = [\n",
    "     \"RandomForest\",\n",
    "     \"LinearRegressionModel\",\n",
    "    #  \"LightGBMModel\",\n",
    "    #  \"CatBoostModel\",\n",
    "     \"XGBModel\",\n",
    "#     \"PoissonRegressor\",\n",
    "#     \"SVMRBF\"\n",
    "]\n",
    "\n",
    "for nstep in range(1,args.n_predicted_period_months+1):\n",
    "# for nstep in range(1,2):\n",
    "    print(f\"✨✨✨✨✨✨✨✨{nstep}✨✨✨✨✨✨✨✨✨✨\")\n",
    "    for model_name in model_name_list:   \n",
    "        print(f\"✨✨✨✨✨✨✨✨{model_name}✨✨✨✨✨✨✨✨✨✨\")\n",
    "        for city in cities:\n",
    "            print(f\"✨✨✨✨✨✨✨✨{city}✨✨✨✨✨✨✨✨✨✨\")\n",
    "            df_train = pd.read_csv(output_process+city+'_train_preprocessed.csv', parse_dates=True, index_col= None, encoding = 'unicode_escape')\n",
    "            df_valid = pd.read_csv(output_process+city+'_test_preprocessed.csv', parse_dates=True, index_col= None, encoding = 'unicode_escape')\n",
    "            scaler = joblib.load(output_process+city+'_train_scalerMinMaxNorm.save') #ok\n",
    "            is_sklearn_model = 0\n",
    "            selected_features = getDataWithSelectedFeature(city, nstep)\n",
    "\n",
    "            lags_by_nstep = args.look_back + nstep - 1\n",
    "            lags_past_covariates_by_nstep = [-lags_by_nstep+2,-lags_by_nstep+1,-lags_by_nstep] #Mảng này chứa ba giá trị tương ứng cho args.lookback 3\n",
    "            is_dl_algo = 0\n",
    "\n",
    "            pl_trainer_kwargs = {\n",
    "                        \"accelerator\": \"cpu\",\n",
    "                        # \"devices\": -1,\n",
    "                        # \"auto_select_gpus\": True,\n",
    "                    }\n",
    "\n",
    "            df_hyper_params = getHyperParams(model_name, city, nstep)\n",
    "\n",
    "            if model_name == \"RandomForest\":\n",
    "                    lags = df_hyper_params['lags'].values[0]\n",
    "                    lags_past_covariates = df_hyper_params['lags_past_covariates'].values[0].split(',')\n",
    "                    lags_past_covariates = [int(i) for i in lags_past_covariates]\n",
    "                    output_chunk_length = df_hyper_params['output_chunk_length'].values[0]\n",
    "                    n_estimators = df_hyper_params['n_estimators'].values[0]\n",
    "                    max_depth = df_hyper_params['max_depth'].values[0]\n",
    "                    random_state = df_hyper_params['random_state'].values[0]\n",
    "                    # Create the RandomForest model\n",
    "                    model = RandomForest(\n",
    "                                    lags = int(lags),\n",
    "                                    lags_past_covariates = lags_past_covariates_by_nstep,\n",
    "                                    output_chunk_length = int(output_chunk_length),\n",
    "                                    n_estimators = int(n_estimators),\n",
    "                                    max_depth = int(max_depth),\n",
    "                                    random_state=int(random_state))\n",
    "            elif model_name == 'XGBModel':\n",
    "                    lags = df_hyper_params['lags'].values[0]\n",
    "                    lags_past_covariates = df_hyper_params['lags_past_covariates'].values[0].split(',')\n",
    "                    lags_past_covariates = [int(i) for i in lags_past_covariates]\n",
    "                    output_chunk_length = df_hyper_params['output_chunk_length'].values[0]\n",
    "                    random_state = df_hyper_params['random_state'].values[0]\n",
    "                    likelihood = df_hyper_params['likelihood'].values[0]\n",
    "                    # Create the  model\n",
    "                    model = XGBModel(\n",
    "                                    lags = int(lags),\n",
    "                                    lags_past_covariates = lags_past_covariates,\n",
    "                                    output_chunk_length = int(output_chunk_length),\n",
    "                                    random_state=int(random_state),\n",
    "                                    likelihood = likelihood\n",
    "                                    )\n",
    "            elif model_name == 'LinearRegressionModel':\n",
    "                    lags = df_hyper_params['lags'].values[0]\n",
    "                    lags_past_covariates = df_hyper_params['lags_past_covariates'].values[0].split(',')\n",
    "                    lags_past_covariates = [int(i) for i in lags_past_covariates]\n",
    "                    output_chunk_length = df_hyper_params['output_chunk_length'].values[0]\n",
    "                    random_state = df_hyper_params['random_state'].values[0]\n",
    "                    # Create the  model\n",
    "                    model = LinearRegressionModel(\n",
    "                                    lags = int(lags),\n",
    "                                    lags_past_covariates = lags_past_covariates,\n",
    "                                    output_chunk_length = int(output_chunk_length),\n",
    "                                    random_state=int(random_state))\n",
    "            elif model_name == \"CatBoostModel\":\n",
    "                    lags = df_hyper_params['lags'].values[0]\n",
    "                    lags_past_covariates = df_hyper_params['lags_past_covariates'].values[0].split(',')\n",
    "                    lags_past_covariates = [int(i) for i in lags_past_covariates]\n",
    "                    learning_rate = df_hyper_params['learning_rate'].values[0]\n",
    "                    n_estimators = df_hyper_params['n_estimators'].values[0]\n",
    "                    max_depth = df_hyper_params['max_depth'].values[0]\n",
    "                    output_chunk_length = df_hyper_params['output_chunk_length'].values[0]\n",
    "                    likelihood = df_hyper_params['likelihood'].values[0]\n",
    "                    bagging_temperature = df_hyper_params['bagging_temperature'].values[0]\n",
    "                    border_count = df_hyper_params['border_count'].values[0]\n",
    "                    l2_leaf_reg = df_hyper_params['l2_leaf_reg'].values[0]\n",
    "                    random_strength = df_hyper_params['random_strength'].values[0]\n",
    "                    opt_quantitles = df_hyper_params['quantiles'].values[0]\n",
    "                    if opt_quantitles == \"IsNone\":\n",
    "                        quantitles = None\n",
    "                    else:\n",
    "                        quantitles = df_hyper_params['quantiles'].values[0].split(',')\n",
    "                    model = CatBoostModel(\n",
    "                                            lags=int(lags),\n",
    "                                            lags_past_covariates=lags_past_covariates, \n",
    "                                            learning_rate=float(learning_rate),\n",
    "                                            n_estimators= int(n_estimators),\n",
    "                                            max_depth=int(max_depth), \n",
    "                                            output_chunk_length = int(output_chunk_length),\n",
    "                                            likelihood = likelihood,\n",
    "                                            # quantiles = float(quantiles), #None or float\n",
    "                                            bagging_temperature = float(bagging_temperature),\n",
    "                                            # border_count = int(border_count),\n",
    "                                            l2_leaf_reg = float(l2_leaf_reg),\n",
    "                                            random_strength = float(random_strength),\n",
    "                                            random_state=int(random_state))\n",
    "            elif model_name == \"LightGBMModel\":\n",
    "                    lags = df_hyper_params['lags'].values[0]\n",
    "                    lags_past_covariates = df_hyper_params['lags_past_covariates'].values[0].split(',')\n",
    "                    lags_past_covariates = [int(i) for i in lags_past_covariates]\n",
    "                    output_chunk_length = 1\n",
    "                    multi_models = df_hyper_params['multi_models'].values[0]\n",
    "                    likelihood = df_hyper_params['likelihood'].values[0]\n",
    "                    num_leaves = df_hyper_params['num_leaves'].values[0]\n",
    "                    learning_rate = df_hyper_params['learning_rate'].values[0]\n",
    "                    feature_fraction = df_hyper_params['feature_fraction'].values[0]\n",
    "                    min_child_samples = df_hyper_params['min_child_samples'].values[0]\n",
    "                    lambda_l1 = df_hyper_params['lambda_l1'].values[0]\n",
    "                    model = LightGBMModel(\n",
    "                        lags =int(lags),\n",
    "                        lags_past_covariates = lags_past_covariates,\n",
    "                        output_chunk_length = int(output_chunk_length),\n",
    "                        multi_models = bool(multi_models),\n",
    "                        likelihood = likelihood,\n",
    "                        num_leaves = int(num_leaves),\n",
    "                        learning_rate = float(learning_rate),\n",
    "                        feature_fraction = float(feature_fraction),\n",
    "                        min_child_samples = int(min_child_samples),\n",
    "                        lambda_l1 = float(lambda_l1),\n",
    "                    )\n",
    "            elif model_name == 'SVMRBF':\n",
    "                    max_iter = df_hyper_params['max_iter'].values[0]\n",
    "                    epsilon = df_hyper_params['epsilon'].values[0],\n",
    "                    model = SVR(\n",
    "                        max_iter = max_iter,\n",
    "                        kernel = 'rbf',\n",
    "                        epsilon = epsilon[0]\n",
    "                    )\n",
    "                    is_sklearn_model = 1\n",
    "            elif model_name == \"PoissonRegressor\":\n",
    "                    max_iter = df_hyper_params['max_iter'].values[0]\n",
    "                    alpha = df_hyper_params['alpha'].values[0],\n",
    "                    model = PoissonRegressor(\n",
    "                        max_iter = max_iter,\n",
    "                        fit_intercept = False,\n",
    "                        alpha = alpha[0]\n",
    "                    )\n",
    "                    is_sklearn_model = 1        \n",
    "            df, model = output_prediction_for_location(df_train, df_valid, model, location=city, feature_list=selected_features,\n",
    "                                                            labels=args.labels, scaler=scaler, is_dl_algo = is_dl_algo,is_sklearn_model = is_sklearn_model, nstep = nstep,model_name = model_name)\n",
    "            df.to_excel(f\"./predict_results/{model_name}/0_train_{nstep}nstep_denguefever_prediction_results_by_{model_name}_in_{city}.xlsx\")\n",
    "            pickle.dump(model, open(f\"./trained_models/{model_name}/{nstep}nstep_denguefever_{model_name}_in_{city}.sav\", 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
