{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xEHFfG50NdzF"
   },
   "source": [
    "# Matrix of metrics of Cross validation  for 5 top Simple Combination \n",
    "Date: 09/12/2022\n",
    "Description: Create cross validation metrics\n",
    "Use 5 combinations only regarding to the ranking from simple combination result.\n",
    "This file creates cross validation metrics for next steps\n",
    "Modificaiton 02/01/2023\n",
    "Sử dụng 5 combination mới"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Ja18nXEglgwF"
   },
   "outputs": [],
   "source": [
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6592,
     "status": "ok",
     "timestamp": 1666073902985,
     "user": {
      "displayName": "Duy Thanh Do",
      "userId": "13373927866316348326"
     },
     "user_tz": -420
    },
    "id": "VW4v2IEcTYHa",
    "outputId": "b4573735-a6ca-4cc7-80c8-b23f3b339c93"
   },
   "outputs": [],
   "source": [
    "# Install packages\n",
    "# !pip install -U scikit-learn\n",
    "# !pip install ftfy\n",
    "# !pip install xlsxwriter \n",
    "# !pip install xlwt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "b8I8S9koHjT_"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import tensorflow as tf \n",
    "# from google.colab import drive\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.models import Sequential, model_from_json\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "import traceback\n",
    "import sys\n",
    "import copy\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# lib for LSTM of Meta model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from ftfy import fix_text\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [7, 3]\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NURg_hMGdv_z"
   },
   "source": [
    "# Global param and project path\n",
    "All mix data 1997-2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3798,
     "status": "ok",
     "timestamp": 1666073906773,
     "user": {
      "displayName": "Duy Thanh Do",
      "userId": "13373927866316348326"
     },
     "user_tz": -420
    },
    "id": "MNAuUk6_HVs1",
    "outputId": "e48d21e8-8b36-46c6-9a85-0359f712e8df"
   },
   "outputs": [],
   "source": [
    "# Attach Google Drive for reading and saving files\n",
    "# drive.mount('/content/drive')\n",
    "prj_path = '/home/mlworker/Quang/HealthCare/Source_14012023_v4/'\n",
    "\n",
    "# Lấy từ 1997 tới 01/01/2018. Bỏ 12 tháng cuối rồi\n",
    "path_all_mix_data =             prj_path+ 'results/all_mix_data/all_mix_data_1997_2018_mae/'\n",
    "path_all_mix_data_cnn =         prj_path+ 'results/all_mix_data/all_mix_data_1997_2018_mae/cnn/'\n",
    "path_all_mix_data_lstm =        prj_path+ 'results/all_mix_data/all_mix_data_1997_2018_mae/lstm/'\n",
    "path_all_mix_data_lstm_att =    prj_path+ 'results/all_mix_data/all_mix_data_1997_2018_mae/lstm_att/'\n",
    "path_all_mix_data_tf =          prj_path+ 'results/all_mix_data/all_mix_data_1997_2018_mae/tf/'\n",
    "\n",
    "prj_path_opt= prj_path+ 'optimize_hyperparam/opt_results/opt_results_1997_2018_mae/'\n",
    "\n",
    "\n",
    "prj_path_result_ensemble =      prj_path+ 'results/ensemble_17022023/'\n",
    "com_path_validation_metrics =   prj_path_result_ensemble+ 'result/weighted_combination/cv_metrics/'\n",
    "\n",
    "# com_eval_path_validation_metrics = 'H:/Store/Research/UIT/PhD Research/Dev Environment/Tropical Diseases/running/eval_result/21_12_2022_weighted_combination-5-cities/'\n",
    "\n",
    "\n",
    "# loockback_window for meta model\n",
    "meta_lookback_window = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xây dựng Meta Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-33edc67b4c3a>:44: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result_test = result_test.append(selected_result['Test Prediction'].tail(36))\n",
      "<ipython-input-5-33edc67b4c3a>:59: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result_test = result_test.append(selected_result['Test Prediction'].tail(36))\n",
      "<ipython-input-5-33edc67b4c3a>:78: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result_test = result_test.append(selected_result['Test Prediction'].tail(36))\n",
      "<ipython-input-5-33edc67b4c3a>:101: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result_test = result_test.append(selected_result['Test Prediction'].tail(36)) # Lấy 36 tháng cuối\n",
      "<ipython-input-5-33edc67b4c3a>:128: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = meta_train_set.append(meta_test_set[3:])\n",
      "<ipython-input-5-33edc67b4c3a>:44: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result_test = result_test.append(selected_result['Test Prediction'].tail(36))\n",
      "<ipython-input-5-33edc67b4c3a>:59: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result_test = result_test.append(selected_result['Test Prediction'].tail(36))\n",
      "<ipython-input-5-33edc67b4c3a>:78: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result_test = result_test.append(selected_result['Test Prediction'].tail(36))\n",
      "<ipython-input-5-33edc67b4c3a>:101: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result_test = result_test.append(selected_result['Test Prediction'].tail(36)) # Lấy 36 tháng cuối\n",
      "<ipython-input-5-33edc67b4c3a>:128: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = meta_train_set.append(meta_test_set[3:])\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "# Xây Dựng Meta Data Set\n",
    "meta_test_set_city = {}\n",
    "meta_train_set_city = {}\n",
    "meta_data_set_city = {}\n",
    "cities = [\n",
    "    'An Giang', 'BR Vũng Tàu', 'Bình Phước', 'Bình Thuận', 'Bình Định',\n",
    "        'Bạc Liêu', 'Bắc Kạn', 'Bắc Giang', \n",
    "#     'Cao Bằng', 'Cà Mau', \n",
    "    \n",
    "#         'Cần Thơ', 'Gia Lai', 'Hà Giang', 'Hà Nội', 'Hà Tĩnh', 'Hòa Bình',\n",
    "#         'Hưng Yên', 'Hải Dương', 'Hải Phòng', \n",
    "#     'Khánh Hòa', 'Kiên Giang',\n",
    "    \n",
    "#         'Kon Tum', 'Lai Châu', 'Long An', 'Lào Cai', 'Lâm Đồng',\n",
    "#         'Lạng Sơn', 'Nam Định', 'Nghệ An', \n",
    "#     'Ninh Bình', 'Ninh Thuận',\n",
    "    \n",
    "#         'Phú Thọ', 'Phú Yên', 'Quảng Bình', 'Quảng Nam', 'Quảng Ngãi',\n",
    "#         'Quảng Ninh', 'Quảng Trị', 'Sóc Trăng',\n",
    "#     'Sơn La', 'TT Huế',\n",
    "    \n",
    "#         'Thanh Hóa', 'Thái Bình', 'Thái Nguyên', 'Tiền Giang', 'Trà Vinh',\n",
    "#         'Tuyên Quang', 'Tây Ninh', 'Vĩnh Phúc', \n",
    "#     'Yên Bái', 'Điện Biên',\n",
    "    \n",
    "#          'Đà Nẵng', 'Đắk Nông', 'Đắk Lắk', 'Đồng Tháp'\n",
    "]\n",
    "cities = ['An Giang','Lạng Sơn']\n",
    "\n",
    "for city in cities:  \n",
    "  meta_test_set = pd.DataFrame()\n",
    "  meta_train_set = pd.DataFrame() \n",
    "  #Note: mỗi bộ all_mix_data sẽ có lenght = 251 (1997 -01/01/2018  và vì 1 dòng đầu bị bỏ đi do quá trình statitionary và remove NAN lúc train)\n",
    "  lenght_whole_data_city = 251\n",
    "  ######################################\n",
    "  # Xử lý data của CNN ứng với mỗi city\n",
    "  df = pd.read_excel(path_all_mix_data_cnn+'mix_train_test_data_'+city+'_cnn.xlsx')\n",
    "  selected_result = df.sort_values(by = 'rmse', ascending=True) # Sắp xếp theo rmse nhỏ nhất\n",
    "  selected_result = selected_result.head(lenght_whole_data_city) # lấy 251 phần tử đầu tiên\n",
    "  selected_result= selected_result.sort_index() \n",
    "  # Tạo meta testset\n",
    "  result_test = selected_result['Train Prediction'].iloc[-meta_lookback_window-36:-36] # Lấy buffer cho phần lookback_window\n",
    "  result_test = result_test.append(selected_result['Test Prediction'].tail(36))\n",
    "  meta_test_cnn = result_test.reset_index(drop=True)  \n",
    "  # Tạo meta trainset\n",
    "  result_train = selected_result['Train Prediction'].iloc[meta_lookback_window:-36]\n",
    "  meta_train_cnn = result_train.reset_index(drop=True)\n",
    "  \n",
    "  ######################################\n",
    "  # Xử lý data của LSTM  ứng với mỗi city\n",
    "  df = pd.read_excel(path_all_mix_data_lstm+'mix_train_test_data_'+city+'_lstm.xlsx')  \n",
    "  selected_result = df.sort_values(by = 'rmse', ascending=True) # Sắp xếp theo rmse nhỏ nhất\n",
    "  selected_result = selected_result.head(lenght_whole_data_city) # lấy 251 phần tử đầu tiên\n",
    "  selected_result= selected_result.sort_index()   \n",
    "  \n",
    "  # Tạo meta testset\n",
    "  result_test = selected_result['Train Prediction'].iloc[-meta_lookback_window-36:-36] # Lấy buffer cho phần lookback_window\n",
    "  result_test = result_test.append(selected_result['Test Prediction'].tail(36))\n",
    "  meta_test_lstm = result_test.reset_index(drop=True)\n",
    "  # Tạo meta trainset\n",
    "  result_train = selected_result['Train Prediction'].iloc[meta_lookback_window:-36]\n",
    "  meta_train_lstm = result_train.reset_index(drop=True)\n",
    "\n",
    "  ###########################################\n",
    "  # Xử lý data của LSTM ATT ứng với mỗi city\n",
    "  df = pd.read_excel(path_all_mix_data_lstm_att+'mix_train_test_data_'+city+'_lstm_att.xlsx') \n",
    "  selected_result = df.sort_values(by = 'rmse', ascending=True) # Sắp xếp theo rmse nhỏ nhất\n",
    "  selected_result = selected_result.head(lenght_whole_data_city) # lấy 251 phần tử đầu tiên\n",
    "  selected_result= selected_result.sort_index()   \n",
    "  \n",
    "  # Tạo meta testset\n",
    "  # Meta testset sẽ có lenght = 36 + lookback_window \n",
    "  meta_test_observed = selected_result['Observed'].iloc[-meta_lookback_window-36:]\n",
    "  meta_test_observed = meta_test_observed.reset_index(drop=True) \n",
    "\n",
    "  result_test = selected_result['Train Prediction'].iloc[-meta_lookback_window-36:-36] # Lấy buffer cho phần lookback_window\n",
    "  result_test = result_test.append(selected_result['Test Prediction'].tail(36))\n",
    "  meta_test_lstm_att = result_test.reset_index(drop=True)  \n",
    "  \n",
    "  # Tạo meta trainset\n",
    "  meta_train_observed = selected_result['Observed'].iloc[meta_lookback_window:-36]\n",
    "  meta_train_observed = meta_train_observed.reset_index(drop=True)\n",
    "  result_train = selected_result['Train Prediction'].iloc[meta_lookback_window:-36]\n",
    "  meta_train_lstm_att = result_train.reset_index(drop=True)\n",
    "\n",
    "\n",
    "  #############################################\n",
    "  # Xử lý data của Transformer ứng với mỗi city\n",
    "  df = pd.read_excel(path_all_mix_data_tf+'mix_train_test_data_'+city+'_tf.xlsx') \n",
    "  selected_result = df.sort_values(by = 'rmse', ascending=True) # Sắp xếp theo rmse nhỏ nhất\n",
    "  selected_result = selected_result.head(lenght_whole_data_city) # lấy 251 phần tử đầu tiên\n",
    "  selected_result= selected_result.sort_index()   \n",
    "  \n",
    "  # Tạo meta testset\n",
    "  # Meta testset sẽ có lenght = 36 + lookback_window \n",
    "  meta_test_observed = selected_result['Observed'].iloc[-meta_lookback_window-36:]\n",
    "  meta_test_observed = meta_test_observed.reset_index(drop=True) \n",
    "\n",
    "  result_test = selected_result['Train Prediction'].iloc[-meta_lookback_window-36:-36] # Lấy buffer cho phần lookback_window\n",
    "  result_test = result_test.append(selected_result['Test Prediction'].tail(36)) # Lấy 36 tháng cuối\n",
    "  meta_test_tf = result_test.reset_index(drop=True)  \n",
    "  \n",
    "  # Tạo meta trainset\n",
    "  meta_train_observed = selected_result['Observed'].iloc[meta_lookback_window:-36] # Lấy từ meta_lookback_window cho đến -36 (3 năm cuối)\n",
    "  meta_train_observed = meta_train_observed.reset_index(drop=True)\n",
    "  result_train = selected_result['Train Prediction'].iloc[meta_lookback_window:-36]\n",
    "  meta_train_tf = result_train.reset_index(drop=True)\n",
    "\n",
    "  \n",
    "  meta_test_set['cnn_pred'] = meta_test_cnn\n",
    "  meta_test_set['lstm_pred'] = meta_test_lstm\n",
    "  meta_test_set['lstm_att_pred'] = meta_test_lstm_att\n",
    "  meta_test_set['tf_pred'] = meta_test_tf\n",
    "  meta_test_set['Observed'] = meta_test_observed\n",
    "   \n",
    "  meta_train_set['cnn_pred'] = meta_train_cnn\n",
    "  meta_train_set['lstm_pred'] = meta_train_lstm\n",
    "  meta_train_set['lstm_att_pred'] = meta_train_lstm_att \n",
    "  meta_train_set['tf_pred'] = meta_train_tf  \n",
    "  meta_train_set['Observed'] = meta_train_observed\n",
    "\n",
    "  meta_test_set_city[city] = meta_test_set # test set có chiều dài 39\n",
    "  meta_train_set_city[city] = meta_train_set # train set có chiều dài 212 = 252 -36 (train) - 1 (station) - 3 (meta_lookback_window)\n",
    "\n",
    "  # Đoạn này làm thêm. Nối dài meta_test_set_city và meta_train_set_city\n",
    "  # lưu ý đoạn này sẽ tạo ra data có chiều dài = 252 - 1 (bỏ đi 1 tháng đầu do xử lý stationary) - meta_lookback_window\n",
    "  df = meta_train_set.append(meta_test_set[3:])\n",
    "  df = df.reset_index(drop=True)\n",
    "  # range date từ 1 + lookback_window tháng đầu tiên\n",
    "  df['year_month'] = pd.Series(pd.date_range(\"1997-05-01\", periods=248, freq=\"M\"))  \n",
    "  meta_data_set_city[city] = df\n",
    "  df.to_excel(prj_path_result_ensemble+'/meta_data/meta_data_'+city+'.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta_train_set_city['Lạng Sơn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta_test_set_city['Lạng Sơn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Danh sách các based Model và các Combination có thể của nó"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cnn_pred', 'lstm_att_pred'),\n",
       " ('cnn_pred', 'lstm_pred'),\n",
       " ('cnn_pred', 'tf_pred'),\n",
       " ('lstm_att_pred', 'lstm_pred'),\n",
       " ('lstm_att_pred', 'tf_pred'),\n",
       " ('lstm_pred', 'tf_pred'),\n",
       " ('cnn_pred', 'lstm_att_pred', 'lstm_pred'),\n",
       " ('cnn_pred', 'lstm_att_pred', 'tf_pred'),\n",
       " ('cnn_pred', 'lstm_pred', 'tf_pred'),\n",
       " ('lstm_att_pred', 'lstm_pred', 'tf_pred'),\n",
       " ('cnn_pred', 'lstm_att_pred', 'lstm_pred', 'tf_pred')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Danh sách các based Model và các Combination có thể của nó\n",
    "list_based_model = ['cnn_pred','lstm_att_pred','lstm_pred','tf_pred']\n",
    "\n",
    "combs_features = list(combinations(list_based_model[0:], 2)) + list(combinations(list_based_model[0:], 3)) + list(combinations(list_based_model[0:], 4))\n",
    "\n",
    "# list_based_model = ['cnn_pred','lstm_att_pred','lstm_pred','tf_pred']\n",
    "\n",
    "# combs_features = list(combinations(list_based_model[0:], 3)) \n",
    "combs_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selected 5 Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bộ sai đầu tiên\n",
    "# selected_combs = ['CNN*lstm_att_pred-tf_pred-Observed','CNN*cnn_pred-lstm_pred-Observed',\n",
    "#                   'CNN*lstm_pred-tf_pred-Observed', 'CNN*cnn_pred-lstm_att_pred-lstm_pred-Observed',\n",
    "#                   'LSTM*lstm_att_pred-lstm_pred-tf_pred-Observed'                 \n",
    "#                  ]\n",
    "# # Bộ này theo top 5 cố định RMSE\n",
    "# selected_combs = ['LSTM*cnn_pred-tf_pred-Observed',\n",
    "#                   'LSTM*lstm_att_pred-lstm_pred-Observed',     \n",
    "\n",
    "#                   'LSTM*cnn_pred-lstm_att_pred-tf_pred-Observed',\n",
    "#                   'CNN*lstm_att_pred-lstm_pred-Observed', \n",
    "#                   'LSTM*cnn_pred-lstm_pred-Observed'           \n",
    "#                  ]\n",
    "\n",
    "# Bộ này theo top 5 cố định MAE\n",
    "# selected_combs = ['LSTM*lstm_att_pred-lstm_pred-tf_pred-Observed',\n",
    "#                   'CNN*lstm_att_pred-tf_pred-Observed', \n",
    "#                   'LSTM*cnn_pred-lstm_att_pred-tf_pred-Observed',\n",
    "#                   'CNN*lstm_att_pred-lstm_pred-Observed',\n",
    "#                   'LSTM*cnn_pred-lstm_pred-Observed'\n",
    "                \n",
    "#                  ]\n",
    "# selected_combs = ['LSTM*lstm_att_pred-lstm_pred-tf_pred-Observed',                    \n",
    "#                   'CNN*lstm_att_pred-tf_pred-Observed'\n",
    "                #  ]\n",
    "selected_combs = ['LSTM*cnn_pred-lstm_att_pred-Observed',\n",
    "                  'LSTM*lstm_att_pred-lstm_pred-Observed', \n",
    "                  'CNN*cnn_pred-lstm_pred-Observed',\n",
    "                  'CNN*lstm_pred-tf_pred-Observed', \n",
    "                  'CNN*lstm_att_pred-lstm_pred-Observed',\n",
    "                  'CNN*lstm_att_pred-lstm_pred-tf_pred-Observed',\n",
    "                  'LSTM*cnn_pred-lstm_att_pred-lstm_pred-Observed',\n",
    "                  'CNN*cnn_pred-lstm_att_pred-Observed'\n",
    "                 ]\n",
    "\n",
    "def is_in_Selected_combination(comb_string = ''):\n",
    "    return any(comb_string in x for x in selected_combs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Functions and Train class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def send_to_telegram(message):\n",
    "\n",
    "    apiToken = '5908735099:AAGVSLrW62aXPBP-GrMvxoVgMsuJxXJpP1Q'\n",
    "    chatID = '@ptn_announcement'\n",
    "    apiURL = f'https://api.telegram.org/bot{apiToken}/sendMessage'\n",
    "\n",
    "    try:\n",
    "        response = requests.post(apiURL, json={'chat_id': chatID, 'text': message})\n",
    "        print(response.text)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "\n",
    "def seed_everything(seed: int):\n",
    "    import random, os    \n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "# Set hyperparameters as args using the Configuration class\n",
    "class Configuration():\n",
    "    def __init__(self):\n",
    "      # lấy bộ test dài 36 tháng = 3 năm\n",
    "        self.test_size = 36\n",
    "        # là nhìn vào dữ liệu trước 3 tháng và dự phóng        \n",
    "        self.look_back = 3\n",
    "        # dự phóng 3 tháng\n",
    "        self.n_predicted_month = 3\n",
    "        self.n_features = 3\n",
    "        self.seed = 42\n",
    "        # mỗi phần tử x trong tập suppervise có độ lớn là 16 = 16 tháng\n",
    "        self.batch_size = 16\n",
    "        # self.device = torch.device(\"cuda\")\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.epochs = 300\n",
    "     \n",
    "\n",
    "# Biến args lưu giá trị Global.\n",
    "args = Configuration()\n",
    "\n",
    "\n",
    "# Lưu ý sau khi stationnary thì dòng đầu tiên trừ cho dòng trước nữa là ko có cho nên về cơ bản sẽ thành NaN\n",
    "# Cần remove dòng đầu tiên ra khỏi data để train và test. Cho nên cơ bản là data sau khi in ra sẽ thiếu 1 đơn vị đầu tiên\n",
    "def convert_to_stationary(city_data):\n",
    "    \"\"\"Subtracts previous value for all cols except disease rates\"\"\"\n",
    "    for col_name in city_data.columns:\n",
    "        if col_name != 'Diarrhoea_rates':\n",
    "            try:\n",
    "                city_data[col_name] = city_data[col_name] - city_data[col_name].shift()\n",
    "            except:\n",
    "                print(col_name)\n",
    "    return city_data\n",
    "\n",
    "def impute_missing_value(city_data):\n",
    "    \"\"\"\n",
    "    Cơ bản dữ liệu bị thiếu sót rất nhiều: Như Điện Biên 1997 -2003 là thiếu dữ liệu về bệnh\n",
    "    Hàm này sẽ tự sinh ra dữ liệu bị thiếu. Nếu tháng nào không có số liệu thì tính như sau:\n",
    "    12 tháng đầu không có số liệu thì gán = 0\n",
    "    tháng 13-24 không có số liệu, sẽ lấy giá trị của tháng cùng kỳ năm trước\n",
    "    tháng từ 24 trở đi sẽ lấy giá trị nhỏ nhất của 2 tháng cùng kỳ trong 2 năm gần nhất.\n",
    "    Do Điện Biên bằng 0 nên sau khi xử lý từ 1997 -2003 là đều = 0.  \n",
    "    \"\"\"\n",
    "    for col in city_data.columns:\n",
    "        for index in range(len(city_data[col])):\n",
    "            if np.isnan(city_data[col].iloc[index]):\n",
    "                if index < 12:\n",
    "                    city_data[col].iloc[index] = 0\n",
    "                elif index >= 12 and index <= 24:\n",
    "                    city_data[col].iloc[index] = city_data[col].iloc[index - 12]\n",
    "                else:\n",
    "                    city_data[col].iloc[index] = min(city_data[col].iloc[index - 12], city_data[col].iloc[index - 24])\n",
    "    return city_data\n",
    "\n",
    "def split_data(data, look_back ):\n",
    "    \"\"\"Splits data into train and test sets based on args (Configuration class)\"\"\"\n",
    "    train = data[: -args.test_size]    \n",
    "    test = data[-args.test_size - look_back: ]\n",
    "    return train, test\n",
    "\n",
    "def to_supervised(data, d_in=args.look_back, d_out=args.n_predicted_month, features_list=[]):\n",
    "    \"\"\"\n",
    "    Frames time-series as supervised learning dataset.\n",
    "    \n",
    "    Args:\n",
    "      d_in: lookback window\n",
    "      d_out: number of predicted months\n",
    "      features_list: list of all features **where last col is the disease incidence**\n",
    "\n",
    "    Returns:\n",
    "      Numpy arrays of disease incidence (y) and other predictors (X)\n",
    "    \"\"\"\n",
    "    X, y = list(), list()\n",
    "    for index, _ in enumerate(data):\n",
    "        in_end = index + d_in\n",
    "        out_end = in_end + d_out\n",
    "        if out_end <= len(data):\n",
    "            if len(features_list) == 0 :\n",
    "                X.append(data[index: in_end, :])\n",
    "            else:\n",
    "                X.append(data[index: in_end, features_list])\n",
    "            y.append(data[in_end: out_end, -1])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "def get_data(train_np, test_np, batch_size, list_selected_features, origin_city_meta_data):\n",
    "    \"\"\"\n",
    "    Returns important feature list and data formatted for input into Pytorch \n",
    "    models\n",
    "    \"\"\" \n",
    "    # origin_city_meta_data chứa data mẫu để lấy index \n",
    "    # important_features = arrary index nên ta phải truyền indexex của columns\n",
    "    important_features = [origin_city_meta_data.columns.get_loc(c) for c in list_selected_features if c in origin_city_meta_data]  \n",
    "\n",
    "    train_X, train_y = to_supervised(train_np, features_list=important_features)\n",
    "    test_X, test_y = to_supervised(test_np, features_list=important_features)\n",
    "    train_tensor = TensorDataset(torch.from_numpy(train_X), torch.from_numpy(train_y))\n",
    "    test_tensor = (torch.from_numpy(test_X), torch.from_numpy(test_y))\n",
    "\n",
    "    train_loader = DataLoader(train_tensor, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return important_features, train_loader, test_tensor\n",
    "\n",
    "#Define Pytorch LSTM model\n",
    "class MultiVariateLSTM(nn.Module):\n",
    "    def __init__(self, n_feature=3, n_layers=2, hidden_size=50):\n",
    "        super(MultiVariateLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=n_feature, hidden_size=hidden_size, num_layers=n_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, args.n_predicted_month)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.sigma = nn.Parameter(torch.ones(args.n_predicted_month))\n",
    "    \n",
    "    def forward(self, X_batch, y_batch=None):\n",
    "        output, (last_hidden, _) = self.lstm(X_batch)\n",
    "        last_hidden_vector = output[:, -1, :]\n",
    "        y_predicted = self.linear(last_hidden_vector)\n",
    "        if y_batch != None:\n",
    "            assert y_predicted.size() == y_batch.size()\n",
    "            loss = self.loss_fn(y_predicted, y_batch)\n",
    "            loss = 0.5 * loss / self.sigma**2\n",
    "            loss = loss.sum() + torch.log(1 + self.sigma.prod())\n",
    "            return y_predicted, loss\n",
    "            #return y_predicted, self.loss_fn(y_predicted, y_batch)\n",
    "        else:\n",
    "            return y_predicted\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = torch.tensor(X, device=args.device)\n",
    "        return self.forward(X)\n",
    "\n",
    "#Define Pytorch LSTM-ATT model\n",
    "class MultiVariateLSTM_Attention(nn.Module):\n",
    "    def __init__(self, n_feature=3, n_layers=2, hidden_size=50):\n",
    "        super(MultiVariateLSTM_Attention, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=n_feature, hidden_size=hidden_size, num_layers=n_layers, batch_first=True)\n",
    "        self.attention_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        # self.linear = nn.Linear(hidden_size*2, args.n_predicted_month)\n",
    "        self.linear = nn.Linear(hidden_size, args.n_predicted_month)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.sigma = nn.Parameter(torch.ones(args.n_predicted_month))\n",
    "    \n",
    "    def forward(self, X_batch, y_batch=None):\n",
    "        output, (last_hidden, _) = self.lstm(X_batch)\n",
    "        last_hidden_vector = last_hidden[-1]\n",
    "        remain_hidden_vector = output\n",
    "        e_t = remain_hidden_vector.bmm(self.attention_linear(last_hidden_vector).unsqueeze(2)).squeeze(-1)\n",
    "        alpha_t = F.softmax(e_t, dim=1)\n",
    "        attenion_vector = remain_hidden_vector.transpose(2, 1).bmm(alpha_t.unsqueeze(2)).squeeze(-1)\n",
    "        # combine_vector = torch.cat((last_hidden_vector, attenion_vector), dim=1)\n",
    "        # combine_vector = last_hidden_vector + attenion_vector\n",
    "        y_predicted = self.linear(attenion_vector)\n",
    "        if y_batch != None:\n",
    "            assert y_predicted.size() == y_batch.size()\n",
    "            loss = self.loss_fn(y_predicted, y_batch)\n",
    "            loss = 0.5 * loss / self.sigma**2\n",
    "            loss = loss.sum() + torch.log(1 + self.sigma.prod())\n",
    "            return y_predicted, loss\n",
    "            # return y_predicted, self.loss_fn(y_predicted, y_batch)\n",
    "        else:\n",
    "            return y_predicted\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = torch.tensor(X, device=args.device)\n",
    "        return self.forward(X)\n",
    "\n",
    "# Define Pytorch CNN model\n",
    "class MultivariateCNN(nn.Module):\n",
    "    def __init__(self, num_filters=[100, 100, 100], dropout=0.01):\n",
    "        super(MultivariateCNN, self).__init__()\n",
    "        self.loss_fn = loss = nn.MSELoss()\n",
    "        self.filter_sizes = [1, 2, 3]\n",
    "        self.conv1d_list = nn.ModuleList([nn.Conv1d(args.n_features, num_filters[i], self.filter_sizes[i]) for i in range(len(self.filter_sizes))])\n",
    "        self.linear = nn.Linear(np.sum(num_filters), args.n_predicted_month)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.sigma = nn.Parameter(torch.ones(args.n_predicted_month))\n",
    "\n",
    "    def forward(self, X_batch, y_batch=None):\n",
    "        X_batch = X_batch.permute(0, 2, 1)  #(batch_size, n_features, n_look_back)\n",
    "        X_conv_list = [F.relu(conv1d(X_batch)) for conv1d in self.conv1d_list]\n",
    "        X_pool_list = [F.max_pool1d(x_conv, kernel_size=x_conv.shape[2]) for x_conv in X_conv_list]\n",
    "        X_fc = torch.cat([x_pool.squeeze(dim=2) for x_pool in X_pool_list], dim=1)\n",
    "        y_predicted = self.linear(self.dropout(X_fc))\n",
    "        if y_batch != None:\n",
    "            assert y_predicted.size() == y_batch.size()\n",
    "            loss = self.loss_fn(y_predicted, y_batch)\n",
    "            loss = 0.5 * loss / self.sigma**2\n",
    "            loss = loss.sum() + torch.log(1 + self.sigma.prod())\n",
    "            return y_predicted, loss\n",
    "            # return y_predicted, self.loss_fn(y_predicted, y_batch)\n",
    "        else:\n",
    "            return y_predicted\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = torch.tensor(X, device=args.device)\n",
    "        return self.forward(X)\n",
    "\n",
    "# Transformer Model\n",
    "# Transformer Model\n",
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, n_feature=3, dropout=0.1, max_length=24):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_dim = n_feature\n",
    "        self.max_length = max_length\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        pe = torch.zeros(max_length, self.hidden_dim)\n",
    "        for pos in range(max_length):\n",
    "            for i in range(0, self.hidden_dim, 2):\n",
    "                pe[pos, i] = math.sin(pos/(10000**(2*i/self.hidden_dim)))\n",
    "                if i + 1 < self.hidden_dim:\n",
    "                    pe[pos, i+1] = math.cos(pos/(10000**((2*i+1)/self.hidden_dim)))\n",
    "        pe = pe.unsqueeze(0)        \n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x*math.sqrt(self.hidden_dim)\n",
    "        length = x.size(1)\n",
    "        pe = Variable(self.pe[:, :length], requires_grad=False)\n",
    "        if x.is_cuda:\n",
    "            pe.cuda()\n",
    "        x = x + pe\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, d_input, n_look_back, hidden_size, n_layers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.pe = PositionalEncoder(n_feature=args.n_features, dropout=dropout)\n",
    "\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=d_input, nhead=d_input, dim_feedforward=hidden_size, dropout=dropout, activation='gelu')\n",
    "        # encoder_layers = nn.TransformerEncoderLayer(d_model=d_input, nhead=d_input, dropout=dropout, activation='gelu')\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, n_layers)\n",
    "        self.decoder = nn.Linear(d_input*n_look_back, args.n_predicted_month)\n",
    "\n",
    "        self.sigma = nn.Parameter(torch.ones(args.n_predicted_month))\n",
    "        \n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        \n",
    "    \n",
    "    def forward(self, X_batch, y_batch=None):\n",
    "        X_batch = self.pe(X_batch)\n",
    "        X_batch = self.transformer_encoder(X_batch)\n",
    "        X_batch = X_batch.view(X_batch.size(0), -1)\n",
    "        \n",
    "        y_predicted = self.decoder(X_batch)\n",
    "        if y_batch != None:\n",
    "            assert y_predicted.size() == y_batch.size()\n",
    "            loss = self.loss_fn(y_predicted, y_batch)\n",
    "            loss = 0.5 * loss / self.sigma**2\n",
    "            loss = loss.sum() + torch.log(1 + self.sigma.prod())\n",
    "            return y_predicted, loss\n",
    "        else:\n",
    "            return y_predicted\n",
    "        return X_batch\n",
    "    \n",
    "    def predict(self, X):\n",
    "        with torch.no_grad():\n",
    "            X = torch.tensor(X, device=args.device)\n",
    "        return self.forward(X).squeeze()\n",
    "\n",
    "# Create class to train and evaluate models\n",
    "class Trainer():\n",
    "    def __init__(self, city, model_type, learning_rate, important_features, train_loader, test_tensor, n_layers=2, hidden_size=128, num_filters=[100, 100, 100], dropout=0.01):\n",
    "        \"\"\"\n",
    "        Initialise trainer, allowing input of LSTM, LSTM-ATT, or CNN \n",
    "        hyperparameters. Adam optimiser used for all models.\n",
    "        \"\"\"\n",
    "        self.model_type = model_type    \n",
    "        self.city = city    \n",
    "        self.model = self.init_model(model_type, n_layers, hidden_size, num_filters, dropout, city)\n",
    "        self.model.double().to(args.device)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        self.important_features, self.train_loader, self.test_tensor = important_features, train_loader, test_tensor\n",
    "    \n",
    "    def init_model(self, model_type, n_layers, hidden_size, num_filters, dropout, city):        \n",
    "        #Get hyper param by name and set for n_layers, hidden_size, num_filters, dropout and overwrite the input param  \n",
    "        # n_layers, hidden_size,num_filters, dropout= get_opt_param_by_name( algo = model_type, city = city)\n",
    "    \n",
    "        \"\"\"Initialise a model based on whether LSTM, LSTM-ATT, or CNN is chosen.\"\"\"\n",
    "        if model_type.lower() == 'lstm':\n",
    "            model = MultiVariateLSTM(args.n_features, n_layers, hidden_size)\n",
    "        elif model_type.lower() == 'lstm_attention':\n",
    "            model = MultiVariateLSTM_Attention(args.n_features, n_layers, hidden_size)\n",
    "        elif model_type.lower() == 'cnn':\n",
    "            model = MultivariateCNN(num_filters, dropout)\n",
    "        elif model_type.lower() == 'transformers':\n",
    "            model = TransformerModel(d_input=args.n_features, n_look_back=args.look_back, hidden_size=hidden_size, n_layers=n_layers, dropout=dropout)\n",
    "        return model\n",
    "\n",
    "    def step(self, batch):\n",
    "        self.model.train()\n",
    "        X_batch, y_batch = tuple(t.to(args.device) for t in batch)\n",
    "        self.optimizer.zero_grad()\n",
    "        y_pred, loss = self.model.forward(X_batch, y_batch)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.mean().item()\n",
    "\n",
    "    def validation(self):\n",
    "        self.model.eval()\n",
    "        eval_loss = 0.0\n",
    "\n",
    "        result = {}\n",
    "\n",
    "        y_true = np.array([])\n",
    "        y_pred = np.array([])\n",
    "\n",
    "        X_batch, y_batch = tuple(t.to(args.device) for t in self.test_tensor)\n",
    "        with torch.no_grad():\n",
    "            outputs, loss = self.model.forward(X_batch, y_batch)\n",
    "            eval_loss = loss.mean().item()\n",
    "\n",
    "        return eval_loss\n",
    "\n",
    "    def train(self, epochs=20):\n",
    "        # print('param of model')\n",
    "        # print('hidensize',self.model.hidden_size)\n",
    "        # print('nlayer',self.model.n_layers)  \n",
    "        best_lost = float(\"inf\")\n",
    "        best_model = None\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0.0\n",
    "            for batch in self.train_loader:\n",
    "                loss = self.step(batch)\n",
    "                total_loss += loss\n",
    "            train_loss = total_loss/len(self.train_loader)\n",
    "            eval_loss = self.validation()\n",
    "            if eval_loss < best_lost:\n",
    "                best_lost = eval_loss\n",
    "                best_model = copy.deepcopy(self.model)\n",
    "            if (epoch + 1) == epochs or (epoch + 1) in [c + 1 for c in range(epochs) if c % int(epochs/4) == 0]:\n",
    "                print(f\"Epoch: {epoch:2}/{epochs:2} - train_loss: {train_loss:.4f} - test_loss: {eval_loss:4f}\")\n",
    "        self.model = best_model\n",
    "        self.model.eval()\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "    # Lưu model vào trong thư mục models\n",
    "    def save_model_to(self, path = '', city =''):       \n",
    "        torch.save(self.model, path)\n",
    "\n",
    "    def load_model_to(self, path = ''):       \n",
    "        return torch.load(path)\n",
    "\n",
    "    def evaluate_model(self, np_data=None, plot=True, scaled=True, city=None, k_steps=None, y_scaler =None):\n",
    "        assert scaled, \"data must be scaled\"\n",
    "        self.model.eval()\n",
    "        tensor_data = torch.from_numpy(np_data)\n",
    "        rmse_list = []\n",
    "        mae_list = [] \n",
    "        mape_list = []\n",
    "\n",
    "        y_predicted_list = []\n",
    "        y_true_list = []\n",
    "\n",
    "        for k_steps in range(1, args.n_predicted_month + 1):\n",
    "            y_predicted = []\n",
    "            y_true = []\n",
    "            for index in range(tensor_data.size(0) - args.look_back):\n",
    "                X = tensor_data[index: index + args.look_back, self.important_features]\n",
    "                # yhat = self.model.predict(X.unsqueeze(0)).squeeze()\n",
    "\n",
    "                yhat = self.model.predict(X.unsqueeze(0))\n",
    "                yhat = yhat.squeeze()\n",
    "\n",
    "                y_predicted.append(yhat.detach().cpu().numpy()[k_steps - 1])\n",
    "                y_true.append(tensor_data[index + args.look_back, -1].detach().cpu().numpy())\n",
    "\n",
    "            y_predicted = y_scaler.inverse_transform(np.array(y_predicted).reshape(-1, 1)).reshape(-1, )\n",
    "            y_true = y_scaler.inverse_transform(np.array(y_true).reshape(-1, 1)).reshape(-1, )\n",
    "\n",
    "            \n",
    "\n",
    "            rmse = mean_squared_error(y_true, y_predicted, squared=False)\n",
    "            mae = mean_absolute_error(y_true, y_predicted)\n",
    "            mape = mean_absolute_percentage_error(y_true, y_predicted)\n",
    "\n",
    "            rmse_list.append(rmse)\n",
    "            mae_list.append(mae)\n",
    "            mape_list.append(mape)\n",
    "\n",
    "            print('City: '+self.city+'  _algo:'+self.model_type+'  -RMSE: '+str(rmse))          \n",
    "            if plot==True:\n",
    "              plt.grid(True)\n",
    "              plt.plot(y_predicted, label='predicted')\n",
    "              plt.plot(y_true, label='actual')\n",
    "              plt.title(f\"k-steps = {k_steps} - city: \"+self.city+'  _algo:'+self.model_type+'  -RMSE: '+str(rmse))\n",
    "              plt.legend()\n",
    "              plt.show()\n",
    "\n",
    "              plt.show()\n",
    "            y_predicted_list.append(y_predicted)\n",
    "            y_true_list.append(y_true)\n",
    "\n",
    "        return y_true_list, y_predicted_list, rmse_list, mae_list, mape_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get optimized Hyper param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 190, 168, 6, 0.004015679480821153, '', '')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_str_num_filter_array(str_filter = ''):  \n",
    "  arr_filter = str_filter.split(',')  \n",
    "  int_arr_filter= [int(x) for x in arr_filter]\n",
    "  return int_arr_filter\n",
    "\n",
    "\n",
    "opt_param_cnn = pd.read_excel(prj_path_opt+'diarrhoea_opt_hyperparam_cnn.xlsx', index_col = 'City')\n",
    "opt_param_lstm = pd.read_excel(prj_path_opt+'diarrhoea_opt_hyperparam_lstm.xlsx', index_col = 'City')\n",
    "opt_param_lstm_att = pd.read_excel(prj_path_opt+'diarrhoea_opt_hyperparam_lstm_att.xlsx', index_col = 'City')\n",
    "opt_param_transformer = pd.read_excel(prj_path_opt+'diarrhoea_opt_hyperparam_transformer.xlsx', index_col = 'City')\n",
    "\n",
    "def get_opt_param_by_name(algo = 'transformers', city = None):\n",
    "  if algo == 'cnn':\n",
    "    city_param = opt_param_cnn.loc[city]\n",
    "    #load  param\n",
    "    n_layers = ''\n",
    "    hidden_size = ''\n",
    "    num_filters = convert_str_num_filter_array(city_param['Num. filters'])\n",
    "    dropout = float(city_param['Dropout rate'])\n",
    "    learning_rate = float(city_param['Learning rate'])\n",
    "    epoch = int(city_param['Epochs'])\n",
    "    lookback_window = int(city_param['Lookback Window'])\n",
    "  elif algo == 'lstm':\n",
    "    city_param = opt_param_lstm.loc[city]\n",
    "    #load  param\n",
    "    n_layers = int(city_param['n Layers'])\n",
    "    hidden_size = int(city_param['Hiden Size'])\n",
    "    num_filters = ''\n",
    "    dropout = ''\n",
    "    learning_rate = float(city_param['Learning rate'])\n",
    "    epoch = int(city_param['Epochs'])\n",
    "    lookback_window = int(city_param['Lookback Window'])\n",
    "  elif algo == 'lstm_attention':\n",
    "    city_param = opt_param_lstm_att.loc[city]\n",
    "    #load  param\n",
    "    n_layers = int(city_param['n Layers'])\n",
    "    hidden_size = int(city_param['Hiden Size'])\n",
    "    num_filters = ''\n",
    "    dropout = ''\n",
    "    learning_rate = float(city_param['Learning rate'])\n",
    "    epoch = int(city_param['Epochs'])\n",
    "    lookback_window = int(city_param['Lookback Window'])\n",
    "  elif algo == 'transformers':\n",
    "    city_param = opt_param_transformer.loc[city]\n",
    "    #load  param\n",
    "    n_layers = int(city_param['n Layers'])\n",
    "    hidden_size = int(city_param['Hidden Size'])\n",
    "    num_filters = ''\n",
    "    dropout = float(city_param['Dropout rate'])\n",
    "    learning_rate = float(city_param['Learning rate'])\n",
    "    epoch = int(city_param['Epochs'])\n",
    "    lookback_window = int(city_param['Lookback Window'])\n",
    "  return n_layers, hidden_size, num_filters, dropout, learning_rate, epoch, lookback_window\n",
    "\n",
    "\n",
    "# Manual Check param before train\n",
    "n_layers, hidden_size, num_filters, dropout, learning_rate, epoch, lookback_window= get_opt_param_by_name(algo='lstm', city='Cao Bằng')\n",
    "lookback_window,epoch, hidden_size, n_layers,learning_rate, num_filters, dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Cross validation Train/Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross validation chỉ trên chiều dài của bộ train = 212 (1997 - 1/1/2018 bỏ đi (36 + 3 windows + 1 station))\n",
    "# tạo l_train_indices\\l_test_indices\n",
    "data = list(range(212))\n",
    "min_train_size = 72 # Chiều dài ban đầu train size = 6 năm = 72\n",
    "test_size = 36 # chiều dài test size 36 tháng\n",
    "l_train_indices= []\n",
    "l_test_indices = []\n",
    "for i in range(min_train_size, len(data)-test_size+1):\n",
    "    train = np.array(data[0:i])\n",
    "    test = np.array(data[i:i+test_size])    \n",
    "    l_train_indices.append(train)\n",
    "    l_test_indices.append(test)\n",
    "\n",
    "len(l_test_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Tạo và Predict. Important (epoch = 1 for test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_train_predict(city ='', algo = '', list_selected_features = '', origin_city_meta_test_set= '', y_scaler = '', s_selected_features= '', train ='', test ='', sample_idx ='', combination_idx= ''):\n",
    "    \n",
    "    metric_reslt = pd.DataFrame()\n",
    "    n_layers, hidden_size, num_filters, dropout, learning_rate, epoch, lookback_window = get_opt_param_by_name( algo = algo, city = city)\n",
    "    # Get data to run model\n",
    "    important_features, train_loader, test_tensor = get_data(train, test, args.batch_size, list_selected_features, origin_city_meta_test_set)   \n",
    "\n",
    "#     epoch = 1 # comment when do experiment\n",
    "    \n",
    "    if( algo =='cnn'): \n",
    "        algo = 'CNN'    # upper case only\n",
    "        trainer = Trainer(\n",
    "                            model_type='cnn',  \n",
    "                            city = city,                    \n",
    "                            learning_rate=learning_rate,\n",
    "                            important_features=important_features,\n",
    "                            train_loader=train_loader,\n",
    "                            test_tensor=test_tensor,\n",
    "                            num_filters= num_filters, \n",
    "                            dropout=dropout )\n",
    "\n",
    "    if( algo =='lstm'):\n",
    "        algo = 'LSTM'    # upper case only        \n",
    "        trainer = Trainer(\n",
    "                            model_type='lstm', \n",
    "                            city = city,\n",
    "                            learning_rate=learning_rate,\n",
    "                            important_features=important_features,\n",
    "                            train_loader=train_loader,\n",
    "                            test_tensor=test_tensor,\n",
    "                            n_layers=n_layers,\n",
    "                            hidden_size=hidden_size)\n",
    "\n",
    "    if( algo =='lstm_attention'): \n",
    "        algo = 'LSTM ATT'    # upper case only       \n",
    "        trainer = Trainer(\n",
    "                            model_type= 'lstm_attention',\n",
    "                            city = city,\n",
    "                            learning_rate= learning_rate,\n",
    "                            important_features=important_features,\n",
    "                            train_loader=train_loader,\n",
    "                            test_tensor=test_tensor,\n",
    "                            n_layers= n_layers,\n",
    "                            hidden_size=hidden_size )\n",
    "\n",
    "    if( algo =='transformers'): \n",
    "        algo = 'TF'    # upper case only       \n",
    "        trainer = Trainer(\n",
    "                            model_type='transformers',\n",
    "                            city = city, \n",
    "                            learning_rate=learning_rate,\n",
    "                            important_features=important_features,\n",
    "                            train_loader=train_loader,\n",
    "                            test_tensor=test_tensor,\n",
    "                            n_layers=n_layers,\n",
    "                            hidden_size=hidden_size,\n",
    "                            dropout=dropout)\n",
    "\n",
    "    \n",
    "    # Train model with province-specific epochs\n",
    "    trainer.train(epochs=epoch)  \n",
    "    # Evaluate test set   \n",
    "    y_true_test, y_pred_test, rmse_list, mae_list, mape_list = trainer.evaluate_model(np_data=test, plot= False, scaled=True, city=city, y_scaler= y_scaler)\n",
    "        \n",
    "    metric_reslt = pd.DataFrame({\n",
    "                                'City': city,\n",
    "                                'Meta_Model_Name': algo, \n",
    "                                'sample_index':sample_idx,\n",
    "                                'Based_Models': s_selected_features,                                                      \n",
    "                                # algo+'_'+str(combination_idx)+'_rmse': rmse_list[0],\n",
    "                                # algo+'_'+str(combination_idx)+'_mae': mae_list[0]\n",
    "                                algo+'*'+s_selected_features+'_rmse': rmse_list[0],\n",
    "                                algo+'*'+s_selected_features+'_mae': mae_list[0]\n",
    "                                }, index=[0]) # do có 1 dòng nên pandas cần index = 0    \n",
    "  \n",
    "\n",
    "        \n",
    "    return metric_reslt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Cross Validation Metrics Creation\n",
    "Tạo ra 105 bộ validation. Xuất ra Metrics (RMSE và MAE) cho 5 bộ giải thuật chọn lựa. (10 cột cho 1 dòng validation )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0/150 - train_loss: 0.8247 - test_loss: 0.682988\n",
      "Epoch: 37/150 - train_loss: 0.3412 - test_loss: 0.209946\n",
      "Epoch: 74/150 - train_loss: 0.3693 - test_loss: 0.187318\n",
      "Epoch: 111/150 - train_loss: 0.3234 - test_loss: 0.169690\n",
      "Epoch: 148/150 - train_loss: 0.3295 - test_loss: 0.192470\n",
      "Epoch: 149/150 - train_loss: 0.2939 - test_loss: 0.192782\n",
      "City: An Giang  _algo:cnn  -RMSE: 33.82002791576664\n",
      "City: An Giang  _algo:cnn  -RMSE: 39.688850610466794\n",
      "City: An Giang  _algo:cnn  -RMSE: 43.46443386125075\n",
      "Epoch:  0/330 - train_loss: 0.8767 - test_loss: 0.650857\n",
      "Epoch: 82/330 - train_loss: 0.3073 - test_loss: 0.178986\n",
      "Epoch: 164/330 - train_loss: 0.2815 - test_loss: 0.177463\n",
      "Epoch: 246/330 - train_loss: 0.2523 - test_loss: 0.174099\n",
      "Epoch: 328/330 - train_loss: 0.2326 - test_loss: 0.142477\n",
      "Epoch: 329/330 - train_loss: 0.2272 - test_loss: 0.130925\n",
      "City: An Giang  _algo:lstm  -RMSE: 27.183732625511837\n",
      "City: An Giang  _algo:lstm  -RMSE: 39.26673928143484\n",
      "City: An Giang  _algo:lstm  -RMSE: 36.366822569297646\n",
      "Epoch:  0/150 - train_loss: 0.9471 - test_loss: 0.736187\n",
      "Epoch: 37/150 - train_loss: 0.3775 - test_loss: 0.238062\n",
      "Epoch: 74/150 - train_loss: 0.3313 - test_loss: 0.226626\n",
      "Epoch: 111/150 - train_loss: 0.3363 - test_loss: 0.209141\n",
      "Epoch: 148/150 - train_loss: 0.3113 - test_loss: 0.202322\n",
      "Epoch: 149/150 - train_loss: 0.3133 - test_loss: 0.196448\n",
      "City: An Giang  _algo:cnn  -RMSE: 46.1113304654177\n",
      "City: An Giang  _algo:cnn  -RMSE: 38.46261763103053\n",
      "City: An Giang  _algo:cnn  -RMSE: 38.081782647991076\n",
      "Epoch:  0/150 - train_loss: 0.8231 - test_loss: 0.673916\n",
      "Epoch: 37/150 - train_loss: 0.3433 - test_loss: 0.200670\n",
      "Epoch: 74/150 - train_loss: 0.3248 - test_loss: 0.226565\n",
      "Epoch: 111/150 - train_loss: 0.3163 - test_loss: 0.184328\n",
      "Epoch: 148/150 - train_loss: 0.2993 - test_loss: 0.179662\n",
      "Epoch: 149/150 - train_loss: 0.2927 - test_loss: 0.174589\n",
      "City: An Giang  _algo:cnn  -RMSE: 45.62688697019511\n",
      "City: An Giang  _algo:cnn  -RMSE: 41.80527078376959\n",
      "City: An Giang  _algo:cnn  -RMSE: 44.98054906532989\n",
      "Epoch:  0/330 - train_loss: 0.7856 - test_loss: 0.673428\n",
      "Epoch: 82/330 - train_loss: 0.3285 - test_loss: 0.222989\n",
      "Epoch: 164/330 - train_loss: 0.2639 - test_loss: 0.171493\n"
     ]
    }
   ],
   "source": [
    "dt_started = datetime.now()\n",
    "\n",
    "l_errCity = {} # có lỗi sẽ lưu vào đây, kiểm tra ngay cell sau\n",
    "\n",
    "\n",
    "# Với mỗi thành phố, loop qua từng bộ tr/validation, cho train/predict N tổ hợp models\n",
    "for city in cities:\n",
    "  pd_all_rows_new_meta_data = pd.DataFrame()\n",
    "\n",
    "  for sample_index in  range(0,len(l_train_indices)):\n",
    "\n",
    "    pd_one_row_new_meta_data = pd.DataFrame()\n",
    "    combination_idx = 0\n",
    "    for combination_features in combs_features:            \n",
    "      list_selected_features = [x for x in combination_features] +['Observed'] # Lấy danh sách tên các features + cột Observed \n",
    "      s_selected_features = '-'.join([str(elem) for elem in list_selected_features])      \n",
    "\n",
    "      # Có trong danh sách chọn lựa thì thực hiện dự đoán\n",
    "      if((is_in_Selected_combination('CNN*'+s_selected_features)==True) or (is_in_Selected_combination('LSTM*'+s_selected_features) == True)):        \n",
    "        args.n_features = len(list_selected_features) \n",
    "        args.look_back = meta_lookback_window  \n",
    "        cnn_metrics = pd.DataFrame()\n",
    "        lstm_metrics = pd.DataFrame()\n",
    "\n",
    "        try:\n",
    "          # Lấy data từ train set và chia ra theo valiation\n",
    "          origin_city_meta_train_set = meta_train_set_city[city]\n",
    "          origin_city_meta_test_set = meta_test_set_city[city]\n",
    "          \n",
    "          train = origin_city_meta_train_set.iloc[l_train_indices[sample_index]]\n",
    "          test = origin_city_meta_train_set.iloc[l_test_indices[sample_index]]        \n",
    "\n",
    "          # Data train/test processing for all Meta model\n",
    "          train = train.astype(float)\n",
    "          test = test.astype(float)\n",
    "\n",
    "          train = train.replace((np.inf, -np.inf, np.nan), 0)\n",
    "          test = test.replace((np.inf, -np.inf, np.nan), 0) \n",
    "        \n",
    "          # Đoạn này impute các trường hợp rỗng\n",
    "          train = impute_missing_value(train)\n",
    "          test = impute_missing_value(test)\n",
    "\n",
    "          # Fit data scaler to training data\n",
    "          full_scaler = MinMaxScaler().fit(train)\n",
    "          y_scaler = MinMaxScaler().fit(train.values[:, -1].reshape(-1, 1))\n",
    "\n",
    "          # Scale train and test data\n",
    "          train = full_scaler.transform(train)\n",
    "          test = full_scaler.transform(test)\n",
    "\n",
    "          if(is_in_Selected_combination('CNN*'+s_selected_features)==True):\n",
    "            ########################################\n",
    "            ########## CNN \n",
    "            ########################################\n",
    "            cnn_metrics = do_train_predict(city =city, algo='cnn', list_selected_features = list_selected_features,\n",
    "                                                origin_city_meta_test_set= origin_city_meta_test_set, y_scaler = y_scaler, \n",
    "                                                s_selected_features= s_selected_features, train = train, test = test, \n",
    "                                                sample_idx = sample_index,combination_idx= combination_idx)\n",
    "\n",
    "          if(is_in_Selected_combination('LSTM*'+s_selected_features)==True):\n",
    "            ########################################\n",
    "            ########## LSTM \n",
    "            ########################################\n",
    "            lstm_metrics = do_train_predict(city =city, algo='lstm', list_selected_features = list_selected_features,\n",
    "                                                origin_city_meta_test_set= origin_city_meta_test_set, y_scaler = y_scaler, \n",
    "                                                s_selected_features= s_selected_features, train = train, test = test, \n",
    "                                                sample_idx = sample_index,combination_idx= combination_idx)\n",
    "          \n",
    "\n",
    "          \n",
    "          \n",
    "          ###############################   \n",
    "          ## Khúc này rất thủ công\n",
    "          if(cnn_metrics.empty==False and lstm_metrics.empty ==False):              \n",
    "            join_2_results = pd.concat([cnn_metrics.iloc[:, [4, 5]], lstm_metrics.iloc[:, [4, 5]]], axis=1)\n",
    "          \n",
    "          if(cnn_metrics.empty== False and lstm_metrics.empty ==True):\n",
    "            join_2_results = cnn_metrics.iloc[:, [4, 5]]\n",
    "          \n",
    "          if(cnn_metrics.empty== True and lstm_metrics.empty ==False):\n",
    "            join_2_results = lstm_metrics.iloc[:, [4, 5]]\n",
    "\n",
    "          if(pd_one_row_new_meta_data.empty):\n",
    "            pd_one_row_new_meta_data = join_2_results\n",
    "          else:\n",
    "            pd_one_row_new_meta_data = pd.concat([join_2_results, pd_one_row_new_meta_data], axis=1)       \n",
    "\n",
    "          combination_idx = combination_idx + 1\n",
    "        except Exception as e:\n",
    "          l_errCity[city] = e\n",
    "          break    \n",
    "\n",
    "        \n",
    "    # Hết vòng lập pd_one_row_new_meta_data chứa 5 bộ kết quả của 1 sameple    \n",
    "    # Qua sample kế tiếp   \n",
    "    pd_all_rows_new_meta_data = pd.concat([pd_all_rows_new_meta_data, pd_one_row_new_meta_data], axis=0) \n",
    "\n",
    "  \n",
    "  # Kết thúc hết tất cả validation -> Lưu kết quả cuối cùng ứng với từng thành phố\n",
    "  pd_all_rows_new_meta_data.to_excel(com_path_validation_metrics+ city+'_5_comb_cv_metrics.xlsx')\n",
    "  \n",
    "  # Telegram vào\n",
    "  send_to_telegram('Server Chạy xong city: '+ city)\n",
    "\n",
    "dt_ended = datetime.now()\n",
    "run_time = round((dt_ended - dt_started).total_seconds()/60)\n",
    "print('kết thúc train trong:', run_time)\n",
    "print ('danh sách error city: ')\n",
    "print(l_errCity)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "290.909px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "7a15bcec7c0559c62175669ec6b991fdfb120f79b3044ee904eb55b269f77428"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
