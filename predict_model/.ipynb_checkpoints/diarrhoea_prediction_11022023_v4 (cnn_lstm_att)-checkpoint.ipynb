{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Fw31Za_INKC"
   },
   "source": [
    "# Prediction Model Diarrhoea Version 2\n",
    "Last Big Modification: 17/10/2022\n",
    "Description: 3 Models CNN, LSTM, LSTM-ATT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1314,
     "status": "ok",
     "timestamp": 1674178221013,
     "user": {
      "displayName": "Nhat Le",
      "userId": "00702307310725808810"
     },
     "user_tz": -420
    },
    "id": "Ja18nXEglgwF",
    "outputId": "ca35aa0e-0e51-4f5d-e326-7cb70df9b6cd",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Feb 11 15:08:35 2023       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 525.60.13    Driver Version: 525.60.13    CUDA Version: 12.0     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Quadro RTX 6000     On   | 00000000:21:00.0 Off |                  Off |\r\n",
      "| 34%   43C    P8    24W / 260W |   1175MiB / 24576MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A     48723      C   ..._diseases_py39/bin/python     1172MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6753,
     "status": "ok",
     "timestamp": 1674178227764,
     "user": {
      "displayName": "Nhat Le",
      "userId": "00702307310725808810"
     },
     "user_tz": -420
    },
    "id": "VW4v2IEcTYHa",
    "outputId": "a145eb88-32fb-4b52-eff9-f0000b4b7ce2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /home/mlworker/lib/python3.10/site-packages (1.2.1)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /home/mlworker/lib/python3.10/site-packages (from scikit-learn) (1.8.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/mlworker/lib/python3.10/site-packages (from scikit-learn) (1.23.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/mlworker/lib/python3.10/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/mlworker/lib/python3.10/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: ftfy in /home/mlworker/lib/python3.10/site-packages (6.1.1)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /home/mlworker/lib/python3.10/site-packages (from ftfy) (0.2.5)\n"
     ]
    }
   ],
   "source": [
    "# Install packages\n",
    "!pip install -U scikit-learn\n",
    "!pip install ftfy\n",
    "# !pip install xlsxwriter \n",
    "# !pip install xlwt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "b8I8S9koHjT_"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import tensorflow as tf \n",
    "import os\n",
    "\n",
    "from tensorflow.keras.models import Sequential, model_from_json\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "import traceback\n",
    "import sys\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from ftfy import fix_text\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [7, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sph_sUXffs1d"
   },
   "source": [
    "# Input path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2806,
     "status": "ok",
     "timestamp": 1674178230566,
     "user": {
      "displayName": "Nhat Le",
      "userId": "00702307310725808810"
     },
     "user_tz": -420
    },
    "id": "MNAuUk6_HVs1",
    "outputId": "9a27cc3b-83c1-4d57-8784-ccc7ac908798"
   },
   "outputs": [],
   "source": [
    "prj_path = '/home/mlworker/Quang/HealthCare/Source_14012023_v4/'\n",
    "prj_path_opt= prj_path + \"optimize_hyperparam/opt_results/opt_results_10022023_v4/\"\n",
    "data_path = prj_path + \"data/new_data/DH/squeezed/\"\n",
    "\n",
    "# result path \n",
    "path = prj_path + \"results/results_11022023/\"\n",
    "path_metrix = path + 'metrix/'\n",
    "path_all_mix_cnn = path + 'all_mix_data/cnn/'\n",
    "path_all_mix_lstm = path + 'all_mix_data/lstm/'\n",
    "path_all_mix_lstm_att = path + 'all_mix_data/lstm_att/'\n",
    "\n",
    "\n",
    "os.chdir(prj_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kVZZNdDFoH_m"
   },
   "source": [
    "# Class Train and Support Functions for Preprocess\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "fVkHGeUPO6cO"
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    import random, os\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    \n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "lEn56jmQtEmD"
   },
   "outputs": [],
   "source": [
    "# Set hyperparameters as args using the Configuration class\n",
    "class Configuration():\n",
    "    def __init__(self):\n",
    "      # lấy bộ test dài 24 tháng = 2 năm\n",
    "        self.test_size = 36\n",
    "        # là nhìn vào dữ liệu trước 3 tháng và dự phóng        \n",
    "        self.look_back = 3\n",
    "        # dự phóng 3 tháng\n",
    "        self.n_predicted_month = 3\n",
    "        self.n_features = 3\n",
    "        self.seed = 42\n",
    "        # mỗi phần tử x trong tập suppervise có độ lớn là 16 = 16 tháng\n",
    "        self.batch_size = 16\n",
    "        self.device = torch.device(\"cuda\")\n",
    "        self.epochs = 300\n",
    "\n",
    "args = Configuration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "RLVLc78wyasW"
   },
   "outputs": [],
   "source": [
    "# Define data (pre-)processing functions\n",
    "# modification\n",
    "def get_city_data(city_name):\n",
    "    \"\"\"Returns Diarrhoea rate and climate data only\"\"\" \n",
    "    city_data = dict_full_data[city_name].drop(columns=['Diarrhoea_cases', 'province', 'year_month',\n",
    "                                                        'Influenza_rates','Dengue_fever_rates',\n",
    "                                                        'Influenza_cases','Dengue_fever_cases', 'year', 'month',\n",
    "                                                        'population_male','population_female','population_urban',\n",
    "                                                        'population_countryside','area','population_density',\n",
    "                                                        'population_average','birth_rate','urban_water_usage_rate',\n",
    "                                                        'clean_water_rate_all','poverty_rate','toilet_rate'\n",
    "                                                        ], \n",
    "                                                                  axis=1, \n",
    "                                                                  inplace=False)    \n",
    "    return city_data\n",
    "\n",
    "def convert_to_stationary(city_data):\n",
    "    \"\"\"Subtracts previous value for all cols except disease rates\"\"\"\n",
    "    for col_name in city_data.columns:\n",
    "        if col_name != 'Diarrhoea_rates':\n",
    "            try:\n",
    "                city_data[col_name] = city_data[col_name] - city_data[col_name].shift()\n",
    "            except:\n",
    "                print(col_name)\n",
    "    return city_data\n",
    "\n",
    "def impute_missing_value(city_data):\n",
    "    \"\"\"\n",
    "    Cơ bản dữ liệu bị thiếu sót rất nhiều: Như Điện Biên 1997 -2003 là thiếu dữ liệu về bệnh\n",
    "    Hàm này sẽ tự sinh ra dữ liệu bị thiếu. Nếu tháng nào không có số liệu thì tính như sau:\n",
    "    12 tháng đầu không có số liệu thì gán = 0\n",
    "    tháng 13-24 không có số liệu, sẽ lấy giá trị của tháng cùng kỳ năm trước\n",
    "    tháng từ 24 trở đi sẽ lấy giá trị nhỏ nhất của 2 tháng cùng kỳ trong 2 năm gần nhất.\n",
    "    Do Điện Biên bằng 0 nên sau khi xử lý từ 1997 -2003 là đều = 0.  \n",
    "    \"\"\"\n",
    "    for col in city_data.columns:\n",
    "        for index in range(len(city_data[col])):\n",
    "            if np.isnan(city_data[col].iloc[index]):\n",
    "                if index < 12:\n",
    "                    city_data[col].iloc[index] = 0\n",
    "                elif index >= 12 and index <= 24:\n",
    "                    city_data[col].iloc[index] = city_data[col].iloc[index - 12]\n",
    "                else:\n",
    "                    city_data[col].iloc[index] = min(city_data[col].iloc[index - 12], city_data[col].iloc[index - 24])\n",
    "    return city_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "SVVksb_PYZ2R"
   },
   "outputs": [],
   "source": [
    "cities = [\n",
    "    'An Giang', 'BR Vũng Tàu', 'Bình Phước', 'Bình Thuận', 'Bình Định',\n",
    "        'Bạc Liêu', 'Bắc Kạn', 'Bắc Giang', 'Cao Bằng', 'Cà Mau',\n",
    "        'Cần Thơ', 'Gia Lai', 'Hà Giang', 'Hà Nội', 'Hà Tĩnh', 'Hòa Bình',\n",
    "        'Hưng Yên', 'Hải Dương', 'Hải Phòng', 'Khánh Hòa', 'Kiên Giang',\n",
    "        'Kon Tum', 'Lai Châu', 'Long An', 'Lào Cai', 'Lâm Đồng',\n",
    "        'Lạng Sơn', 'Nam Định', 'Nghệ An', 'Ninh Bình', 'Ninh Thuận',\n",
    "        'Phú Thọ', 'Phú Yên', 'Quảng Bình', 'Quảng Nam', 'Quảng Ngãi',\n",
    "        'Quảng Ninh', 'Quảng Trị', 'Sóc Trăng', 'Sơn La','TT Huế',\n",
    "        'Thanh Hóa', 'Thái Bình', 'Thái Nguyên', 'Tiền Giang', 'Trà Vinh',\n",
    "        'Tuyên Quang', 'Tây Ninh', 'Vĩnh Phúc', 'Yên Bái', 'Điện Biên',\n",
    "         'Đà Nẵng', 'Đắk Nông', 'Đắk Lắk', 'Đồng Tháp']\n",
    "\n",
    "\n",
    "\n",
    "def get_dict_all_city_data():\n",
    "  cities_data = {}  \n",
    "  for city in cities:\n",
    "    city_result = pd.read_excel(data_path+'squeezed_'+city+'.xlsx')  \n",
    "    # Đoạn này rất quan trọng. Optimize không được đụng vào 36 tháng (2015-2017) để dự báo. \n",
    "    # Lúc train và predict thì ok. Lấy từ 1997 -2017. \n",
    "    # Function split data sẽ lấy 3 năm cuối làm test data theo config     \n",
    "    city_result = city_result.loc[city_result['year_month'] < '2018-1-1'] \n",
    "    cities_data[city] = city_result\n",
    "  return cities_data\n",
    "\n",
    "dict_full_data = get_dict_all_city_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "RT8LmtHts4fQ"
   },
   "outputs": [],
   "source": [
    "def split_data(data, look_back ):\n",
    "    \"\"\"Splits data into train and test sets based on args (Configuration class)\"\"\"\n",
    "    train = data[: -args.test_size]    \n",
    "    test = data[-args.test_size - look_back: ]\n",
    "    return train, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "6Wdc44U0uMbP"
   },
   "outputs": [],
   "source": [
    "def to_supervised(data, d_in=args.look_back, d_out=args.n_predicted_month, features_list=[]):\n",
    "    \"\"\"\n",
    "    Frames time-series as supervised learning dataset.\n",
    "    \n",
    "    Args:\n",
    "      d_in: lookback window\n",
    "      d_out: number of predicted months\n",
    "      features_list: list of all features **where last col is the disease incidence**\n",
    "\n",
    "    Returns:\n",
    "      Numpy arrays of disease incidence (y) and other predictors (X)\n",
    "    \"\"\"\n",
    "    X, y = list(), list()\n",
    "    for index, _ in enumerate(data):\n",
    "        in_end = index + d_in\n",
    "        out_end = in_end + d_out\n",
    "        if out_end <= len(data):\n",
    "            if len(features_list) == 0 :\n",
    "                X.append(data[index: in_end, :])\n",
    "            else:\n",
    "                X.append(data[index: in_end, features_list])\n",
    "            y.append(data[in_end: out_end, -1])\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "x_RfqYtVGTAk"
   },
   "outputs": [],
   "source": [
    "def select_feature(train, specific_data):\n",
    "    \"\"\"Selects args.n_features top features using RFE\"\"\"\n",
    "    train_X, train_y = to_supervised(train, d_in=1, d_out=1)\n",
    "    train_X, train_y = np.squeeze(train_X), np.squeeze(train_y)\n",
    "    rfe = RFE(RandomForestRegressor(n_estimators=500, random_state=args.seed), n_features_to_select=args.n_features)\n",
    "    fit = rfe.fit(train_X, train_y)\n",
    "    important_features = list()\n",
    "    print(\"Important Feature:\")\n",
    "    for i in range(len(fit.support_)):\n",
    "        if fit.support_[i]:\n",
    "            important_features.append(i)\n",
    "            print(specific_data.columns[i])\n",
    "    return np.array(important_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "OsgC3mbHV96C"
   },
   "outputs": [],
   "source": [
    "def get_data(train_np, test_np, batch_size, specific_data):\n",
    "    \"\"\"\n",
    "    Returns important feature list and data formatted for input into Pytorch \n",
    "    models\n",
    "    \"\"\"\n",
    "    important_features = select_feature(train_np, specific_data)\n",
    "\n",
    "    train_X, train_y = to_supervised(train_np, features_list=important_features)\n",
    "    test_X, test_y = to_supervised(test_np, features_list=important_features)\n",
    "    train_tensor = TensorDataset(torch.from_numpy(train_X), torch.from_numpy(train_y))\n",
    "    test_tensor = (torch.from_numpy(test_X), torch.from_numpy(test_y))\n",
    "\n",
    "    train_loader = DataLoader(train_tensor, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return important_features, train_loader, test_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "PW18DMZgF5K_"
   },
   "outputs": [],
   "source": [
    "#Define Pytorch LSTM model\n",
    "class MultiVariateLSTM(nn.Module):\n",
    "    def __init__(self, n_feature=3, n_layers=2, hidden_size=50):\n",
    "        super(MultiVariateLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=n_feature, hidden_size=hidden_size, num_layers=n_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, args.n_predicted_month)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.sigma = nn.Parameter(torch.ones(args.n_predicted_month))\n",
    "    \n",
    "    def forward(self, X_batch, y_batch=None):\n",
    "        output, (last_hidden, _) = self.lstm(X_batch)\n",
    "        last_hidden_vector = output[:, -1, :]\n",
    "        y_predicted = self.linear(last_hidden_vector)\n",
    "        if y_batch != None:\n",
    "            assert y_predicted.size() == y_batch.size()\n",
    "            loss = self.loss_fn(y_predicted, y_batch)\n",
    "            loss = 0.5 * loss / self.sigma**2\n",
    "            loss = loss.sum() + torch.log(1 + self.sigma.prod())\n",
    "            return y_predicted, loss\n",
    "            #return y_predicted, self.loss_fn(y_predicted, y_batch)\n",
    "        else:\n",
    "            return y_predicted\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = torch.tensor(X, device=args.device)\n",
    "        return self.forward(X)\n",
    "\n",
    "#Define Pytorch LSTM-ATT model\n",
    "class MultiVariateLSTM_Attention(nn.Module):\n",
    "    def __init__(self, n_feature=3, n_layers=2, hidden_size=50):\n",
    "        super(MultiVariateLSTM_Attention, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=n_feature, hidden_size=hidden_size, num_layers=n_layers, batch_first=True)\n",
    "        self.attention_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        # self.linear = nn.Linear(hidden_size*2, args.n_predicted_month)\n",
    "        self.linear = nn.Linear(hidden_size, args.n_predicted_month)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.sigma = nn.Parameter(torch.ones(args.n_predicted_month))\n",
    "    \n",
    "    def forward(self, X_batch, y_batch=None):\n",
    "        output, (last_hidden, _) = self.lstm(X_batch)\n",
    "        last_hidden_vector = last_hidden[-1]\n",
    "        remain_hidden_vector = output\n",
    "        e_t = remain_hidden_vector.bmm(self.attention_linear(last_hidden_vector).unsqueeze(2)).squeeze(-1)\n",
    "        alpha_t = F.softmax(e_t, dim=1)\n",
    "        attenion_vector = remain_hidden_vector.transpose(2, 1).bmm(alpha_t.unsqueeze(2)).squeeze(-1)\n",
    "        # combine_vector = torch.cat((last_hidden_vector, attenion_vector), dim=1)\n",
    "        # combine_vector = last_hidden_vector + attenion_vector\n",
    "        y_predicted = self.linear(attenion_vector)\n",
    "        if y_batch != None:\n",
    "            assert y_predicted.size() == y_batch.size()\n",
    "            loss = self.loss_fn(y_predicted, y_batch)\n",
    "            loss = 0.5 * loss / self.sigma**2\n",
    "            loss = loss.sum() + torch.log(1 + self.sigma.prod())\n",
    "            return y_predicted, loss\n",
    "            # return y_predicted, self.loss_fn(y_predicted, y_batch)\n",
    "        else:\n",
    "            return y_predicted\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = torch.tensor(X, device=args.device)\n",
    "        return self.forward(X)\n",
    "\n",
    "# Define Pytorch CNN model\n",
    "class MultivariateCNN(nn.Module):\n",
    "    def __init__(self, num_filters=[100, 100, 100], dropout=0.01):\n",
    "        super(MultivariateCNN, self).__init__()\n",
    "        self.loss_fn = loss = nn.MSELoss()\n",
    "        self.filter_sizes = [1, 2, 3]\n",
    "        self.conv1d_list = nn.ModuleList([nn.Conv1d(args.n_features, num_filters[i], self.filter_sizes[i]) for i in range(len(self.filter_sizes))])\n",
    "        self.linear = nn.Linear(np.sum(num_filters), args.n_predicted_month)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.sigma = nn.Parameter(torch.ones(args.n_predicted_month))\n",
    "\n",
    "    def forward(self, X_batch, y_batch=None):\n",
    "        X_batch = X_batch.permute(0, 2, 1)  #(batch_size, n_features, n_look_back)\n",
    "        X_conv_list = [F.relu(conv1d(X_batch)) for conv1d in self.conv1d_list]\n",
    "        X_pool_list = [F.max_pool1d(x_conv, kernel_size=x_conv.shape[2]) for x_conv in X_conv_list]\n",
    "        X_fc = torch.cat([x_pool.squeeze(dim=2) for x_pool in X_pool_list], dim=1)\n",
    "        y_predicted = self.linear(self.dropout(X_fc))\n",
    "        if y_batch != None:\n",
    "            assert y_predicted.size() == y_batch.size()\n",
    "            loss = self.loss_fn(y_predicted, y_batch)\n",
    "            loss = 0.5 * loss / self.sigma**2\n",
    "            loss = loss.sum() + torch.log(1 + self.sigma.prod())\n",
    "            return y_predicted, loss\n",
    "            # return y_predicted, self.loss_fn(y_predicted, y_batch)\n",
    "        else:\n",
    "            return y_predicted\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = torch.tensor(X, device=args.device)\n",
    "        return self.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "lWNY7faYe52P"
   },
   "outputs": [],
   "source": [
    "# Create class to train and evaluate models\n",
    "class Trainer():\n",
    "    def __init__(self, city, model_type, learning_rate, important_features, train_loader, test_tensor, n_layers=2, hidden_size=128, num_filters=[100, 100, 100], dropout=0.01):\n",
    "        \"\"\"\n",
    "        Initialise trainer, allowing input of LSTM, LSTM-ATT, or CNN \n",
    "        hyperparameters. Adam optimiser used for all models.\n",
    "        \"\"\"\n",
    "        self.model_type = model_type    \n",
    "        self.city = city    \n",
    "        self.model = self.init_model(model_type, n_layers, hidden_size, num_filters, dropout, city)\n",
    "        self.model.double().to(args.device)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        self.important_features, self.train_loader, self.test_tensor = important_features, train_loader, test_tensor\n",
    "    \n",
    "    def init_model(self, model_type, n_layers, hidden_size, num_filters, dropout, city):        \n",
    "        #Get hyper param by name and set for n_layers, hidden_size, num_filters, dropout and overwrite the input param  \n",
    "        # n_layers, hidden_size,num_filters, dropout= get_opt_param_by_name( algo = model_type, city = city)\n",
    "    \n",
    "        \"\"\"Initialise a model based on whether LSTM, LSTM-ATT, or CNN is chosen.\"\"\"\n",
    "        if model_type.lower() == 'lstm':\n",
    "            model = MultiVariateLSTM(args.n_features, n_layers, hidden_size)\n",
    "        elif model_type.lower() == 'lstm_attention':\n",
    "            model = MultiVariateLSTM_Attention(args.n_features, n_layers, hidden_size)\n",
    "        elif model_type.lower() == 'cnn':\n",
    "            model = MultivariateCNN(num_filters, dropout)\n",
    "        return model\n",
    "\n",
    "    def step(self, batch):\n",
    "        self.model.train()\n",
    "        X_batch, y_batch = tuple(t.to(args.device) for t in batch)\n",
    "        self.optimizer.zero_grad()\n",
    "        y_pred, loss = self.model.forward(X_batch, y_batch)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.mean().item()\n",
    "\n",
    "    def validation(self):\n",
    "        self.model.eval()\n",
    "        eval_loss = 0.0\n",
    "\n",
    "        result = {}\n",
    "\n",
    "        y_true = np.array([])\n",
    "        y_pred = np.array([])\n",
    "\n",
    "        X_batch, y_batch = tuple(t.to(args.device) for t in self.test_tensor)\n",
    "        with torch.no_grad():\n",
    "            outputs, loss = self.model.forward(X_batch, y_batch)\n",
    "            eval_loss = loss.mean().item()\n",
    "\n",
    "        return eval_loss\n",
    "\n",
    "    def train(self, epochs=20):\n",
    "        # print('param of model')\n",
    "        # print('hidensize',self.model.hidden_size)\n",
    "        # print('nlayer',self.model.n_layers)  \n",
    "        best_lost = float(\"inf\")\n",
    "        best_model = None\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0.0\n",
    "            for batch in self.train_loader:\n",
    "                loss = self.step(batch)\n",
    "                total_loss += loss\n",
    "            train_loss = total_loss/len(self.train_loader)\n",
    "            eval_loss = self.validation()\n",
    "            if eval_loss < best_lost:\n",
    "                best_lost = eval_loss\n",
    "                best_model = copy.deepcopy(self.model)\n",
    "            if (epoch + 1) == epochs or (epoch + 1) in [c + 1 for c in range(epochs) if c % int(epochs/4) == 0]:\n",
    "                print(f\"Epoch: {epoch:2}/{epochs:2} - train_loss: {train_loss:.4f} - test_loss: {eval_loss:4f}\")\n",
    "        self.model = best_model\n",
    "        self.model.eval()\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "    # Lưu model vào trong thư mục models\n",
    "    def save_model_to(self, path = '', city =''):       \n",
    "        torch.save(self.model, path)\n",
    "\n",
    "    def load_model_to(self, path = ''):       \n",
    "        return torch.load(path)\n",
    "\n",
    "    def evaluate_model(self, np_data=None, plot=True, scaled=True, city=None, k_steps=None, y_scaler =None):\n",
    "        assert scaled, \"data must be scaled\"\n",
    "        self.model.eval()\n",
    "        tensor_data = torch.from_numpy(np_data)\n",
    "        rmse_list = []\n",
    "        mae_list = [] \n",
    "        mape_list = []\n",
    "\n",
    "        y_predicted_list = []\n",
    "        y_true_list = []\n",
    "\n",
    "        for k_steps in range(1, args.n_predicted_month + 1):\n",
    "            y_predicted = []\n",
    "            y_true = []\n",
    "            for index in range(tensor_data.size(0) - args.look_back):\n",
    "                X = tensor_data[index: index + args.look_back, self.important_features]\n",
    "                # yhat = self.model.predict(X.unsqueeze(0)).squeeze()\n",
    "\n",
    "                yhat = self.model.predict(X.unsqueeze(0))\n",
    "                yhat = yhat.squeeze()\n",
    "\n",
    "                y_predicted.append(yhat.detach().cpu().numpy()[k_steps - 1])\n",
    "                y_true.append(tensor_data[index + args.look_back, -1].detach().cpu().numpy())\n",
    "\n",
    "            y_predicted = y_scaler.inverse_transform(np.array(y_predicted).reshape(-1, 1)).reshape(-1, )\n",
    "            y_true = y_scaler.inverse_transform(np.array(y_true).reshape(-1, 1)).reshape(-1, )\n",
    "\n",
    "            \n",
    "\n",
    "            rmse = mean_squared_error(y_true, y_predicted, squared=False)\n",
    "            mae = mean_absolute_error(y_true, y_predicted)\n",
    "            mape = mean_absolute_percentage_error(y_true, y_predicted)\n",
    "\n",
    "            rmse_list.append(rmse)\n",
    "            mae_list.append(mae)\n",
    "            mape_list.append(mape)\n",
    "\n",
    "#             print('City: '+self.city+'  _algo:'+self.model_type+'  -RMSE: '+str(rmse))          \n",
    "#             if plot==True:\n",
    "#               plt.grid(True)\n",
    "#               plt.plot(y_predicted, label='predicted')\n",
    "#               plt.plot(y_true, label='actual')\n",
    "#               plt.title(f\"k-steps = {k_steps} - city: \"+self.city+'  _algo:'+self.model_type+'  -RMSE: '+str(rmse))\n",
    "#               plt.legend()\n",
    "#               plt.show()\n",
    "\n",
    "            print('City: '+self.city+'  _algo:'+self.model_type+'  -MAE: '+str(mae))          \n",
    "            if plot==True:\n",
    "              plt.grid(True)\n",
    "              plt.plot(y_predicted, label='predicted')\n",
    "              plt.plot(y_true, label='actual')\n",
    "              plt.title(f\"k-steps = {k_steps} - city: \"+self.city+'  _algo:'+self.model_type+'  -MAE: '+str(mae))\n",
    "              plt.legend()\n",
    "              plt.show()\n",
    "    \n",
    "            y_predicted_list.append(y_predicted)\n",
    "            y_true_list.append(y_true)\n",
    "\n",
    "        return y_true_list, y_predicted_list, rmse_list, mae_list, mape_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "DblQQmN0NWv2"
   },
   "outputs": [],
   "source": [
    "def convert_str_num_filter_array(str_filter = ''):  \n",
    "  arr_filter = str_filter.split(',')  \n",
    "  int_arr_filter= [int(x) for x in arr_filter]\n",
    "  return int_arr_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0nf-w3k5rPZ9"
   },
   "source": [
    "# Get Optimized HyperParam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "t9dJEoLMsfhl"
   },
   "outputs": [],
   "source": [
    "opt_param_cnn = pd.read_excel(prj_path_opt+'/diarrhoea_opt_hyperparam_cnn.xlsx', index_col = 'City')\n",
    "opt_param_lstm = pd.read_excel(prj_path_opt+'/diarrhoea_opt_hyperparam_lstm.xlsx', index_col = 'City')\n",
    "opt_param_lstm_att = pd.read_excel(prj_path_opt+'/diarrhoea_opt_hyperparam_lstm_att.xlsx', index_col = 'City')\n",
    "\n",
    "def get_opt_param_by_name(algo = 'cnn', city = None):\n",
    "  if algo == 'cnn':\n",
    "    city_param = opt_param_cnn.loc[city]\n",
    "    #load  param\n",
    "    n_layers = ''\n",
    "    hidden_size = ''\n",
    "    num_filters = convert_str_num_filter_array(city_param['Num. filters'])\n",
    "    dropout = float(city_param['Dropout rate'])\n",
    "    learning_rate = float(city_param['Learning rate'])\n",
    "    epoch = int(city_param['Epochs'])\n",
    "    lookback_window = int(city_param['Lookback Window'])\n",
    "  elif algo == 'lstm':\n",
    "    city_param = opt_param_lstm.loc[city]\n",
    "    #load  param\n",
    "    n_layers = int(city_param['n Layers'])\n",
    "    hidden_size = int(city_param['Hiden Size'])\n",
    "    num_filters = ''\n",
    "    dropout = ''\n",
    "    learning_rate = float(city_param['Learning rate'])\n",
    "    epoch = int(city_param['Epochs'])\n",
    "    lookback_window = int(city_param['Lookback Window'])\n",
    "  elif algo == 'lstm_attention':\n",
    "    city_param = opt_param_lstm_att.loc[city]\n",
    "    #load  param\n",
    "    n_layers = int(city_param['n Layers'])\n",
    "    hidden_size = int(city_param['Hiden Size'])\n",
    "    num_filters = ''\n",
    "    dropout = ''\n",
    "    learning_rate = float(city_param['Learning rate'])\n",
    "    epoch = int(city_param['Epochs'])\n",
    "    lookback_window = int(city_param['Lookback Window'])\n",
    "  return n_layers, hidden_size, num_filters, dropout, learning_rate, epoch, lookback_window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1674178363822,
     "user": {
      "displayName": "Nhat Le",
      "userId": "00702307310725808810"
     },
     "user_tz": -420
    },
    "id": "GoxJdzkr1pGW",
    "outputId": "2c6f1cb7-b99b-47a0-b6a8-9c35f8867cb5"
   },
   "outputs": [],
   "source": [
    "# Manual Check param before train\n",
    "# n_layers, hidden_size, num_filters, dropout, learning_rate, epoch, lookback_window= get_opt_param_by_name( algo = 'cnn', city = 'Cao Bằng')\n",
    "# lookback_window,epoch, hidden_size, n_layers,learning_rate, num_filters, dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "irOFnBgCodUS"
   },
   "source": [
    "# Chạy tay test từng giải thuật"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "JkI241lYhz2l"
   },
   "outputs": [],
   "source": [
    "# ################################################\n",
    "# ## Manual test each city or cities lstm att\n",
    "# ################################################\n",
    "# n_LoopEachExpRun = 1\n",
    "# expRuntime = 0 # basicly just increase each time\n",
    "\n",
    "# cities = ['BR Vũng Tàu']\n",
    "# # cities = ['Điện Biên', 'Thái Bình', 'Lào Cai', 'Kon Tum', 'Cao Bằng']\n",
    "\n",
    "# # An Giang', 'BR Vũng Tàu', 'Bình Phước', 'Bình Thuận', 'Bình Định',\n",
    "# #        'Bạc Liêu', 'Bắc Cạn', 'Bắc Giang', 'Cao Bằng', 'Cà Mau',\n",
    "# #        'Cần Thơ', 'Gia Lai', 'Hà Giang', 'Hà Nội', 'Hà Tính', 'Hòa Bình',\n",
    "# #        'Hưng Yên', 'Hải Dương', 'Hải Phòng', 'Khánh Hòa', 'Kiên Giang',\n",
    "# #        'Kon Tum', 'Lai Châu', 'Long An', 'Lào Cai', 'Lâm Đồng',\n",
    "# #        'Lạng Sơn', 'Nam Định', 'Nghệ An', 'Ninh Bình', 'Ninh Thuận',\n",
    "# #        'Phú Thọ', 'Phú Yên', 'Quảng Bình', 'Quảng Nam', 'Quảng Ngãi',\n",
    "# #        'Quảng Ninh', 'Quảng Trị', 'Sóc Trăng', 'Sơn La', 'TT Huế',\n",
    "# #        'Thanh Hóa', 'Thái Bình', 'Thái Nguyên', 'Tiền Giang', 'Trà Vinh',\n",
    "# #        'Tuyên Quang', 'Tây Ninh', 'Vĩnh Phúc', 'Yên Bái', 'Điện Biên',\n",
    "# #        'Đà Nẵng', 'Đắc Nông', 'Đắk Lắk', 'Đồng Tháp'\n",
    "\n",
    "\n",
    "# path = '/content/drive/MyDrive/Colab Notebooks/HealthCare/Source_28_11/results/'\n",
    "# path_metrix = '/content/drive/MyDrive/Colab Notebooks/HealthCare/Source_28_11/results/metrix/'\n",
    "# path_models = '/content/drive/MyDrive/Colab Notebooks/HealthCare/Source_28_11/results/models/'\n",
    "# # provinces = all cities\n",
    "\n",
    "# l_errCity = {}\n",
    "# # Với mỗi thành phố ta sẽ chạy n_LoopEachExpRun lần ứng với từng giải thuật.\n",
    "# # Nếu chỉ chạy 1 lần thì để n_LoopEachExpRun = 0 \n",
    "\n",
    "# for city in cities:\n",
    "#   try:\n",
    "\n",
    "#     total_result = pd.DataFrame() # final result cho mỗi thành phố of all runninng time\n",
    "#     total_metrix_result = pd.DataFrame() # final matrix result cho mỗi thành phố of all runninng time\n",
    "\n",
    "#     for interation in range(n_LoopEachExpRun):\n",
    "\n",
    "#       # Pre-process data\n",
    "#       specific_data = get_city_data(fix_text(city))   \n",
    "#       specific_data = impute_missing_value(specific_data)\n",
    "#       specific_data = convert_to_stationary(specific_data)\n",
    "#       specific_data.dropna(inplace=True)\n",
    "#       specific_data_cnn= specific_data_lstm= specific_data_lstm_att = specific_data # lưu cho giải thuật sau.\n",
    "\n",
    "#       #############################################################\n",
    "#       # Generate LSTM-ATT model and define province-specific params\n",
    "#       #############################################################\n",
    "#       # Get right optimize parame\n",
    "#       n_layers, hidden_size, num_filters, dropout, learning_rate, epoch, lookback_window = get_opt_param_by_name( algo = 'lstm_attention', city = city)\n",
    "#       train, test = split_data(specific_data_lstm_att, lookback_window )\n",
    "\n",
    "#       # Fit data scaler to training data\n",
    "#       full_scaler = MinMaxScaler().fit(train)\n",
    "#       y_scaler = MinMaxScaler().fit(train.values[:, -1].reshape(-1, 1))\n",
    "\n",
    "#       # Scale train and test data\n",
    "#       train = full_scaler.transform(train)\n",
    "#       test = full_scaler.transform(test)\n",
    "\n",
    "      \n",
    "#       # Get data to run model\n",
    "#       important_features, train_loader, test_tensor = get_data(train, test, args.batch_size, specific_data_lstm_att)             \n",
    "\n",
    "     \n",
    "\n",
    "#       # Overwrite parameters for province-specific diarrhoea prediction\n",
    "#       args.look_back = lookback_window\n",
    "\n",
    "#       trainer = Trainer(model_type= 'lstm_attention',\n",
    "#                         city = city,\n",
    "#                         learning_rate= learning_rate,\n",
    "#                         important_features=important_features,\n",
    "#                         train_loader=train_loader,\n",
    "#                         test_tensor=test_tensor,\n",
    "#                         n_layers= n_layers,\n",
    "#                         hidden_size= hidden_size)\n",
    "\n",
    "#       # Train model with province-specific epochs\n",
    "#       trainer.train(epochs=epoch)\n",
    "\n",
    "#       # Evaluate model  \n",
    "#       # get predicted value of Test set \n",
    "#       y_true_test, y_pred_test, rmse_list, mae_list, mape_list = trainer.evaluate_model(np_data=test, plot= True, scaled=True, city=city, y_scaler= y_scaler) \n",
    "\n",
    "#       # get predicted value of Train set \n",
    "#       y_true_train, y_pred_train,  _, _, _, = trainer.evaluate_model(np_data=train, plot= True, scaled=True, city=city, y_scaler= y_scaler)                \n",
    "     \n",
    "      \n",
    "#       # #############################################################\n",
    "#       # Generate LSTM model and define province-specific params\n",
    "#       # Get right optimize parame\n",
    "#       n_layers, hidden_size, num_filters, dropout, learning_rate, epoch, lookback_window = get_opt_param_by_name( algo = 'lstm', city = city)\n",
    "#       train, test = split_data(specific_data_lstm_att, lookback_window )\n",
    "\n",
    "#       # Fit data scaler to training data\n",
    "#       full_scaler = MinMaxScaler().fit(train)\n",
    "#       y_scaler = MinMaxScaler().fit(train.values[:, -1].reshape(-1, 1))\n",
    "\n",
    "#       # Scale train and test data\n",
    "#       train = full_scaler.transform(train)\n",
    "#       test = full_scaler.transform(test)\n",
    "\n",
    "      \n",
    "#       # Get data to run model\n",
    "#       important_features, train_loader, test_tensor = get_data(train, test, args.batch_size, specific_data_lstm_att)  \n",
    "\n",
    "#       # Overwrite parameters for province-specific diarrhoea prediction\n",
    "#       args.look_back = lookback_window\n",
    "\n",
    "#       trainer = Trainer(model_type='lstm', \n",
    "#                         city = city,\n",
    "#                         learning_rate=learning_rate,\n",
    "#                         important_features=important_features,\n",
    "#                         train_loader=train_loader,\n",
    "#                         test_tensor=test_tensor,\n",
    "#                         n_layers= n_layers,\n",
    "#                         hidden_size= hidden_size)   \n",
    "    \n",
    "#       # Train model with province-specific epochs\n",
    "#       print('epoc',epoch)\n",
    "#       trainer.train(epochs=epoch)\n",
    "\n",
    "#       # Evaluate model  \n",
    "#       # get predicted value of Test set \n",
    "#       y_true_test, y_pred_test, rmse_list, mae_list, mape_list = trainer.evaluate_model(np_data=test, plot= True, scaled=True, city=city, y_scaler= y_scaler) \n",
    "\n",
    "#       # get predicted value of Train set \n",
    "#       y_true_train, y_pred_train,  _, _, _, = trainer.evaluate_model(np_data=train, plot= True, scaled=True, city=city, y_scaler= y_scaler) \n",
    "#   except Exception as e:\n",
    "#     l_errCity[city] = e\n",
    "#     break    \n",
    "\n",
    "# print ('danh sách error city: ')\n",
    "# print(l_errCity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "ztNnBmAst-RU"
   },
   "outputs": [],
   "source": [
    "# Plot kết quả\n",
    "# Ntest = y_true_test[0].size # 33\n",
    "# Loockback_window = lookback_window #6\n",
    "# mix_train_test_result = pd.DataFrame()\n",
    "# mix_train_test_result['Observed']= specific_data['Diarrhoea_rates'] #239\n",
    "# mix_train_test_result = mix_train_test_result.reset_index()\n",
    "# mix_train_test_result['Train Prediction'] = None\n",
    "# mix_train_test_result['Test Prediction'] = None\n",
    "# mix_train_test_result['Train Prediction'][Loockback_window:Loockback_window+y_pred_train[0].size] = y_pred_train[0]\n",
    "# mix_train_test_result['Test Prediction'].iloc[-(y_pred_test[0].size):] = y_pred_test[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "H2FElrU2mCPI"
   },
   "outputs": [],
   "source": [
    "# cols = ['Observed',\n",
    "#         'Train Prediction',\n",
    "#         'Test Prediction']      \n",
    "\n",
    "# mix_train_test_result[cols].plot(figsize=(25, 5),grid = True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "EsVzBqVRRTuN"
   },
   "outputs": [],
   "source": [
    "# Loockback_window\n",
    "# mix_train_test_result['Observed'].size\n",
    "# y_pred_train[0].size\n",
    "# y_pred_test[0].size\n",
    "# train1['Diarrhoea_rates'].size\n",
    "# test1['Diarrhoea_rates'].size\n",
    "# y_true_test[0].size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ujcW0K-sU2i3"
   },
   "source": [
    "# Mix Result và Lưu vào folder tương ứng theo từng algo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "id44nVQZoqf7"
   },
   "outputs": [],
   "source": [
    "def mix_results(city, algo, specific_data, y_pred_test, y_pred_train, lookback_window, iteration, rmse, mae, mape):\n",
    "  new_cols = [\"City\",\"Algorithm\",\"Run Seq\",\"Observed\",\"Train Prediction\",\"Test Prediction\",\"rmse\",\"mae\",\"mape\"]\n",
    "  mix_train_test_result = pd.DataFrame()\n",
    "  mix_train_test_result['Observed']= specific_data['Diarrhoea_rates'] \n",
    "  mix_train_test_result = mix_train_test_result.reset_index(drop=True)  \n",
    "  mix_train_test_result['City'] = city\n",
    "  mix_train_test_result['Algorithm'] = algo\n",
    "  mix_train_test_result['Run Seq'] = iteration \n",
    "  mix_train_test_result['Train Prediction'] = None\n",
    "  mix_train_test_result['Test Prediction'] = None\n",
    "  mix_train_test_result['Train Prediction'][lookback_window:lookback_window+y_pred_train[0].size] = y_pred_train[0]\n",
    "  mix_train_test_result['Test Prediction'].iloc[-(y_pred_test[0].size):] = y_pred_test[0]\n",
    "  mix_train_test_result['rmse'] = rmse\n",
    "  mix_train_test_result['mae'] = mae\n",
    "  mix_train_test_result['mape'] = mape\n",
    "  mix_train_test_result= mix_train_test_result.reindex(columns=new_cols)\n",
    "  return mix_train_test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "w-5_RLI3U0gJ"
   },
   "outputs": [],
   "source": [
    "def save_results(all_mix_train_test_result, algo,city):  \n",
    "  if algo == 'cnn':    \n",
    "    all_mix_train_test_result.to_excel(path_all_mix_cnn+'mix_train_test_data_'+city+'_'+algo+'.xlsx', index = False)\n",
    "  elif algo == 'lstm':\n",
    "    all_mix_train_test_result.to_excel(path_all_mix_lstm+'mix_train_test_data_'+city+'_'+algo+'.xlsx', index = False)\n",
    "  elif algo == 'lstm_att':\n",
    "    all_mix_train_test_result.to_excel(path_all_mix_lstm_att+'mix_train_test_data_'+city+'_'+algo+'.xlsx', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XPVzxxXioijX"
   },
   "source": [
    "# Main Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "VLj6VKl3_fDD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kết thúc train trong: 0\n",
      "{'An Giang': KeyError('An Giang')}\n"
     ]
    }
   ],
   "source": [
    "dt_started = datetime.now()\n",
    "################################################\n",
    "## Main run all city for 3 Algorithm\n",
    "## Mỗi city sẽ cho chạy N lần 3 bộ giải thuật.\n",
    "## Output sẽ là mỗi thành phố 3 dạng file: 1 file bao gồm predict 36 tháng + metrics, 1 file metric, 3 file predict cả train và test + metrics (bỏ vào mỗi folder).\n",
    "################################################\n",
    "n_LoopEachExpRun = 5\n",
    "expRuntime = 0 # basicly just increase each time\n",
    "\n",
    "cities = [\n",
    "        'An Giang', 'BR Vũng Tàu', 'Bình Phước', 'Bình Thuận', 'Bình Định',\n",
    "        'Bạc Liêu', 'Bắc Kạn', 'Bắc Giang', 'Cao Bằng', 'Cà Mau',\n",
    "#         'Cần Thơ', 'Gia Lai', 'Hà Giang', 'Hà Nội', 'Hà Tĩnh', 'Hòa Bình',\n",
    "#         'Hưng Yên', 'Hải Dương', 'Hải Phòng', 'Khánh Hòa', 'Kiên Giang',\n",
    "#         'Kon Tum', 'Lai Châu', 'Long An', 'Lào Cai', 'Lâm Đồng',\n",
    "#         'Lạng Sơn', 'Nam Định', 'Nghệ An', 'Ninh Bình', 'Ninh Thuận',\n",
    "#         'Phú Thọ', 'Phú Yên', 'Quảng Bình', 'Quảng Nam', 'Quảng Ngãi',\n",
    "#         'Quảng Ninh', 'Quảng Trị', 'Sóc Trăng', 'Sơn La', 'TT Huế',\n",
    "#         'Thanh Hóa', 'Thái Bình', 'Thái Nguyên', 'Tiền Giang', 'Trà Vinh',\n",
    "#         'Tuyên Quang', 'Tây Ninh', 'Vĩnh Phúc', 'Yên Bái', 'Điện Biên',\n",
    "#          'Đà Nẵng', 'Đắk Nông', 'Đắk Lắk', 'Đồng Tháp'\n",
    "         ]\n",
    "\n",
    "\n",
    "l_errCity = {} # có lỗi sẽ lưu vào đây, kiểm tra ngay cell sau\n",
    "\n",
    "for city in cities:\n",
    "  # Lưu kết quả full cho từng thành phố\n",
    "  mix_train_test_result_cnn=pd.DataFrame()\n",
    "  mix_train_test_result_lstm=pd.DataFrame()\n",
    "  mix_train_test_result_lstm_att=pd.DataFrame()\n",
    "  try:\n",
    "\n",
    "    total_result = pd.DataFrame() # final result cho mỗi thành phố of all runninng time\n",
    "    total_metrix_result = pd.DataFrame() # final matrix result cho mỗi thành phố of all runninng time\n",
    "    \n",
    "    for interation in range(n_LoopEachExpRun):\n",
    "      \n",
    "      # Pre-process data\n",
    "      specific_data = get_city_data(fix_text(city))   \n",
    "      specific_data = impute_missing_value(specific_data)\n",
    "      specific_data = convert_to_stationary(specific_data)\n",
    "      specific_data.dropna(inplace=True)\n",
    "\n",
    "      specific_data_cnn  = specific_data\n",
    "      specific_data_lstm = specific_data\n",
    "      specific_data_lstm_att = specific_data\n",
    "      ########################################################\n",
    "      # Generate CNN model and define province-specific params\n",
    "      ########################################################\n",
    "      # Get right optimize parame\n",
    "      n_layers, hidden_size, num_filters, dropout, learning_rate, epoch, lookback_window = get_opt_param_by_name( algo = 'cnn', city = city)\n",
    "\n",
    "      train, test = split_data(specific_data_cnn, lookback_window)\n",
    "\n",
    "      # Fit data scaler to training data\n",
    "      full_scaler = MinMaxScaler().fit(train)\n",
    "      y_scaler = MinMaxScaler().fit(train.values[:, -1].reshape(-1, 1))\n",
    "\n",
    "      # Scale train and test data\n",
    "      train = full_scaler.transform(train)\n",
    "      test = full_scaler.transform(test)\n",
    "\n",
    "      \n",
    "      # Get data to run model\n",
    "      important_features, train_loader, test_tensor = get_data(train, test, args.batch_size, specific_data_cnn)    \n",
    "      \n",
    "\n",
    "      # Overwrite parameters for province-specific diarrhoea prediction\n",
    "      args.look_back = lookback_window\n",
    "      trainer = Trainer(\n",
    "                        model_type='cnn',  \n",
    "                        city = city,                    \n",
    "                        learning_rate=learning_rate,\n",
    "                        important_features=important_features,\n",
    "                        train_loader=train_loader,\n",
    "                        test_tensor=test_tensor,\n",
    "                        num_filters= num_filters, \n",
    "                        dropout=dropout )\n",
    "    \n",
    "      # Train model with province-specific epochs\n",
    "      trainer.train(epochs=epoch)           \n",
    "      \n",
    "\n",
    "      # Evaluate test set   \n",
    "      y_true_test, y_pred_test, rmse_list, mae_list, mape_list = trainer.evaluate_model(np_data=test, plot= False, scaled=True, city=city, y_scaler= y_scaler)\n",
    "      \n",
    "      # Evaluate train  set   \n",
    "      y_true_train, y_pred_train,  _, _, _, = trainer.evaluate_model(np_data=train, plot= False, scaled=True, city=city, y_scaler= y_scaler)\n",
    "      \n",
    "      # Lưu tất cả kết quả predicted train và predicted test\n",
    "      mix_train_test_result_cnn = mix_train_test_result_cnn.append(mix_results(city, 'cnn', specific_data_cnn, y_pred_test, y_pred_train,lookback_window,interation, rmse_list[0],mae_list[0],mape_list[0])) \n",
    "\n",
    "      cnn_reslt = pd.DataFrame({\n",
    "                              'City': city,\n",
    "                              'Alg_name': 'cnn',\n",
    "                              'Run Seq': interation,\n",
    "                              'Observed': y_true_test[0], \n",
    "                              '1-month': y_pred_test[0], \n",
    "                              '2-months': y_pred_test[1],\n",
    "                              '3-months': y_pred_test[2],\n",
    "                              'RMSE_1-month': rmse_list[0],\n",
    "                              'RMSE_2-month': rmse_list[1],\n",
    "                              'RMSE_3-month': rmse_list[2],\n",
    "                              'MAE_1-month': mae_list[0],\n",
    "                              'MAE_2-month': mae_list[1],\n",
    "                              'MAE_3-month': mae_list[2],\n",
    "                              'MAPE_1-month': mape_list[0],\n",
    "                              'MAPE_2-month': mape_list[1],\n",
    "                              'MAPE_3-month': mape_list[2]})\n",
    "      \n",
    "      cnn_metrix_reslt = pd.DataFrame({\n",
    "                              'City': city,\n",
    "                              'Alg_name': 'cnn',\n",
    "                              'Run Seq': interation,                          \n",
    "                              'RMSE_1-month': rmse_list[0],\n",
    "                              'RMSE_2-month': rmse_list[1],\n",
    "                              'RMSE_3-month': rmse_list[2],\n",
    "                              'MAE_1-month': mae_list[0],\n",
    "                              'MAE_2-month': mae_list[1],\n",
    "                              'MAE_3-month': mae_list[2],\n",
    "                              'MAPE_1-month': mape_list[0],\n",
    "                              'MAPE_2-month': mape_list[1],\n",
    "                              'MAPE_3-month': [mape_list[2]]}) # do có 1 dòng nên pandas cần index = 0\n",
    "      \n",
    "\n",
    "      #############################################################\n",
    "      # Generate LSTM-ATT model and define province-specific params\n",
    "      #############################################################\n",
    "      # Get right optimize parame\n",
    "      n_layers, hidden_size, num_filters, dropout, learning_rate, epoch, lookback_window = get_opt_param_by_name( algo = 'lstm_attention', city = city)\n",
    "\n",
    "      train, test = split_data(specific_data_lstm_att, lookback_window)\n",
    "\n",
    "      # Fit data scaler to training data\n",
    "      full_scaler = MinMaxScaler().fit(train)\n",
    "      y_scaler = MinMaxScaler().fit(train.values[:, -1].reshape(-1, 1))\n",
    "\n",
    "      # Scale train and test data\n",
    "      train = full_scaler.transform(train)\n",
    "      test = full_scaler.transform(test)\n",
    "\n",
    "      \n",
    "      # Get data to run model\n",
    "      important_features, train_loader, test_tensor = get_data(train, test, args.batch_size, specific_data_lstm_att) \n",
    "\n",
    "      # Overwrite parameters for province-specific diarrhoea prediction\n",
    "      args.look_back = lookback_window\n",
    "\n",
    "      trainer = Trainer(model_type= 'lstm_attention',\n",
    "                        city = city,\n",
    "                        learning_rate= learning_rate,\n",
    "                        important_features=important_features,\n",
    "                        train_loader=train_loader,\n",
    "                        test_tensor=test_tensor,\n",
    "                        n_layers= n_layers,\n",
    "                        hidden_size=hidden_size)\n",
    "\n",
    "      # Train model with province-specific epochs\n",
    "      trainer.train(epochs=epoch)\n",
    "      \n",
    "      # Evaluate test set   \n",
    "      y_true_test, y_pred_test, rmse_list, mae_list, mape_list = trainer.evaluate_model(np_data=test, plot= False, scaled=True, city=city, y_scaler= y_scaler)\n",
    "      \n",
    "      # Evaluate train  set   \n",
    "      y_true_train, y_pred_train,  _, _, _, = trainer.evaluate_model(np_data=train, plot= False, scaled=True, city=city, y_scaler= y_scaler)\n",
    "      \n",
    "      # Lưu tất cả kết quả predicted train và predicted test\n",
    "      mix_train_test_result_lstm_att = mix_train_test_result_lstm_att.append(mix_results(city,'lstm_att', specific_data_lstm_att, y_pred_test, y_pred_train,lookback_window,interation, rmse_list[0],mae_list[0],mape_list[0])) \n",
    "\n",
    "\n",
    "      lstm_attention_reslt = pd.DataFrame({\n",
    "                              'City': city,\n",
    "                              'Alg_name': 'lstm_attention',\n",
    "                              'Run Seq': interation,\n",
    "                              'Observed': y_true_test[0], \n",
    "                              '1-month': y_pred_test[0], \n",
    "                              '2-months': y_pred_test[1],\n",
    "                              '3-months': y_pred_test[2],\n",
    "                              'RMSE_1-month': rmse_list[0],\n",
    "                              'RMSE_2-month': rmse_list[1],\n",
    "                              'RMSE_3-month': rmse_list[2],\n",
    "                              'MAE_1-month': mae_list[0],\n",
    "                              'MAE_2-month': mae_list[1],\n",
    "                              'MAE_3-month': mae_list[2],\n",
    "                              'MAPE_1-month': mape_list[0],\n",
    "                              'MAPE_2-month': mape_list[1],\n",
    "                              'MAPE_3-month': mape_list[2]})\n",
    "      \n",
    "      lstm_attention_metrix_reslt = pd.DataFrame({\n",
    "                              'City': city,\n",
    "                              'Alg_name': 'lstm_attention',\n",
    "                              'Run Seq': interation,                          \n",
    "                              'RMSE_1-month': rmse_list[0],\n",
    "                              'RMSE_2-month': rmse_list[1],\n",
    "                              'RMSE_3-month': rmse_list[2],\n",
    "                              'MAE_1-month': mae_list[0],\n",
    "                              'MAE_2-month': mae_list[1],\n",
    "                              'MAE_3-month': mae_list[2],\n",
    "                              'MAPE_1-month': mape_list[0],\n",
    "                              'MAPE_2-month': mape_list[1],\n",
    "                              'MAPE_3-month': mape_list[2]}, index=[0]) # do có 1 dòng nên pandas cần index = 0\n",
    "\n",
    "\n",
    "      # #############################################################\n",
    "      # Generate LSTM model and define province-specific params\n",
    "      n_layers, hidden_size, num_filters, dropout, learning_rate, epoch, lookback_window = get_opt_param_by_name( algo = 'lstm', city = city)\n",
    "\n",
    "      train, test = split_data(specific_data_lstm, lookback_window)\n",
    "\n",
    "      # Fit data scaler to training data\n",
    "      full_scaler = MinMaxScaler().fit(train)\n",
    "      y_scaler = MinMaxScaler().fit(train.values[:, -1].reshape(-1, 1))\n",
    "\n",
    "      # Scale train and test data\n",
    "      train = full_scaler.transform(train)\n",
    "      test = full_scaler.transform(test)\n",
    "\n",
    "      \n",
    "      # Get data to run model\n",
    "      important_features, train_loader, test_tensor = get_data(train, test, args.batch_size, specific_data_lstm) \n",
    "\n",
    "      # Overwrite parameters for province-specific diarrhoea prediction\n",
    "      args.look_back = lookback_window\n",
    "\n",
    "      trainer = Trainer(model_type='lstm', \n",
    "                        city = city,\n",
    "                        learning_rate=learning_rate,\n",
    "                        important_features=important_features,\n",
    "                        train_loader=train_loader,\n",
    "                        test_tensor=test_tensor,\n",
    "                        n_layers=n_layers,\n",
    "                        hidden_size=hidden_size)   \n",
    "    \n",
    "      # Train model with province-specific epochs\n",
    "      trainer.train(epochs=epoch)\n",
    "\n",
    "      # Evaluate test set   \n",
    "      y_true_test, y_pred_test, rmse_list, mae_list, mape_list = trainer.evaluate_model(np_data=test, plot= False, scaled=True, city=city, y_scaler= y_scaler)\n",
    "      \n",
    "      # Evaluate train  set   \n",
    "      y_true_train, y_pred_train,  _, _, _, = trainer.evaluate_model(np_data=train, plot= False, scaled=True, city=city, y_scaler= y_scaler)\n",
    "      \n",
    "      # Lưu tất cả kết quả predicted train và predicted test, metrics all\n",
    "      mix_train_test_result_lstm = mix_train_test_result_lstm.append(mix_results(city, 'lstm', specific_data_lstm, y_pred_test, y_pred_train,lookback_window,interation, rmse_list[0],mae_list[0],mape_list[0])) \n",
    "\n",
    "      # Lưu full kết quả predict và metrics      \n",
    "      lstm_reslt = pd.DataFrame({\n",
    "                              'City': city,\n",
    "                              'Alg_name': 'lstm',\n",
    "                              'Run Seq': interation,\n",
    "                              'Observed': y_true_test[0], \n",
    "                              '1-month': y_pred_test[0], \n",
    "                              '2-months': y_pred_test[1],\n",
    "                              '3-months': y_pred_test[2],\n",
    "                              'RMSE_1-month': rmse_list[0],\n",
    "                              'RMSE_2-month': rmse_list[1],\n",
    "                              'RMSE_3-month': rmse_list[2],\n",
    "                              'MAE_1-month': mae_list[0],\n",
    "                              'MAE_2-month': mae_list[1],\n",
    "                              'MAE_3-month': mae_list[2],\n",
    "                              'MAPE_1-month': mape_list[0],\n",
    "                              'MAPE_2-month': mape_list[1],\n",
    "                              'MAPE_3-month': mape_list[2]})\n",
    "\n",
    "      # Lưu full kết quả metrics thôi\n",
    "      lstm_metrix_reslt = pd.DataFrame({\n",
    "                              'City': city,\n",
    "                              'Alg_name': 'lstm',\n",
    "                              'Run Seq': interation,                          \n",
    "                              'RMSE_1-month': rmse_list[0],\n",
    "                              'RMSE_2-month': rmse_list[1],\n",
    "                              'RMSE_3-month': rmse_list[2],\n",
    "                              'MAE_1-month': mae_list[0],\n",
    "                              'MAE_2-month': mae_list[1],\n",
    "                              'MAE_3-month': mae_list[2],\n",
    "                              'MAPE_1-month': mape_list[0],\n",
    "                              'MAPE_2-month': mape_list[1],\n",
    "                              'MAPE_3-month': mape_list[2]}, index=[0]) # do có 1 dòng nên pandas cần index = 0\n",
    "      \n",
    "      \n",
    "\n",
    "      total_result = total_result.append(pd.DataFrame(cnn_reslt))\n",
    "      total_result = total_result.append(pd.DataFrame(lstm_attention_reslt))\n",
    "      total_result = total_result.append(pd.DataFrame(lstm_reslt))\n",
    "\n",
    "      total_metrix_result = total_metrix_result.append(pd.DataFrame(cnn_metrix_reslt))\n",
    "      total_metrix_result = total_metrix_result.append(pd.DataFrame(lstm_attention_metrix_reslt))\n",
    "      total_metrix_result = total_metrix_result.append(pd.DataFrame(lstm_metrix_reslt))\n",
    "      \n",
    "    # Nếu tron n lần chạy 3 bộ giải thuật không có lỗi gì thì:\n",
    "      # Lưu xuống file excel ứng với thành phố và 3 giải thuật deep learning tương ứng\n",
    "      # Lưu xuống mix predicted train và test data ứng với từng thành phố/ từng giải thuật\n",
    "    total_result.to_excel(path+'diarrhoea_'+city+'_'+str(expRuntime)+'.xlsx')  \n",
    "    total_metrix_result.to_excel(path_metrix+'diarrhoea_metrix_'+city+'_'+str(expRuntime)+'.xlsx')\n",
    "\n",
    "    # Lưu xuống mix predicted train và test data ứng với từng thành phố/ từng giải thuật\n",
    "    save_results(mix_train_test_result_cnn,'cnn', city)\n",
    "    save_results(mix_train_test_result_lstm,'lstm', city)\n",
    "    save_results(mix_train_test_result_lstm_att,'lstm_att', city)\n",
    "  except Exception as e:\n",
    "    l_errCity[city] = e\n",
    "    break    \n",
    "\n",
    "\n",
    "dt_ended = datetime.now()\n",
    "print('kết thúc train trong:', round((dt_ended - dt_started).total_seconds()/60))\n",
    "print(l_errCity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "BujV4MM-yasc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"ok\":true,\"result\":{\"message_id\":447,\"sender_chat\":{\"id\":-1001712314864,\"title\":\"PTN_Announcement\",\"username\":\"ptn_announcement\",\"type\":\"channel\"},\"chat\":{\"id\":-1001712314864,\"title\":\"PTN_Announcement\",\"username\":\"ptn_announcement\",\"type\":\"channel\"},\"date\":1676128121,\"text\":\"train + predict xong\"}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def send_to_telegram(message):\n",
    "\n",
    "    apiToken = '5908735099:AAGVSLrW62aXPBP-GrMvxoVgMsuJxXJpP1Q'\n",
    "    chatID = '@ptn_announcement'\n",
    "    apiURL = f'https://api.telegram.org/bot{apiToken}/sendMessage'\n",
    "\n",
    "    try:\n",
    "        response = requests.post(apiURL, json={'chat_id': chatID, 'text': message})\n",
    "        print(response.text)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "send_to_telegram(\"train + predict xong\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "irOFnBgCodUS"
   ],
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "290.909px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "6f5583cf1d9466b5c27e75c89cc6b383bed5736d6b16c51c8074d8690011a952"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
