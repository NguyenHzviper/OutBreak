{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Fw31Za_INKC"
   },
   "source": [
    "# Transformer Prediction Model Diarrhoea Version 3\n",
    "Date: 28/11/2022\n",
    "Description: Transformer\n",
    "Data: new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7734,
     "status": "ok",
     "timestamp": 1675130644251,
     "user": {
      "displayName": "Nhat Le",
      "userId": "00702307310725808810"
     },
     "user_tz": -420
    },
    "id": "VW4v2IEcTYHa",
    "outputId": "f9f36e19-cbaa-4d76-b3c9-e022515d33b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /home/mlworker/lib/python3.10/site-packages (1.2.1)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /home/mlworker/lib/python3.10/site-packages (from scikit-learn) (1.8.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/mlworker/lib/python3.10/site-packages (from scikit-learn) (1.23.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/mlworker/lib/python3.10/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/mlworker/lib/python3.10/site-packages (from scikit-learn) (1.2.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: ftfy in /home/mlworker/lib/python3.10/site-packages (6.1.1)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /home/mlworker/lib/python3.10/site-packages (from ftfy) (0.2.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install packages\n",
    "!pip install -U scikit-learn\n",
    "!pip install ftfy\n",
    "# !pip install xlsxwriter \n",
    "# !pip install xlwt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1675130644252,
     "user": {
      "displayName": "Nhat Le",
      "userId": "00702307310725808810"
     },
     "user_tz": -420
    },
    "id": "b8I8S9koHjT_"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import tensorflow as tf \n",
    "# from google.colab import drive\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.models import Sequential, model_from_json\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "import traceback\n",
    "import sys\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from ftfy import fix_text\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [7, 3]\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2jYmnLTWfmXT"
   },
   "source": [
    "# Input path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3889,
     "status": "ok",
     "timestamp": 1675130648135,
     "user": {
      "displayName": "Nhat Le",
      "userId": "00702307310725808810"
     },
     "user_tz": -420
    },
    "id": "MNAuUk6_HVs1",
    "outputId": "f141850b-857c-47e8-a06f-88ec871f1296"
   },
   "outputs": [],
   "source": [
    "# Attach Google Drive for reading and saving files\n",
    "# drive.mount('/content/drive')\n",
    "prj_path = '/home/mlworker/Quang/HealthCare/Source_14012023_v4/'\n",
    "prj_path_opt=  prj_path + '/optimize_hyperparam/opt_results/'\n",
    "data_path = prj_path + '/data/new_data/DH/squeezed/'\n",
    "\n",
    "# result path\n",
    "path =  prj_path + '/results/'\n",
    "path_metrix =  prj_path + '/results/metrix/'\n",
    "\n",
    "path_all_mix_cnn= prj_path + 'results/all_mix_data/cnn/'\n",
    "path_all_mix_lstm= prj_path + 'results/all_mix_data/lstm/'\n",
    "path_all_mix_lstm_att= prj_path + 'results/all_mix_data/lstm_att/'\n",
    "path_all_mix_tf= prj_path + 'results/all_mix_data/transformer/'\n",
    "\n",
    "os.chdir(prj_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kVZZNdDFoH_m"
   },
   "source": [
    "# Class Train and Support Functions for Preprocess\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1675130648136,
     "user": {
      "displayName": "Nhat Le",
      "userId": "00702307310725808810"
     },
     "user_tz": -420
    },
    "id": "fVkHGeUPO6cO"
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    import random, os\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    \n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1675130648136,
     "user": {
      "displayName": "Nhat Le",
      "userId": "00702307310725808810"
     },
     "user_tz": -420
    },
    "id": "lEn56jmQtEmD"
   },
   "outputs": [],
   "source": [
    "# Set hyperparameters as args using the Configuration class\n",
    "class Configuration():\n",
    "    def __init__(self):\n",
    "        # lấy bộ test dài 24 tháng = 2 năm\n",
    "        self.test_size = 24\n",
    "        # là nhìn vào dữ liệu trước 3 tháng và dự phóng        \n",
    "        self.look_back = 3\n",
    "        # dự phóng 3 tháng\n",
    "        self.n_predicted_month = 3\n",
    "        self.n_features = 3\n",
    "        self.seed = 42\n",
    "        # mỗi phần tử x trong tập suppervise có độ lớn là 16 = 16 tháng\n",
    "        self.batch_size = 16\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # self.device = torch.device(\"cpu\")\n",
    "        self.epochs = 300\n",
    "\n",
    "args = Configuration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zcb85xutVvxF"
   },
   "source": [
    "# Pre-processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 4717,
     "status": "ok",
     "timestamp": 1675130652849,
     "user": {
      "displayName": "Nhat Le",
      "userId": "00702307310725808810"
     },
     "user_tz": -420
    },
    "id": "kTzhmwZKP50G"
   },
   "outputs": [],
   "source": [
    "cities = ['An Giang', 'BR Vũng Tàu', 'Bình Phước', 'Bình Thuận', 'Bình Định',\n",
    "        'Bạc Liêu', 'Bắc Kạn', 'Bắc Giang', 'Cao Bằng', 'Cà Mau',\n",
    "        'Cần Thơ', 'Gia Lai', 'Hà Giang', 'Hà Nội', 'Hà Tĩnh', 'Hòa Bình',\n",
    "        'Hưng Yên', 'Hải Dương', 'Hải Phòng', 'Khánh Hòa', 'Kiên Giang',\n",
    "        'Kon Tum', 'Lai Châu', 'Long An', 'Lào Cai', 'Lâm Đồng',\n",
    "        'Lạng Sơn', 'Nam Định', 'Nghệ An', 'Ninh Bình', 'Ninh Thuận',\n",
    "        'Phú Thọ', 'Phú Yên', 'Quảng Bình', 'Quảng Nam', 'Quảng Ngãi',\n",
    "        'Quảng Ninh', 'Quảng Trị', 'Sóc Trăng', 'Sơn La', 'TT Huế',\n",
    "        'Thanh Hóa', 'Thái Bình', 'Thái Nguyên', 'Tiền Giang', 'Trà Vinh',\n",
    "        'Tuyên Quang', 'Tây Ninh', 'Vĩnh Phúc', 'Yên Bái', 'Điện Biên',\n",
    "         'Đà Nẵng', 'Đắk Nông', 'Đắk Lắk', 'Đồng Tháp']\n",
    "\n",
    "def get_dict_all_city_data():\n",
    "  cities_data = {}  \n",
    "  for city in cities:\n",
    "    city_result = pd.read_excel(data_path+'squeezed_'+city+'.xlsx')  \n",
    "    # Đoạn này rất quan trọng. Optimize không được đụng vào 36 tháng (2015-2017) để dự báo. \n",
    "    # Lúc train và predict thì ok. Lấy từ 1997 -2017. \n",
    "    # Function split data sẽ lấy 3 năm cuối làm test data theo config     \n",
    "    city_result = city_result.loc[city_result['year_month'] < '2016-1-1'] \n",
    "    cities_data[city] = city_result\n",
    "  return cities_data\n",
    "\n",
    "dict_full_data = get_dict_all_city_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1675130652850,
     "user": {
      "displayName": "Nhat Le",
      "userId": "00702307310725808810"
     },
     "user_tz": -420
    },
    "id": "ukoPuS5CHvdl"
   },
   "outputs": [],
   "source": [
    "# Define data (pre-)processing functions\n",
    "# modification\n",
    "def get_city_data(city_name):\n",
    "    \"\"\"Returns Diarrhoea rate and climate data only\"\"\" \n",
    "    city_data = dict_full_data[city_name].drop(columns=['Diarrhoea_cases', 'province', 'year_month',\n",
    "                                                        'Influenza_rates','Dengue_fever_rates',\n",
    "                                                        'Influenza_cases','Dengue_fever_cases', 'year', 'month',\n",
    "                                                        'population_male','population_female','population_urban',\n",
    "                                                        'population_countryside','area','population_density',\n",
    "                                                        'population_average','birth_rate','urban_water_usage_rate',\n",
    "                                                        'clean_water_rate_all','poverty_rate','toilet_rate'\n",
    "                                                        ], \n",
    "                                                                  axis=1, \n",
    "                                                                  inplace=False)    \n",
    "    return city_data\n",
    "\n",
    "\n",
    "def convert_to_stationary(city_data):\n",
    "    \"\"\"Subtracts previous value for all cols except disease rates\"\"\"\n",
    "    for col_name in city_data.columns:\n",
    "        if col_name != 'Diarrhoea_rates':\n",
    "            try:\n",
    "                city_data[col_name] = city_data[col_name] - city_data[col_name].shift()\n",
    "            except:\n",
    "                print(col_name)\n",
    "    return city_data\n",
    "\n",
    "def impute_missing_value(city_data):\n",
    "    \"\"\"\n",
    "    Cơ bản dữ liệu bị thiếu sót rất nhiều: Như Điện Biên 1997 -2003 là thiếu dữ liệu về bệnh\n",
    "    Hàm này sẽ tự sinh ra dữ liệu bị thiếu. Nếu tháng nào không có số liệu thì tính như sau:\n",
    "    12 tháng đầu không có số liệu thì gán = 0\n",
    "    tháng 13-24 không có số liệu, sẽ lấy giá trị của tháng cùng kỳ năm trước\n",
    "    tháng từ 24 trở đi sẽ lấy giá trị nhỏ nhất của 2 tháng cùng kỳ trong 2 năm gần nhất.\n",
    "    Do Điện Biên bằng 0 nên sau khi xử lý từ 1997 -2003 là đều = 0.  \n",
    "    \"\"\"\n",
    "    for col in city_data.columns:\n",
    "        for index in range(len(city_data[col])):\n",
    "            if np.isnan(city_data[col].iloc[index]):\n",
    "                if index < 12:\n",
    "                    city_data[col].iloc[index] = 0\n",
    "                elif index >= 12 and index <= 24:\n",
    "                    city_data[col].iloc[index] = city_data[col].iloc[index - 12]\n",
    "                else:\n",
    "                    city_data[col].iloc[index] = min(city_data[col].iloc[index - 12], city_data[col].iloc[index - 24])\n",
    "    return city_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1675130652850,
     "user": {
      "displayName": "Nhat Le",
      "userId": "00702307310725808810"
     },
     "user_tz": -420
    },
    "id": "oFMprYqPQTn5"
   },
   "outputs": [],
   "source": [
    "dict_full_data['Tiền Giang']\n",
    "city_TienGiang = get_city_data('Tiền Giang')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1675130652850,
     "user": {
      "displayName": "Nhat Le",
      "userId": "00702307310725808810"
     },
     "user_tz": -420
    },
    "id": "RT8LmtHts4fQ"
   },
   "outputs": [],
   "source": [
    "def split_data(data, look_back ):\n",
    "    \"\"\"Splits data into train and test sets based on args (Configuration class)\"\"\"\n",
    "    train = data[: -args.test_size]\n",
    "    print('lookback', look_back)\n",
    "    test = data[-args.test_size - look_back: ]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1675130652850,
     "user": {
      "displayName": "Nhat Le",
      "userId": "00702307310725808810"
     },
     "user_tz": -420
    },
    "id": "6Wdc44U0uMbP"
   },
   "outputs": [],
   "source": [
    "def to_supervised(data, d_in=args.look_back, d_out=args.n_predicted_month, features_list=[]):\n",
    "    \"\"\"\n",
    "    Frames time-series as supervised learning dataset.\n",
    "    \n",
    "    Args:\n",
    "      d_in: lookback window\n",
    "      d_out: number of predicted months\n",
    "      features_list: list of all features **where last col is the disease incidence**\n",
    "\n",
    "    Returns:\n",
    "      Numpy arrays of disease incidence (y) and other predictors (X)\n",
    "    \"\"\"\n",
    "    X, y = list(), list()\n",
    "    for index, _ in enumerate(data):\n",
    "        in_end = index + d_in\n",
    "        out_end = in_end + d_out\n",
    "        if out_end <= len(data):\n",
    "            if len(features_list) == 0 :\n",
    "                X.append(data[index: in_end, :])\n",
    "            else:\n",
    "                X.append(data[index: in_end, features_list])\n",
    "            y.append(data[in_end: out_end, -1])\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1675130652851,
     "user": {
      "displayName": "Nhat Le",
      "userId": "00702307310725808810"
     },
     "user_tz": -420
    },
    "id": "x_RfqYtVGTAk"
   },
   "outputs": [],
   "source": [
    "def select_feature(train, specific_data):\n",
    "    \"\"\"Selects args.n_features top features using RFE\"\"\"\n",
    "    train_X, train_y = to_supervised(train, d_in=1, d_out=1)\n",
    "    train_X, train_y = np.squeeze(train_X), np.squeeze(train_y)\n",
    "    rfe = RFE(RandomForestRegressor(n_estimators=500, random_state=args.seed), n_features_to_select=args.n_features)\n",
    "    fit = rfe.fit(train_X, train_y)\n",
    "    important_features = list()\n",
    "    print(\"Important Feature:\")\n",
    "    for i in range(len(fit.support_)):\n",
    "        if fit.support_[i]:\n",
    "            important_features.append(i)\n",
    "            print(specific_data.columns[i])\n",
    "    return np.array(important_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1675130652851,
     "user": {
      "displayName": "Nhat Le",
      "userId": "00702307310725808810"
     },
     "user_tz": -420
    },
    "id": "OsgC3mbHV96C"
   },
   "outputs": [],
   "source": [
    "def get_data(train_np, test_np, batch_size, specific_data):\n",
    "    \"\"\"\n",
    "    Returns important feature list and data formatted for input into Pytorch \n",
    "    models\n",
    "    \"\"\"\n",
    "    important_features = select_feature(train_np, specific_data)\n",
    "\n",
    "    train_X, train_y = to_supervised(train_np, features_list=important_features)\n",
    "    test_X, test_y = to_supervised(test_np, features_list=important_features)\n",
    "    train_tensor = TensorDataset(torch.from_numpy(train_X), torch.from_numpy(train_y))\n",
    "    test_tensor = (torch.from_numpy(test_X), torch.from_numpy(test_y))\n",
    "\n",
    "    train_loader = DataLoader(train_tensor, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return important_features, train_loader, test_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1675130652851,
     "user": {
      "displayName": "Nhat Le",
      "userId": "00702307310725808810"
     },
     "user_tz": -420
    },
    "id": "PW18DMZgF5K_"
   },
   "outputs": [],
   "source": [
    "#Define Pytorch LSTM model\n",
    "class MultiVariateLSTM(nn.Module):\n",
    "    def __init__(self, n_feature=3, n_layers=2, hidden_size=50):\n",
    "        super(MultiVariateLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=n_feature, hidden_size=hidden_size, num_layers=n_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, args.n_predicted_month)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.sigma = nn.Parameter(torch.ones(args.n_predicted_month))\n",
    "    \n",
    "    def forward(self, X_batch, y_batch=None):\n",
    "        output, (last_hidden, _) = self.lstm(X_batch)\n",
    "        last_hidden_vector = output[:, -1, :]\n",
    "        y_predicted = self.linear(last_hidden_vector)\n",
    "        if y_batch != None:\n",
    "            assert y_predicted.size() == y_batch.size()\n",
    "            loss = self.loss_fn(y_predicted, y_batch)\n",
    "            loss = 0.5 * loss / self.sigma**2\n",
    "            loss = loss.sum() + torch.log(1 + self.sigma.prod())\n",
    "            return y_predicted, loss\n",
    "            #return y_predicted, self.loss_fn(y_predicted, y_batch)\n",
    "        else:\n",
    "            return y_predicted\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = torch.tensor(X, device=args.device)\n",
    "        return self.forward(X)\n",
    "\n",
    "#Define Pytorch LSTM-ATT model\n",
    "class MultiVariateLSTM_Attention(nn.Module):\n",
    "    def __init__(self, n_feature=3, n_layers=2, hidden_size=50):\n",
    "        super(MultiVariateLSTM_Attention, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=n_feature, hidden_size=hidden_size, num_layers=n_layers, batch_first=True)\n",
    "        self.attention_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        # self.linear = nn.Linear(hidden_size*2, args.n_predicted_month)\n",
    "        self.linear = nn.Linear(hidden_size, args.n_predicted_month)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.sigma = nn.Parameter(torch.ones(args.n_predicted_month))\n",
    "    \n",
    "    def forward(self, X_batch, y_batch=None):\n",
    "        output, (last_hidden, _) = self.lstm(X_batch)\n",
    "        last_hidden_vector = last_hidden[-1]\n",
    "        remain_hidden_vector = output\n",
    "        e_t = remain_hidden_vector.bmm(self.attention_linear(last_hidden_vector).unsqueeze(2)).squeeze(-1)\n",
    "        alpha_t = F.softmax(e_t, dim=1)\n",
    "        attenion_vector = remain_hidden_vector.transpose(2, 1).bmm(alpha_t.unsqueeze(2)).squeeze(-1)\n",
    "        # combine_vector = torch.cat((last_hidden_vector, attenion_vector), dim=1)\n",
    "        # combine_vector = last_hidden_vector + attenion_vector\n",
    "        y_predicted = self.linear(attenion_vector)\n",
    "        if y_batch != None:\n",
    "            assert y_predicted.size() == y_batch.size()\n",
    "            loss = self.loss_fn(y_predicted, y_batch)\n",
    "            loss = 0.5 * loss / self.sigma**2\n",
    "            loss = loss.sum() + torch.log(1 + self.sigma.prod())\n",
    "            return y_predicted, loss\n",
    "            # return y_predicted, self.loss_fn(y_predicted, y_batch)\n",
    "        else:\n",
    "            return y_predicted\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = torch.tensor(X, device=args.device)\n",
    "        return self.forward(X)\n",
    "\n",
    "# Define Pytorch CNN model\n",
    "class MultivariateCNN(nn.Module):\n",
    "    def __init__(self, num_filters=[100, 100, 100], dropout=0.01):\n",
    "        super(MultivariateCNN, self).__init__()\n",
    "        self.loss_fn = loss = nn.MSELoss()\n",
    "        self.filter_sizes = [1, 2, 3]\n",
    "        self.conv1d_list = nn.ModuleList([nn.Conv1d(args.n_features, num_filters[i], self.filter_sizes[i]) for i in range(len(self.filter_sizes))])\n",
    "        self.linear = nn.Linear(np.sum(num_filters), args.n_predicted_month)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.sigma = nn.Parameter(torch.ones(args.n_predicted_month))\n",
    "\n",
    "    def forward(self, X_batch, y_batch=None):\n",
    "        X_batch = X_batch.permute(0, 2, 1)  #(batch_size, n_features, n_look_back)\n",
    "        X_conv_list = [F.relu(conv1d(X_batch)) for conv1d in self.conv1d_list]\n",
    "        X_pool_list = [F.max_pool1d(x_conv, kernel_size=x_conv.shape[2]) for x_conv in X_conv_list]\n",
    "        X_fc = torch.cat([x_pool.squeeze(dim=2) for x_pool in X_pool_list], dim=1)\n",
    "        y_predicted = self.linear(self.dropout(X_fc))\n",
    "        if y_batch != None:\n",
    "            assert y_predicted.size() == y_batch.size()\n",
    "            loss = self.loss_fn(y_predicted, y_batch)\n",
    "            loss = 0.5 * loss / self.sigma**2\n",
    "            loss = loss.sum() + torch.log(1 + self.sigma.prod())\n",
    "            return y_predicted, loss\n",
    "            # return y_predicted, self.loss_fn(y_predicted, y_batch)\n",
    "        else:\n",
    "            return y_predicted\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = torch.tensor(X, device=args.device)\n",
    "        return self.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1675130652851,
     "user": {
      "displayName": "Nhat Le",
      "userId": "00702307310725808810"
     },
     "user_tz": -420
    },
    "id": "c5hC3vWmVHv1"
   },
   "outputs": [],
   "source": [
    "# Transformer Model\n",
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, d_model=3, n_feature=3, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        pe = torch.zeros(n_feature, d_model)\n",
    "        for pos in range(n_feature):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = math.sin(pos/(10000**(2*i/d_model)))\n",
    "                if i + 1 < d_model:\n",
    "                    pe[pos, i+1] = math.cos(pos/(10000**((2*i+1)/d_model)))\n",
    "        pe = pe.unsqueeze(0)        \n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x*math.sqrt(self.d_model)\n",
    "        length = x.size(1)\n",
    "        pe = Variable(self.pe[:, :length], requires_grad=False)\n",
    "        if x.is_cuda:\n",
    "            pe.cuda()\n",
    "        x = x + pe\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, d_input, n_head, hidden_size, n_layers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.pe = PositionalEncoder(dropout=dropout)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=d_input, nhead=n_head, dim_feedforward=hidden_size, dropout=dropout, activation='gelu')\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, n_layers)\n",
    "        self.decoder = nn.Linear(d_input*n_head, args.n_predicted_month)\n",
    "        self.sigma = nn.Parameter(torch.ones(args.n_predicted_month))\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "    \n",
    "    def forward(self, X_batch, y_batch=None):\n",
    "        X_batch = self.pe(X_batch)\n",
    "        X_batch = self.transformer_encoder(X_batch)\n",
    "        X_batch = X_batch.view(X_batch.size(0), -1)\n",
    "        \n",
    "        y_predicted = self.decoder(X_batch)\n",
    "        if y_batch != None:\n",
    "            assert y_predicted.size() == y_batch.size()\n",
    "            loss = self.loss_fn(y_predicted, y_batch)\n",
    "            loss = 0.5 * loss / self.sigma**2\n",
    "            loss = loss.sum() + torch.log(1 + self.sigma.prod())\n",
    "            return y_predicted, loss\n",
    "        else:\n",
    "            return y_predicted\n",
    "        return X_batch\n",
    "    \n",
    "    def predict(self, X):\n",
    "        with torch.no_grad():\n",
    "            X = torch.tensor(X, device=args.device)\n",
    "        return self.forward(X).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1675130652851,
     "user": {
      "displayName": "Nhat Le",
      "userId": "00702307310725808810"
     },
     "user_tz": -420
    },
    "id": "lWNY7faYe52P"
   },
   "outputs": [],
   "source": [
    "# Create class to train and evaluate models\n",
    "class Trainer():\n",
    "    def __init__(self, city, model_type, learning_rate, important_features, train_loader, test_tensor, n_layers=2, hidden_size=128, num_filters=[100, 100, 100], dropout=0.01):\n",
    "        \"\"\"\n",
    "        Initialise trainer, allowing input of LSTM, LSTM-ATT, or CNN \n",
    "        hyperparameters. Adam optimiser used for all models.\n",
    "        \"\"\"\n",
    "        self.model_type = model_type    \n",
    "        self.city = city    \n",
    "        self.model = self.init_model(model_type, n_layers, hidden_size, num_filters, dropout, city)\n",
    "        self.model.double().to(args.device)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        self.important_features, self.train_loader, self.test_tensor = important_features, train_loader, test_tensor\n",
    "    \n",
    "    def init_model(self, model_type, n_layers, hidden_size, num_filters, dropout, city):\n",
    "        \"\"\"Initialise a model based on whether LSTM, LSTM-ATT, or CNN is chosen.\"\"\"\n",
    "        if model_type.lower() == 'lstm':\n",
    "            model = MultiVariateLSTM(args.n_features, n_layers, hidden_size)\n",
    "        elif model_type.lower() == 'lstm_attention':\n",
    "            model = MultiVariateLSTM_Attention(args.n_features, n_layers, hidden_size)\n",
    "        elif model_type.lower() == 'cnn':\n",
    "            model = MultivariateCNN(num_filters, dropout)\n",
    "        elif model_type.lower() == 'transformers':\n",
    "            model = TransformerModel(d_input=args.look_back, n_head=3, hidden_size=hidden_size, n_layers=n_layers, dropout=dropout)   \n",
    "        return model\n",
    "\n",
    "    def step(self, batch):\n",
    "        self.model.train()\n",
    "        X_batch, y_batch = tuple(t.to(args.device) for t in batch)\n",
    "        self.optimizer.zero_grad()\n",
    "        y_pred, loss = self.model.forward(X_batch, y_batch)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.mean().item()\n",
    "\n",
    "    def validation(self):\n",
    "        self.model.eval()\n",
    "        eval_loss = 0.0\n",
    "\n",
    "        result = {}\n",
    "\n",
    "        y_true = np.array([])\n",
    "        y_pred = np.array([])\n",
    "\n",
    "        X_batch, y_batch = tuple(t.to(args.device) for t in self.test_tensor)\n",
    "        with torch.no_grad():\n",
    "            outputs, loss = self.model.forward(X_batch, y_batch)\n",
    "            eval_loss = loss.mean().item()\n",
    "\n",
    "        return eval_loss\n",
    "\n",
    "    def train(self, epochs=20):\n",
    "        best_lost = float(\"inf\")\n",
    "        best_model = None\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0.0\n",
    "            for batch in self.train_loader:\n",
    "                loss = self.step(batch)\n",
    "                total_loss += loss\n",
    "            train_loss = total_loss/len(self.train_loader)\n",
    "            eval_loss = self.validation()\n",
    "            if eval_loss < best_lost:\n",
    "                best_lost = eval_loss\n",
    "                best_model = copy.deepcopy(self.model)\n",
    "            if (epoch + 1) == epochs or (epoch + 1) in [c + 1 for c in range(epochs) if c % int(epochs/4) == 0]:\n",
    "                print(f\"Epoch: {epoch:2}/{epochs:2} - train_loss: {train_loss:.4f} - test_loss: {eval_loss:4f}\")\n",
    "        self.model = best_model\n",
    "        self.model.eval()\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "    # Lưu model vào trong thư mục models\n",
    "    def save_model_to(self, path = '', city =''):       \n",
    "        torch.save(self.model, path)\n",
    "\n",
    "    def load_model_to(self, path = ''):       \n",
    "        return torch.load(path)\n",
    "\n",
    "    def evaluate_model(self, np_data=None, plot=True, scaled=True, city=None, k_steps=None, y_scaler =None):\n",
    "        assert scaled, \"data must be scaled\"\n",
    "        self.model.eval()\n",
    "        tensor_data = torch.from_numpy(np_data)\n",
    "        rmse_list = []\n",
    "        mae_list = [] \n",
    "        mape_list = []\n",
    "\n",
    "        y_predicted_list = []\n",
    "        y_true_list = []\n",
    "\n",
    "        for k_steps in range(1, args.n_predicted_month + 1):\n",
    "            y_predicted = []\n",
    "            y_true = []\n",
    "            for index in range(tensor_data.size(0) - args.look_back):\n",
    "                X = tensor_data[index: index + args.look_back, self.important_features]\n",
    "                # yhat = self.model.predict(X.unsqueeze(0)).squeeze()\n",
    "\n",
    "                yhat = self.model.predict(X.unsqueeze(0))\n",
    "                yhat = yhat.squeeze()\n",
    "\n",
    "                y_predicted.append(yhat.detach().cpu().numpy()[k_steps - 1])\n",
    "                y_true.append(tensor_data[index + args.look_back, -1].detach().cpu().numpy())\n",
    "\n",
    "            y_predicted = y_scaler.inverse_transform(np.array(y_predicted).reshape(-1, 1)).reshape(-1, )\n",
    "            y_true = y_scaler.inverse_transform(np.array(y_true).reshape(-1, 1)).reshape(-1, )\n",
    "\n",
    "            \n",
    "\n",
    "            rmse = mean_squared_error(y_true, y_predicted, squared=False)\n",
    "            mae = mean_absolute_error(y_true, y_predicted)\n",
    "            mape = mean_absolute_percentage_error(y_true, y_predicted)\n",
    "\n",
    "            rmse_list.append(rmse)\n",
    "            mae_list.append(mae)\n",
    "            mape_list.append(mape)\n",
    "\n",
    "            print('City: '+self.city+'  _algo:'+self.model_type+'  -RMSE: '+str(rmse))          \n",
    "            if plot==True:\n",
    "              plt.grid(True)\n",
    "              plt.plot(y_predicted, label='predicted')\n",
    "              plt.plot(y_true, label='actual')\n",
    "              plt.title(f\"k-steps = {k_steps} - city: \"+self.city+'  _algo:'+self.model_type+'  -RMSE: '+str(rmse))\n",
    "              plt.legend()\n",
    "              plt.show()\n",
    "\n",
    "              plt.show()\n",
    "            y_predicted_list.append(y_predicted)\n",
    "            y_true_list.append(y_true)\n",
    "\n",
    "        return y_true_list, y_predicted_list, rmse_list, mae_list, mape_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1675130652851,
     "user": {
      "displayName": "Nhat Le",
      "userId": "00702307310725808810"
     },
     "user_tz": -420
    },
    "id": "DblQQmN0NWv2"
   },
   "outputs": [],
   "source": [
    "def convert_str_num_filter_array(str_filter = ''):  \n",
    "  arr_filter = str_filter.split(',')  \n",
    "  int_arr_filter= [int(x) for x in arr_filter]\n",
    "  return int_arr_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0nf-w3k5rPZ9"
   },
   "source": [
    "# Get Optimized HyperParam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1675130654391,
     "user": {
      "displayName": "Nhat Le",
      "userId": "00702307310725808810"
     },
     "user_tz": -420
    },
    "id": "t9dJEoLMsfhl"
   },
   "outputs": [],
   "source": [
    "\n",
    "# prj_path_opt= '/content/drive/MyDrive/HealthCare/Source/optimize_hyperparam/opt_results/'\n",
    "# opt_param_cnn = pd.read_excel(prj_path_opt+'diarrhoea_opt_hyperparam_cnn.xlsx', index_col = 'City')\n",
    "# opt_param_lstm = pd.read_excel(prj_path_opt+'diarrhoea_opt_hyperparam_lstm.xlsx', index_col = 'City')\n",
    "# opt_param_lstm_att = pd.read_excel(prj_path_opt+'diarrhoea_opt_hyperparam_lstm_att.xlsx', index_col = 'City')\n",
    "opt_param_transformer = pd.read_excel(prj_path_opt+'/transformer_30012023_v4/diarrhoea_opt_hyperparam_transformer.xlsx', index_col = 'City')\n",
    "# opt_param_transformer = pd.read_excel(prj_path_opt+'/diarrhoea_opt_hyperparam_transformer.xlsx', index_col = 'City')\n",
    "\n",
    "def get_opt_param_by_name(algo = 'transformer', city = None):\n",
    "  # if algo == 'cnn':\n",
    "  #   city_param = opt_param_cnn.loc[city]\n",
    "  #   #load  param\n",
    "  #   n_layers = ''\n",
    "  #   hidden_size = ''\n",
    "  #   num_filters = convert_str_num_filter_array(city_param['Num. filters'])\n",
    "  #   dropout = float(city_param['Dropout rate'])\n",
    "  #   learning_rate = float(city_param['Learning rate'])\n",
    "  #   epoch = int(city_param['Epochs'])\n",
    "  #   lookback_window = int(city_param['Lookback Window'])\n",
    "  # elif algo == 'lstm':\n",
    "  #   city_param = opt_param_lstm.loc[city]\n",
    "  #   #load  param\n",
    "  #   n_layers = int(city_param['n Layers'])\n",
    "  #   hidden_size = int(city_param['Hiden Size'])\n",
    "  #   num_filters = ''\n",
    "  #   dropout = ''\n",
    "  #   learning_rate = float(city_param['Learning rate'])\n",
    "  #   epoch = int(city_param['Epochs'])\n",
    "  #   lookback_window = int(city_param['Lookback Window'])\n",
    "  # elif algo == 'lstm_attention':\n",
    "  #   city_param = opt_param_lstm_att.loc[city]\n",
    "  #   #load  param\n",
    "  #   n_layers = int(city_param['n Layers'])\n",
    "  #   hidden_size = int(city_param['Hiden Size'])\n",
    "  #   num_filters = ''\n",
    "  #   dropout = ''\n",
    "  #   learning_rate = float(city_param['Learning rate'])\n",
    "  #   epoch = int(city_param['Epochs'])\n",
    "  #   lookback_window = int(city_param['Lookback Window'])\n",
    "  if algo == 'transformer':\n",
    "    city_param = opt_param_transformer.loc[city]\n",
    "    #load  param\n",
    "    n_layers = int(city_param['n Layers'])\n",
    "    hidden_size = int(city_param['Hidden Size'])\n",
    "    num_filters = ''\n",
    "    dropout = float(city_param['Dropout rate'])\n",
    "    learning_rate = float(city_param['Learning rate'])\n",
    "    epoch = int(city_param['Epochs'])\n",
    "    lookback_window = int(city_param['Lookback Window'])\n",
    "  return n_layers, hidden_size, num_filters, dropout, learning_rate, epoch, lookback_window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1675130654392,
     "user": {
      "displayName": "Nhat Le",
      "userId": "00702307310725808810"
     },
     "user_tz": -420
    },
    "id": "GoxJdzkr1pGW",
    "outputId": "6a8e631b-bc6b-479a-c6c4-a98fbd15246f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 410, 15, 5, 0.004862539219123058, '', 0.4366102806654315)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manual Check param before train\n",
    "n_layers, hidden_size, num_filters, dropout, learning_rate, epoch, lookback_window= get_opt_param_by_name(algo='transformer', city='Cao Bằng')\n",
    "lookback_window,epoch, hidden_size, n_layers,learning_rate, num_filters, dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ujcW0K-sU2i3"
   },
   "source": [
    "# Mix Result và Lưu vào folder tương ứng theo từng algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1675130654392,
     "user": {
      "displayName": "Nhat Le",
      "userId": "00702307310725808810"
     },
     "user_tz": -420
    },
    "id": "id44nVQZoqf7"
   },
   "outputs": [],
   "source": [
    "def mix_results(city, algo, specific_data, y_pred_test, y_pred_train, lookback_window, iteration, rmse, mae, mape):\n",
    "    new_cols = [\"City\",\"Algorithm\",\"Run Seq\",\"Observed\",\"Train Prediction\",\"Test Prediction\",\"rmse\",\"mae\",\"mape\"]\n",
    "    mix_train_test_result = pd.DataFrame()\n",
    "    mix_train_test_result['Observed']= specific_data['Diarrhoea_rates'] #default lenght 239\n",
    "    mix_train_test_result = mix_train_test_result.reset_index(drop=True)  \n",
    "    mix_train_test_result['City'] = city\n",
    "    mix_train_test_result['Algorithm'] = algo\n",
    "    mix_train_test_result['Run Seq'] = iteration \n",
    "    mix_train_test_result['Train Prediction'] = None\n",
    "    mix_train_test_result['Test Prediction'] = None\n",
    "    mix_train_test_result['Train Prediction'][lookback_window:lookback_window+y_pred_train[0].size] = y_pred_train[0]\n",
    "    mix_train_test_result['Test Prediction'].iloc[-(y_pred_test[0].size):] = y_pred_test[0]\n",
    "    mix_train_test_result['rmse'] = rmse\n",
    "    mix_train_test_result['mae'] = rmse\n",
    "    mix_train_test_result['mape'] = rmse\n",
    "    mix_train_test_result= mix_train_test_result.reindex(columns=new_cols)\n",
    "    return mix_train_test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1675130654393,
     "user": {
      "displayName": "Nhat Le",
      "userId": "00702307310725808810"
     },
     "user_tz": -420
    },
    "id": "w-5_RLI3U0gJ"
   },
   "outputs": [],
   "source": [
    "def save_results(all_mix_train_test_result, algo,city):\n",
    "  if algo == 'cnn':  \n",
    "    all_mix_train_test_result.to_excel(path_all_mix_cnn+'mix_train_test_data_'+city+'_'+algo+'.xlsx', index = False)\n",
    "  elif algo == 'lstm':\n",
    "    all_mix_train_test_result.to_excel(path_all_mix_lstm+'mix_train_test_data_'+city+'_'+algo+'.xlsx', index = False)\n",
    "  elif algo == 'lstm_att':\n",
    "    all_mix_train_test_result.to_excel(path_all_mix_lstm_att+'mix_train_test_data_'+city+'_'+algo+'.xlsx', index = False)\n",
    "  elif algo == 'tf':\n",
    "    all_mix_train_test_result.to_excel(path_all_mix_tf+'mix_train_test_data_'+city+'_'+algo+'.xlsx', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BSE6GM63Wr_S"
   },
   "source": [
    "# Transformer train and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 598347,
     "status": "error",
     "timestamp": 1675131829508,
     "user": {
      "displayName": "Nhat Le",
      "userId": "00702307310725808810"
     },
     "user_tz": -420
    },
    "id": "EpnxJvwkXO1M",
    "outputId": "4c56275b-d76b-4bf8-f917-c75bb2928573"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lookback 3\n"
     ]
    }
   ],
   "source": [
    "dt_started = datetime.now()\n",
    "################################################\n",
    "## Main run all city for Transformer\n",
    "################################################\n",
    "n_LoopEachExpRun = 5\n",
    "expRuntime = 0 # basicly just increase each time\n",
    "\n",
    "cities = ['An Giang', 'BR Vũng Tàu', 'Bình Phước', 'Bình Thuận', 'Bình Định', 'Đà Nẵng'\n",
    "'Bạc Liêu', 'Bắc Kạn', 'Bắc Giang', 'Cao Bằng', 'Cà Mau']\n",
    "\n",
    "# 'An Giang', 'BR Vũng Tàu', 'Bình Phước', 'Bình Thuận', 'Bình Định', 'Đà Nẵng'\n",
    "# 'Bạc Liêu', 'Bắc Kạn', 'Bắc Giang', 'Cao Bằng', 'Cà Mau',\n",
    "\n",
    "# 'Cần Thơ', 'Gia Lai', 'Hà Giang', 'Hà Nội', 'Hà Tĩnh', 'Hòa Bình',\n",
    "# 'Hưng Yên', 'Hải Dương', 'Hải Phòng', 'Khánh Hòa', 'Kiên Giang',\n",
    "\n",
    "# 'Kon Tum', 'Lai Châu', 'Long An', 'Lào Cai', 'Lâm Đồng', 'Đắk Nông'\n",
    "# 'Lạng Sơn', 'Nam Định', 'Nghệ An', 'Ninh Bình', 'Ninh Thuận',\n",
    "\n",
    "# 'Phú Thọ', 'Phú Yên', 'Quảng Bình', 'Quảng Nam', 'Quảng Ngãi', 'Đắk Lắk'\n",
    "# 'Quảng Ninh', 'Quảng Trị', 'Sóc Trăng', 'Sơn La', 'TT Huế',\n",
    "\n",
    "# 'Thanh Hóa', 'Thái Bình', 'Thái Nguyên', 'Tiền Giang', 'Trà Vinh', 'Đồng Tháp'\n",
    "# 'Tuyên Quang', 'Tây Ninh', 'Vĩnh Phúc', 'Yên Bái', 'Điện Biên',\n",
    "\n",
    "\n",
    "l_errCity = {}\n",
    "# Với mỗi thành phố ta sẽ chạy n_LoopEachExpRun lần ứng với từng giải thuật.\n",
    "# Nếu chỉ chạy 1 lần thì để n_LoopEachExpRun = 0\n",
    "\n",
    "for city in cities:\n",
    "  try:\n",
    "    \n",
    "    total_result = pd.DataFrame() # final result cho mỗi thành phố of all runninng time\n",
    "    total_metrix_result = pd.DataFrame() # final metrix result cho mỗi thành phố of all runninng time\n",
    "    mix_train_test_result_tf=pd.DataFrame()\n",
    "\n",
    "    for interation in range(n_LoopEachExpRun):\n",
    "      \n",
    "      # Pre-process data\n",
    "      specific_data = get_city_data(fix_text(city))\n",
    "      specific_data = impute_missing_value(specific_data)\n",
    "      specific_data = convert_to_stationary(specific_data)\n",
    "      specific_data.dropna(inplace=True)\n",
    "      \n",
    "      # Get right optimize parame\n",
    "      n_layers, hidden_size, _, dropout, learning_rate, epoch, lookback_window = get_opt_param_by_name(algo='transformer', city=city)\n",
    "      \n",
    "      train, test = split_data(specific_data,lookback_window)\n",
    "\n",
    "      # Fit data scaler to training data\n",
    "      full_scaler = MinMaxScaler().fit(train)\n",
    "      y_scaler = MinMaxScaler().fit(train.values[:, -1].reshape(-1, 1))\n",
    "\n",
    "      # Scale train and test data\n",
    "      train = full_scaler.transform(train)\n",
    "      test = full_scaler.transform(test)\n",
    "\n",
    "      # Get data to run model\n",
    "      important_features, train_loader, test_tensor = get_data(train, test, args.batch_size, specific_data)\n",
    "\n",
    "      # Overwrite parameters for province-specific diarrhoea prediction\n",
    "      args.look_back = lookback_window\n",
    "      print(city)\n",
    "      trainer = Trainer(\n",
    "                      model_type='transformers',\n",
    "                      city = city, \n",
    "                      learning_rate=learning_rate,\n",
    "                      important_features=important_features,\n",
    "                      train_loader=train_loader,\n",
    "                      test_tensor=test_tensor,\n",
    "                      n_layers=n_layers,\n",
    "                      hidden_size=hidden_size,\n",
    "                      dropout=dropout)\n",
    "\n",
    "      # Train model with province-specific epochs\n",
    "      trainer.train(epochs=epoch)\n",
    "\n",
    "      print(\"Evaluate on test set: \")\n",
    "      y_true_test, y_pred_test, rmse_list, mae_list, mape_list = trainer.evaluate_model(np_data=test, plot= True, scaled=True, city=city, y_scaler= y_scaler)\n",
    "\n",
    "      print(\"Evaluate on train set: \")\n",
    "      y_true_train, y_pred_train,  _, _, _, = trainer.evaluate_model(np_data=train, plot=True, scaled=True, city=city, y_scaler= y_scaler)\n",
    "\n",
    "      # Lưu tất cả kết quả predicted train và predicted test\n",
    "      mix_train_test_result_tf = mix_train_test_result_tf.append(mix_results(city,'transformer', specific_data, y_pred_test, \n",
    "                                                                              y_pred_train,lookback_window,interation, rmse_list[0],mae_list[0],mape_list[0])) \n",
    "\n",
    "\n",
    "      transformer_reslt = pd.DataFrame({\n",
    "                                'City': city,\n",
    "                                'Alg_name': 'transformers',\n",
    "                                'Run Seq': interation,\n",
    "                                'Observed': y_true_test[0], \n",
    "                                '1-month': y_pred_test[0], \n",
    "                                '2-months': y_pred_test[1],\n",
    "                                '3-months': y_pred_test[2],\n",
    "                                'RMSE_1-month': rmse_list[0],\n",
    "                                'RMSE_2-month': rmse_list[1],\n",
    "                                'RMSE_3-month': rmse_list[2],\n",
    "                                'MAE_1-month': mae_list[0],\n",
    "                                'MAE_2-month': mae_list[1],\n",
    "                                'MAE_3-month': mae_list[2],\n",
    "                                'MAPE_1-month': mape_list[0],\n",
    "                                'MAPE_2-month': mape_list[1],\n",
    "                                'MAPE_3-month': mape_list[2]})\n",
    "\n",
    "      transformer_metrix_reslt = pd.DataFrame({\n",
    "                                'City': city,\n",
    "                                'Alg_name': 'transformers',\n",
    "                                'Run Seq': interation,                          \n",
    "                                'RMSE_1-month': rmse_list[0],\n",
    "                                'RMSE_2-month': rmse_list[1],\n",
    "                                'RMSE_3-month': rmse_list[2],\n",
    "                                'MAE_1-month': mae_list[0],\n",
    "                                'MAE_2-month': mae_list[1],\n",
    "                                'MAE_3-month': mae_list[2],\n",
    "                                'MAPE_1-month': mape_list[0],\n",
    "                                'MAPE_2-month': mape_list[1],\n",
    "                                'MAPE_3-month': [mape_list[2]]}) # do có 1 dòng nên pandas cần index = 0\n",
    "      \n",
    "      total_result = total_result.append(pd.DataFrame(transformer_reslt))\n",
    "      total_metrix_result = total_metrix_result.append(pd.DataFrame(transformer_metrix_reslt))\n",
    "    \n",
    "    # Lưu xuống file excel ứng với từng thành phố\n",
    "    total_result.to_excel(path+'/diarrhoea_transformer_'+city+'_'+str(expRuntime)+'.xlsx')  \n",
    "    total_metrix_result.to_excel(path_metrix+'/diarrhoea_metrix_transformer_'+city+'_'+str(expRuntime)+'.xlsx')\n",
    "    \n",
    "    # Lưu xuống mix predicted train và test data ứng với từng thành phố/ từng giải thuật\n",
    "    save_results(mix_train_test_result_tf,'tf', city)\n",
    "  except Exception as e:\n",
    "    l_errCity[city] = e\n",
    "    break\n",
    "\n",
    "dt_ended = datetime.now()\n",
    "print('kết thúc train trong:', round((dt_ended - dt_started).total_seconds()/60))\n",
    "print ('danh sách error city: ')\n",
    "print(l_errCity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wv0VvFzTaniK"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "aborted",
     "timestamp": 1675131206513,
     "user": {
      "displayName": "Nhat Le",
      "userId": "00702307310725808810"
     },
     "user_tz": -420
    },
    "id": "IfDAJeMKaniK"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def send_to_telegram(message):\n",
    "\n",
    "    apiToken = '5908735099:AAGVSLrW62aXPBP-GrMvxoVgMsuJxXJpP1Q'\n",
    "    chatID = '@ptn_announcement'\n",
    "    apiURL = f'https://api.telegram.org/bot{apiToken}/sendMessage'\n",
    "\n",
    "    try:\n",
    "        response = requests.post(apiURL, json={'chat_id': chatID, 'text': message})\n",
    "        print(response.text)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "send_to_telegram(\"TEST\\nList city:\\n\" + ',\\n'.join(str(city) for city in cities) + \"\\nalgo: transformer ntry = 10\" )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "1XNp1Mc2M4Qs0bWXvZhPNai-9s1CkRAyH",
     "timestamp": 1666921005339
    },
    {
     "file_id": "11sZvnnk-xz-m1vrSmwgD7ALJnLtbPvs9",
     "timestamp": 1666536649860
    }
   ],
   "toc_visible": true
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "24f3299fa11d098f3e36f80146bbe61fdadb7bfe8872ee0c4a379787469f5f10"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
