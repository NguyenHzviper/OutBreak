{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xEHFfG50NdzF"
   },
   "source": [
    "# Predict and Train 7 top RMSE and MAE combination\n",
    "Date: 09/12/2022\n",
    "Description: Create cross validation metrics\n",
    "Use 5 combinations only regarding to the ranking from simple combination result.\n",
    "This file creates cross validation metrics for next steps\n",
    "Modificaiton 02/01/2023\n",
    "Sử dụng 5 combination mới"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Ja18nXEglgwF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Feb  8 00:10:11 2023       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 525.60.13    Driver Version: 525.60.13    CUDA Version: 12.0     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Quadro RTX 6000     On   | 00000000:21:00.0 Off |                  Off |\r\n",
      "| 34%   59C    P2   115W / 260W |   5649MiB / 24576MiB |     92%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A     27440      C   ..._diseases_py39/bin/python     1042MiB |\r\n",
      "|    0   N/A  N/A     33001      C   ..._diseases_py39/bin/python     3558MiB |\r\n",
      "|    0   N/A  N/A     40360      C   ..._diseases_py39/bin/python     1046MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6592,
     "status": "ok",
     "timestamp": 1666073902985,
     "user": {
      "displayName": "Duy Thanh Do",
      "userId": "13373927866316348326"
     },
     "user_tz": -420
    },
    "id": "VW4v2IEcTYHa",
    "outputId": "b4573735-a6ca-4cc7-80c8-b23f3b339c93"
   },
   "outputs": [],
   "source": [
    "# Install packages\n",
    "# !pip install -U scikit-learn\n",
    "# !pip install ftfy\n",
    "# !pip install xlsxwriter \n",
    "# !pip install xlwt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "b8I8S9koHjT_"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import tensorflow as tf \n",
    "# from google.colab import drive\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.models import Sequential, model_from_json\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "import traceback\n",
    "import sys\n",
    "import copy\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# lib for LSTM of Meta model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from ftfy import fix_text\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [7, 3]\n",
    "import math\n",
    "\n",
    "import optuna\n",
    "from optuna.trial import TrialState"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NURg_hMGdv_z"
   },
   "source": [
    "# Global param and project path\n",
    "All mix data 1997-2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3798,
     "status": "ok",
     "timestamp": 1666073906773,
     "user": {
      "displayName": "Duy Thanh Do",
      "userId": "13373927866316348326"
     },
     "user_tz": -420
    },
    "id": "MNAuUk6_HVs1",
    "outputId": "e48d21e8-8b36-46c6-9a85-0359f712e8df"
   },
   "outputs": [],
   "source": [
    "# Attach Google Drive for reading and saving files\n",
    "# drive.mount('/content/drive')\n",
    "prj_path = '/home/mlworker/Quang/HealthCare/Source_14012023_v4/'\n",
    "\n",
    "path_all_mix_data =             prj_path+ 'results/all_mix_data/all_mix_data_1997_2018/'\n",
    "path_all_mix_data_cnn =         prj_path+ 'results/all_mix_data/all_mix_data_1997_2018/cnn/'\n",
    "path_all_mix_data_lstm =        prj_path+ 'results/all_mix_data/all_mix_data_1997_2018/lstm/'\n",
    "path_all_mix_data_lstm_att =    prj_path+ 'results/all_mix_data/all_mix_data_1997_2018/lstm_att/'\n",
    "path_all_mix_data_tf =          prj_path+ 'results/all_mix_data/all_mix_data_1997_2018/transformer/'\n",
    "\n",
    "prj_path_opt= prj_path + '/optimize_hyperparam/opt_results/optimize_1997_2018/'\n",
    "prj_opt_combination = prj_path_opt+'opt_combination/'\n",
    "\n",
    "prj_path_result_ensemble =      prj_path+ 'results/ensemble/'\n",
    "com_path_validation_metrics =   prj_path_result_ensemble+ 'result/weighted_combination/cv_metrics/'\n",
    "prj_path_result_top_simple_com = prj_path+ 'results/ensemble/result/top simple combinations/'\n",
    "\n",
    "com_eval_path_validation_metrics = prj_path + 'eval_result/21_12_2022_weighted_combination-5-cities/'\n",
    "\n",
    "# loockback_window for meta model\n",
    "meta_lookback_window = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xây dựng Meta Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-fbf48e42b817>:39: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result_test = result_test.append(selected_result['Test Prediction'].tail(36))\n",
      "<ipython-input-5-fbf48e42b817>:54: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result_test = result_test.append(selected_result['Test Prediction'].tail(36))\n",
      "<ipython-input-5-fbf48e42b817>:73: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result_test = result_test.append(selected_result['Test Prediction'].tail(36))\n",
      "<ipython-input-5-fbf48e42b817>:96: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result_test = result_test.append(selected_result['Test Prediction'].tail(36)) # Lấy 36 tháng cuối\n",
      "<ipython-input-5-fbf48e42b817>:123: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = meta_train_set.append(meta_test_set[3:])\n",
      "<ipython-input-5-fbf48e42b817>:39: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result_test = result_test.append(selected_result['Test Prediction'].tail(36))\n",
      "<ipython-input-5-fbf48e42b817>:54: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result_test = result_test.append(selected_result['Test Prediction'].tail(36))\n",
      "<ipython-input-5-fbf48e42b817>:73: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result_test = result_test.append(selected_result['Test Prediction'].tail(36))\n",
      "<ipython-input-5-fbf48e42b817>:96: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result_test = result_test.append(selected_result['Test Prediction'].tail(36)) # Lấy 36 tháng cuối\n",
      "<ipython-input-5-fbf48e42b817>:123: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = meta_train_set.append(meta_test_set[3:])\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "# Xây Dựng Meta Data Set\n",
    "meta_test_set_city = {}\n",
    "meta_train_set_city = {}\n",
    "meta_data_set_city = {}\n",
    "cities = [\n",
    "#     'An Giang', \n",
    "#     'BR Vũng Tàu', 'Bình Phước', 'Bình Thuận',\n",
    "#     'Bình Định',\n",
    "#         'Bạc Liêu', 'Bắc Kạn', 'Bắc Giang', \n",
    "#     'Cao Bằng', \n",
    "#     'Cà Mau',\n",
    "#         'Cần Thơ', 'Gia Lai', 'Hà Giang', 'Hà Nội', 'Hà Tĩnh',\n",
    "#     'Hòa Bình',\n",
    "#         'Hưng Yên', 'Hải Dương', 'Hải Phòng',\n",
    "    'Khánh Hòa', 'Kiên Giang',\n",
    "#         'Kon Tum', 'Lai Châu', 'Long An', 'Lào Cai', 'Lâm Đồng',\n",
    "#         'Lạng Sơn', 'Nam Định', 'Nghệ An', 'Ninh Bình', 'Ninh Thuận',\n",
    "#         'Phú Thọ', 'Phú Yên', 'Quảng Bình', 'Quảng Nam', 'Quảng Ngãi',\n",
    "#         'Quảng Ninh', 'Quảng Trị', 'Sóc Trăng', 'Sơn La', 'TT Huế',\n",
    "#         'Thanh Hóa', 'Thái Bình', 'Thái Nguyên', 'Tiền Giang', 'Trà Vinh',\n",
    "#         'Tuyên Quang', 'Tây Ninh', 'Vĩnh Phúc', 'Yên Bái', 'Điện Biên',\n",
    "#          'Đà Nẵng', 'Đắk Nông', 'Đắk Lắk', 'Đồng Tháp'\n",
    "]\n",
    "\n",
    "for city in cities:  \n",
    "  meta_test_set = pd.DataFrame()\n",
    "  meta_train_set = pd.DataFrame() \n",
    "  #Note: mỗi bộ all_mix_data sẽ có lenght = 251 (10997 -01/01/2018  và vì 1 dòng đầu bị bỏ đi do quá trình statitionary và remove NAN lúc train)\n",
    "  lenght_whole_data_city = 251\n",
    "  ######################################\n",
    "  # Xử lý data của CNN ứng với mỗi city\n",
    "  df = pd.read_excel(path_all_mix_data_cnn+'mix_train_test_data_'+city+'_cnn.xlsx')\n",
    "  selected_result = df.sort_values(by = 'rmse', ascending=True) # Sắp xếp theo rmse nhỏ nhất\n",
    "  selected_result = selected_result.head(lenght_whole_data_city) # lấy 251 phần tử đầu tiên\n",
    "  selected_result= selected_result.sort_index() \n",
    "  # Tạo meta testset\n",
    "  result_test = selected_result['Train Prediction'].iloc[-meta_lookback_window-36:-36] # Lấy buffer cho phần lookback_window\n",
    "  result_test = result_test.append(selected_result['Test Prediction'].tail(36))\n",
    "  meta_test_cnn = result_test.reset_index(drop=True)  \n",
    "  # Tạo meta trainset\n",
    "  result_train = selected_result['Train Prediction'].iloc[meta_lookback_window:-36]\n",
    "  meta_train_cnn = result_train.reset_index(drop=True)\n",
    "  \n",
    "  ######################################\n",
    "  # Xử lý data của LSTM  ứng với mỗi city\n",
    "  df = pd.read_excel(path_all_mix_data_lstm+'mix_train_test_data_'+city+'_lstm.xlsx')  \n",
    "  selected_result = df.sort_values(by = 'rmse', ascending=True) # Sắp xếp theo rmse nhỏ nhất\n",
    "  selected_result = selected_result.head(lenght_whole_data_city) # lấy 251 phần tử đầu tiên\n",
    "  selected_result= selected_result.sort_index()   \n",
    "  \n",
    "  # Tạo meta testset\n",
    "  result_test = selected_result['Train Prediction'].iloc[-meta_lookback_window-36:-36] # Lấy buffer cho phần lookback_window\n",
    "  result_test = result_test.append(selected_result['Test Prediction'].tail(36))\n",
    "  meta_test_lstm = result_test.reset_index(drop=True)\n",
    "  # Tạo meta trainset\n",
    "  result_train = selected_result['Train Prediction'].iloc[meta_lookback_window:-36]\n",
    "  meta_train_lstm = result_train.reset_index(drop=True)\n",
    "\n",
    "  ###########################################\n",
    "  # Xử lý data của LSTM ATT ứng với mỗi city\n",
    "  df = pd.read_excel(path_all_mix_data_lstm_att+'mix_train_test_data_'+city+'_lstm_att.xlsx') \n",
    "  selected_result = df.sort_values(by = 'rmse', ascending=True) # Sắp xếp theo rmse nhỏ nhất\n",
    "  selected_result = selected_result.head(lenght_whole_data_city) # lấy 251 phần tử đầu tiên\n",
    "  selected_result= selected_result.sort_index()   \n",
    "  \n",
    "  # Tạo meta testset\n",
    "  # Meta testset sẽ có lenght = 36 + lookback_window \n",
    "  meta_test_observed = selected_result['Observed'].iloc[-meta_lookback_window-36:]\n",
    "  meta_test_observed = meta_test_observed.reset_index(drop=True) \n",
    "\n",
    "  result_test = selected_result['Train Prediction'].iloc[-meta_lookback_window-36:-36] # Lấy buffer cho phần lookback_window\n",
    "  result_test = result_test.append(selected_result['Test Prediction'].tail(36))\n",
    "  meta_test_lstm_att = result_test.reset_index(drop=True)  \n",
    "  \n",
    "  # Tạo meta trainset\n",
    "  meta_train_observed = selected_result['Observed'].iloc[meta_lookback_window:-36]\n",
    "  meta_train_observed = meta_train_observed.reset_index(drop=True)\n",
    "  result_train = selected_result['Train Prediction'].iloc[meta_lookback_window:-36]\n",
    "  meta_train_lstm_att = result_train.reset_index(drop=True)\n",
    "\n",
    "\n",
    "  #############################################\n",
    "  # Xử lý data của Transformer ứng với mỗi city\n",
    "  df = pd.read_excel(path_all_mix_data_tf+'mix_train_test_data_'+city+'_tf.xlsx') \n",
    "  selected_result = df.sort_values(by = 'rmse', ascending=True) # Sắp xếp theo rmse nhỏ nhất\n",
    "  selected_result = selected_result.head(lenght_whole_data_city) # lấy 251 phần tử đầu tiên\n",
    "  selected_result= selected_result.sort_index()   \n",
    "  \n",
    "  # Tạo meta testset\n",
    "  # Meta testset sẽ có lenght = 36 + lookback_window \n",
    "  meta_test_observed = selected_result['Observed'].iloc[-meta_lookback_window-36:]\n",
    "  meta_test_observed = meta_test_observed.reset_index(drop=True) \n",
    "\n",
    "  result_test = selected_result['Train Prediction'].iloc[-meta_lookback_window-36:-36] # Lấy buffer cho phần lookback_window\n",
    "  result_test = result_test.append(selected_result['Test Prediction'].tail(36)) # Lấy 36 tháng cuối\n",
    "  meta_test_tf = result_test.reset_index(drop=True)  \n",
    "  \n",
    "  # Tạo meta trainset\n",
    "  meta_train_observed = selected_result['Observed'].iloc[meta_lookback_window:-36] # Lấy từ meta_lookback_window cho đến -36 (3 năm cuối)\n",
    "  meta_train_observed = meta_train_observed.reset_index(drop=True)\n",
    "  result_train = selected_result['Train Prediction'].iloc[meta_lookback_window:-36]\n",
    "  meta_train_tf = result_train.reset_index(drop=True)\n",
    "\n",
    "  \n",
    "  meta_test_set['cnn_pred'] = meta_test_cnn\n",
    "  meta_test_set['lstm_pred'] = meta_test_lstm\n",
    "  meta_test_set['lstm_att_pred'] = meta_test_lstm_att\n",
    "  meta_test_set['tf_pred'] = meta_test_tf\n",
    "  meta_test_set['Observed'] = meta_test_observed\n",
    "   \n",
    "  meta_train_set['cnn_pred'] = meta_train_cnn\n",
    "  meta_train_set['lstm_pred'] = meta_train_lstm\n",
    "  meta_train_set['lstm_att_pred'] = meta_train_lstm_att \n",
    "  meta_train_set['tf_pred'] = meta_train_tf  \n",
    "  meta_train_set['Observed'] = meta_train_observed\n",
    "\n",
    "  meta_test_set_city[city] = meta_test_set # test set có chiều dài 39\n",
    "  meta_train_set_city[city] = meta_train_set # train set có chiều dài 212 = 252 -36 (train) - 1 (station) - 3 (meta_lookback_window)\n",
    "\n",
    "  # Đoạn này làm thêm. Nối dài meta_test_set_city và meta_train_set_city\n",
    "  # lưu ý đoạn này sẽ tạo ra data có chiều dài = 252 - 1 (bỏ đi 1 tháng đầu do xử lý stationary) - meta_lookback_window\n",
    "  df = meta_train_set.append(meta_test_set[3:])\n",
    "  df = df.reset_index(drop=True)\n",
    "  # range date từ 1 + lookback_window tháng đầu tiên\n",
    "  df['year_month'] = pd.Series(pd.date_range(\"1997-05-01\", periods=248, freq=\"M\"))  \n",
    "  meta_data_set_city[city] = df\n",
    "  df.to_excel(prj_path_result_ensemble+'/meta_data/meta_data_'+city+'.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta_train_set_city['Lạng Sơn'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Danh sách các based Model và các Combination có thể của nó"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cnn_pred', 'lstm_att_pred'),\n",
       " ('cnn_pred', 'lstm_pred'),\n",
       " ('cnn_pred', 'tf_pred'),\n",
       " ('lstm_att_pred', 'lstm_pred'),\n",
       " ('lstm_att_pred', 'tf_pred'),\n",
       " ('lstm_pred', 'tf_pred'),\n",
       " ('cnn_pred', 'lstm_att_pred', 'lstm_pred'),\n",
       " ('cnn_pred', 'lstm_att_pred', 'tf_pred'),\n",
       " ('cnn_pred', 'lstm_pred', 'tf_pred'),\n",
       " ('lstm_att_pred', 'lstm_pred', 'tf_pred'),\n",
       " ('cnn_pred', 'lstm_att_pred', 'lstm_pred', 'tf_pred')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Danh sách các based Model và các Combination có thể của nó\n",
    "list_based_model = ['cnn_pred','lstm_att_pred','lstm_pred','tf_pred']\n",
    "\n",
    "combs_features = list(combinations(list_based_model[0:], 2)) + list(combinations(list_based_model[0:], 3)) + list(combinations(list_based_model[0:], 4))\n",
    "\n",
    "# list_based_model = ['cnn_pred','lstm_att_pred','lstm_pred','tf_pred']\n",
    "\n",
    "# combs_features = list(combinations(list_based_model[0:], 3)) \n",
    "combs_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selected 5 Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Bộ này theo top 7 ( rmse va mae)\n",
    "selected_combs = ['LSTM*cnn_pred-tf_pred-Observed',\n",
    "                  'LSTM*lstm_att_pred-lstm_pred-Observed',    \n",
    "\n",
    "                  'LSTM*cnn_pred-lstm_att_pred-tf_pred-Observed',\n",
    "                  'CNN*lstm_att_pred-lstm_pred-Observed', \n",
    "                  'LSTM*cnn_pred-lstm_pred-Observed',\n",
    "\n",
    "                  'LSTM*lstm_att_pred-lstm_pred-tf_pred-Observed',                    \n",
    "                  'CNN*lstm_att_pred-tf_pred-Observed'      \n",
    "                 ]\n",
    "\n",
    "def is_in_Selected_combination(comb_string = ''):\n",
    "    return any(comb_string in x for x in selected_combs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Functions and Train class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def send_to_telegram(message):\n",
    "\n",
    "    apiToken = '5908735099:AAGVSLrW62aXPBP-GrMvxoVgMsuJxXJpP1Q'\n",
    "    chatID = '@ptn_announcement'\n",
    "    apiURL = f'https://api.telegram.org/bot{apiToken}/sendMessage'\n",
    "\n",
    "    try:\n",
    "        response = requests.post(apiURL, json={'chat_id': chatID, 'text': message})\n",
    "        print(response.text)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "\n",
    "def seed_everything(seed: int):\n",
    "    import random, os    \n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "# Set hyperparameters as args using the Configuration class\n",
    "class Configuration():\n",
    "    def __init__(self):\n",
    "      # lấy bộ test dài 36 tháng = 3 năm\n",
    "        self.test_size = 36\n",
    "        # là nhìn vào dữ liệu trước 3 tháng và dự phóng        \n",
    "        self.look_back = 3\n",
    "        # dự phóng 3 tháng\n",
    "        self.n_predicted_month = 3\n",
    "        self.n_features = 3\n",
    "        self.seed = 42\n",
    "        # mỗi phần tử x trong tập suppervise có độ lớn là 16 = 16 tháng\n",
    "        self.batch_size = 16\n",
    "        # self.device = torch.device(\"cuda\")\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.epochs = 300\n",
    "     \n",
    "\n",
    "# Biến args lưu giá trị Global.\n",
    "args = Configuration()\n",
    "\n",
    "\n",
    "# Lưu ý sau khi stationnary thì dòng đầu tiên trừ cho dòng trước nữa là ko có cho nên về cơ bản sẽ thành NaN\n",
    "# Cần remove dòng đầu tiên ra khỏi data để train và test. Cho nên cơ bản là data sau khi in ra sẽ thiếu 1 đơn vị đầu tiên\n",
    "def convert_to_stationary(city_data):\n",
    "    \"\"\"Subtracts previous value for all cols except disease rates\"\"\"\n",
    "    for col_name in city_data.columns:\n",
    "        if col_name != 'Diarrhoea_rates':\n",
    "            try:\n",
    "                city_data[col_name] = city_data[col_name] - city_data[col_name].shift()\n",
    "            except:\n",
    "                print(col_name)\n",
    "    return city_data\n",
    "\n",
    "def impute_missing_value(city_data):\n",
    "    \"\"\"\n",
    "    Cơ bản dữ liệu bị thiếu sót rất nhiều: Như Điện Biên 1997 -2003 là thiếu dữ liệu về bệnh\n",
    "    Hàm này sẽ tự sinh ra dữ liệu bị thiếu. Nếu tháng nào không có số liệu thì tính như sau:\n",
    "    12 tháng đầu không có số liệu thì gán = 0\n",
    "    tháng 13-24 không có số liệu, sẽ lấy giá trị của tháng cùng kỳ năm trước\n",
    "    tháng từ 24 trở đi sẽ lấy giá trị nhỏ nhất của 2 tháng cùng kỳ trong 2 năm gần nhất.\n",
    "    Do Điện Biên bằng 0 nên sau khi xử lý từ 1997 -2003 là đều = 0.  \n",
    "    \"\"\"\n",
    "    for col in city_data.columns:\n",
    "        for index in range(len(city_data[col])):\n",
    "            if np.isnan(city_data[col].iloc[index]):\n",
    "                if index < 12:\n",
    "                    city_data[col].iloc[index] = 0\n",
    "                elif index >= 12 and index <= 24:\n",
    "                    city_data[col].iloc[index] = city_data[col].iloc[index - 12]\n",
    "                else:\n",
    "                    city_data[col].iloc[index] = min(city_data[col].iloc[index - 12], city_data[col].iloc[index - 24])\n",
    "    return city_data\n",
    "\n",
    "def split_data(data, look_back ):\n",
    "    \"\"\"Splits data into train and test sets based on args (Configuration class)\"\"\"\n",
    "    train = data[: -args.test_size]    \n",
    "    test = data[-args.test_size - look_back: ]\n",
    "    return train, test\n",
    "\n",
    "def to_supervised(data, d_in=args.look_back, d_out=args.n_predicted_month, features_list=[]):\n",
    "    \"\"\"\n",
    "    Frames time-series as supervised learning dataset.\n",
    "    \n",
    "    Args:\n",
    "      d_in: lookback window\n",
    "      d_out: number of predicted months\n",
    "      features_list: list of all features **where last col is the disease incidence**\n",
    "\n",
    "    Returns:\n",
    "      Numpy arrays of disease incidence (y) and other predictors (X)\n",
    "    \"\"\"\n",
    "    X, y = list(), list()\n",
    "    for index, _ in enumerate(data):\n",
    "        in_end = index + d_in\n",
    "        out_end = in_end + d_out\n",
    "        if out_end <= len(data):\n",
    "            if len(features_list) == 0 :\n",
    "                X.append(data[index: in_end, :])\n",
    "            else:\n",
    "                X.append(data[index: in_end, features_list])\n",
    "            y.append(data[in_end: out_end, -1])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "def get_data(train_np, test_np, batch_size, list_selected_features, origin_city_meta_data):\n",
    "    \"\"\"\n",
    "    Returns important feature list and data formatted for input into Pytorch \n",
    "    models\n",
    "    \"\"\" \n",
    "    # origin_city_meta_data chứa data mẫu để lấy index \n",
    "    # important_features = arrary index nên ta phải truyền indexex của columns\n",
    "    important_features = [origin_city_meta_data.columns.get_loc(c) for c in list_selected_features if c in origin_city_meta_data]  \n",
    "\n",
    "    train_X, train_y = to_supervised(train_np, features_list=important_features)\n",
    "    test_X, test_y = to_supervised(test_np, features_list=important_features)\n",
    "    train_tensor = TensorDataset(torch.from_numpy(train_X), torch.from_numpy(train_y))\n",
    "    test_tensor = (torch.from_numpy(test_X), torch.from_numpy(test_y))\n",
    "\n",
    "    train_loader = DataLoader(train_tensor, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return important_features, train_loader, test_tensor\n",
    "\n",
    "#Define Pytorch LSTM model\n",
    "class MultiVariateLSTM(nn.Module):\n",
    "    def __init__(self, n_feature=3, n_layers=2, hidden_size=50):\n",
    "        super(MultiVariateLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=n_feature, hidden_size=hidden_size, num_layers=n_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, args.n_predicted_month)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.sigma = nn.Parameter(torch.ones(args.n_predicted_month))\n",
    "    \n",
    "    def forward(self, X_batch, y_batch=None):\n",
    "        output, (last_hidden, _) = self.lstm(X_batch)\n",
    "        last_hidden_vector = output[:, -1, :]\n",
    "        y_predicted = self.linear(last_hidden_vector)\n",
    "        if y_batch != None:\n",
    "            assert y_predicted.size() == y_batch.size()\n",
    "            loss = self.loss_fn(y_predicted, y_batch)\n",
    "            loss = 0.5 * loss / self.sigma**2\n",
    "            loss = loss.sum() + torch.log(1 + self.sigma.prod())\n",
    "            return y_predicted, loss\n",
    "            #return y_predicted, self.loss_fn(y_predicted, y_batch)\n",
    "        else:\n",
    "            return y_predicted\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = torch.tensor(X, device=args.device)\n",
    "        return self.forward(X)\n",
    "\n",
    "#Define Pytorch LSTM-ATT model\n",
    "class MultiVariateLSTM_Attention(nn.Module):\n",
    "    def __init__(self, n_feature=3, n_layers=2, hidden_size=50):\n",
    "        super(MultiVariateLSTM_Attention, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=n_feature, hidden_size=hidden_size, num_layers=n_layers, batch_first=True)\n",
    "        self.attention_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        # self.linear = nn.Linear(hidden_size*2, args.n_predicted_month)\n",
    "        self.linear = nn.Linear(hidden_size, args.n_predicted_month)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.sigma = nn.Parameter(torch.ones(args.n_predicted_month))\n",
    "    \n",
    "    def forward(self, X_batch, y_batch=None):\n",
    "        output, (last_hidden, _) = self.lstm(X_batch)\n",
    "        last_hidden_vector = last_hidden[-1]\n",
    "        remain_hidden_vector = output\n",
    "        e_t = remain_hidden_vector.bmm(self.attention_linear(last_hidden_vector).unsqueeze(2)).squeeze(-1)\n",
    "        alpha_t = F.softmax(e_t, dim=1)\n",
    "        attenion_vector = remain_hidden_vector.transpose(2, 1).bmm(alpha_t.unsqueeze(2)).squeeze(-1)\n",
    "        # combine_vector = torch.cat((last_hidden_vector, attenion_vector), dim=1)\n",
    "        # combine_vector = last_hidden_vector + attenion_vector\n",
    "        y_predicted = self.linear(attenion_vector)\n",
    "        if y_batch != None:\n",
    "            assert y_predicted.size() == y_batch.size()\n",
    "            loss = self.loss_fn(y_predicted, y_batch)\n",
    "            loss = 0.5 * loss / self.sigma**2\n",
    "            loss = loss.sum() + torch.log(1 + self.sigma.prod())\n",
    "            return y_predicted, loss\n",
    "            # return y_predicted, self.loss_fn(y_predicted, y_batch)\n",
    "        else:\n",
    "            return y_predicted\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = torch.tensor(X, device=args.device)\n",
    "        return self.forward(X)\n",
    "\n",
    "# Define Pytorch CNN model\n",
    "class MultivariateCNN(nn.Module):\n",
    "    def __init__(self, num_filters=[100, 100, 100], dropout=0.01):\n",
    "        super(MultivariateCNN, self).__init__()\n",
    "        self.loss_fn = loss = nn.MSELoss()\n",
    "        self.filter_sizes = [1, 2, 3]\n",
    "        self.conv1d_list = nn.ModuleList([nn.Conv1d(args.n_features, num_filters[i], self.filter_sizes[i]) for i in range(len(self.filter_sizes))])\n",
    "        self.linear = nn.Linear(np.sum(num_filters), args.n_predicted_month)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.sigma = nn.Parameter(torch.ones(args.n_predicted_month))\n",
    "\n",
    "    def forward(self, X_batch, y_batch=None):\n",
    "        X_batch = X_batch.permute(0, 2, 1)  #(batch_size, n_features, n_look_back)\n",
    "        X_conv_list = [F.relu(conv1d(X_batch)) for conv1d in self.conv1d_list]\n",
    "        X_pool_list = [F.max_pool1d(x_conv, kernel_size=x_conv.shape[2]) for x_conv in X_conv_list]\n",
    "        X_fc = torch.cat([x_pool.squeeze(dim=2) for x_pool in X_pool_list], dim=1)\n",
    "        y_predicted = self.linear(self.dropout(X_fc))\n",
    "        if y_batch != None:\n",
    "            assert y_predicted.size() == y_batch.size()\n",
    "            loss = self.loss_fn(y_predicted, y_batch)\n",
    "            loss = 0.5 * loss / self.sigma**2\n",
    "            loss = loss.sum() + torch.log(1 + self.sigma.prod())\n",
    "            return y_predicted, loss\n",
    "            # return y_predicted, self.loss_fn(y_predicted, y_batch)\n",
    "        else:\n",
    "            return y_predicted\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = torch.tensor(X, device=args.device)\n",
    "        return self.forward(X)\n",
    "\n",
    "# Transformer Model\n",
    "# Transformer Model\n",
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, n_feature=3, dropout=0.1, max_length=24):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_dim = n_feature\n",
    "        self.max_length = max_length\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        pe = torch.zeros(max_length, self.hidden_dim)\n",
    "        for pos in range(max_length):\n",
    "            for i in range(0, self.hidden_dim, 2):\n",
    "                pe[pos, i] = math.sin(pos/(10000**(2*i/self.hidden_dim)))\n",
    "                if i + 1 < self.hidden_dim:\n",
    "                    pe[pos, i+1] = math.cos(pos/(10000**((2*i+1)/self.hidden_dim)))\n",
    "        pe = pe.unsqueeze(0)        \n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x*math.sqrt(self.hidden_dim)\n",
    "        length = x.size(1)\n",
    "        pe = Variable(self.pe[:, :length], requires_grad=False)\n",
    "        if x.is_cuda:\n",
    "            pe.cuda()\n",
    "        x = x + pe\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, d_input, n_look_back, hidden_size, n_layers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.pe = PositionalEncoder(n_feature=args.n_features, dropout=dropout)\n",
    "\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=d_input, nhead=d_input, dim_feedforward=hidden_size, dropout=dropout, activation='gelu')\n",
    "        # encoder_layers = nn.TransformerEncoderLayer(d_model=d_input, nhead=d_input, dropout=dropout, activation='gelu')\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, n_layers)\n",
    "        self.decoder = nn.Linear(d_input*n_look_back, args.n_predicted_month)\n",
    "\n",
    "        self.sigma = nn.Parameter(torch.ones(args.n_predicted_month))\n",
    "        \n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        \n",
    "    \n",
    "    def forward(self, X_batch, y_batch=None):\n",
    "        X_batch = self.pe(X_batch)\n",
    "        X_batch = self.transformer_encoder(X_batch)\n",
    "        X_batch = X_batch.view(X_batch.size(0), -1)\n",
    "        \n",
    "        y_predicted = self.decoder(X_batch)\n",
    "        if y_batch != None:\n",
    "            assert y_predicted.size() == y_batch.size()\n",
    "            loss = self.loss_fn(y_predicted, y_batch)\n",
    "            loss = 0.5 * loss / self.sigma**2\n",
    "            loss = loss.sum() + torch.log(1 + self.sigma.prod())\n",
    "            return y_predicted, loss\n",
    "        else:\n",
    "            return y_predicted\n",
    "        return X_batch\n",
    "    \n",
    "    def predict(self, X):\n",
    "        with torch.no_grad():\n",
    "            X = torch.tensor(X, device=args.device)\n",
    "        return self.forward(X).squeeze()\n",
    "\n",
    "# Create class to train and evaluate models\n",
    "class Trainer():\n",
    "    def __init__(self, city, model_type, learning_rate, important_features, train_loader, test_tensor, n_layers=2, hidden_size=128, num_filters=[100, 100, 100], dropout=0.01):\n",
    "        \"\"\"\n",
    "        Initialise trainer, allowing input of LSTM, LSTM-ATT, or CNN \n",
    "        hyperparameters. Adam optimiser used for all models.\n",
    "        \"\"\"\n",
    "        self.model_type = model_type    \n",
    "        self.city = city    \n",
    "        self.model = self.init_model(model_type, n_layers, hidden_size, num_filters, dropout, city)\n",
    "        self.model.double().to(args.device)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        self.important_features, self.train_loader, self.test_tensor = important_features, train_loader, test_tensor\n",
    "    \n",
    "    def init_model(self, model_type, n_layers, hidden_size, num_filters, dropout, city):        \n",
    "        #Get hyper param by name and set for n_layers, hidden_size, num_filters, dropout and overwrite the input param  \n",
    "        # n_layers, hidden_size,num_filters, dropout= get_opt_param_by_name( algo = model_type, city = city)\n",
    "    \n",
    "        \"\"\"Initialise a model based on whether LSTM, LSTM-ATT, or CNN is chosen.\"\"\"\n",
    "        if model_type.lower() == 'lstm':\n",
    "            model = MultiVariateLSTM(args.n_features, n_layers, hidden_size)\n",
    "        elif model_type.lower() == 'lstm_attention':\n",
    "            model = MultiVariateLSTM_Attention(args.n_features, n_layers, hidden_size)\n",
    "        elif model_type.lower() == 'cnn':\n",
    "            model = MultivariateCNN(num_filters, dropout)\n",
    "        elif model_type.lower() == 'transformers':\n",
    "            model = TransformerModel(d_input=args.n_features, n_look_back=args.look_back, hidden_size=hidden_size, n_layers=n_layers, dropout=dropout)\n",
    "        return model\n",
    "\n",
    "    def step(self, batch):\n",
    "        self.model.train()\n",
    "        X_batch, y_batch = tuple(t.to(args.device) for t in batch)\n",
    "        self.optimizer.zero_grad()\n",
    "        y_pred, loss = self.model.forward(X_batch, y_batch)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.mean().item()\n",
    "\n",
    "    def validation(self):\n",
    "        self.model.eval()\n",
    "        eval_loss = 0.0\n",
    "\n",
    "        result = {}\n",
    "\n",
    "        y_true = np.array([])\n",
    "        y_pred = np.array([])\n",
    "\n",
    "        X_batch, y_batch = tuple(t.to(args.device) for t in self.test_tensor)\n",
    "        with torch.no_grad():\n",
    "            outputs, loss = self.model.forward(X_batch, y_batch)\n",
    "            eval_loss = loss.mean().item()\n",
    "\n",
    "        return eval_loss\n",
    "\n",
    "    def train(self, epochs=20):\n",
    "        # print('param of model')\n",
    "        # print('hidensize',self.model.hidden_size)\n",
    "        # print('nlayer',self.model.n_layers)  \n",
    "        best_lost = float(\"inf\")\n",
    "        best_model = None\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0.0\n",
    "            for batch in self.train_loader:\n",
    "                loss = self.step(batch)\n",
    "                total_loss += loss\n",
    "            train_loss = total_loss/len(self.train_loader)\n",
    "            eval_loss = self.validation()\n",
    "            if eval_loss < best_lost:\n",
    "                best_lost = eval_loss\n",
    "                best_model = copy.deepcopy(self.model)\n",
    "            if (epoch + 1) == epochs or (epoch + 1) in [c + 1 for c in range(epochs) if c % int(epochs/4) == 0]:\n",
    "                print(f\"Epoch: {epoch:2}/{epochs:2} - train_loss: {train_loss:.4f} - test_loss: {eval_loss:4f}\")\n",
    "        self.model = best_model\n",
    "        self.model.eval()\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "    # Lưu model vào trong thư mục models\n",
    "    def save_model_to(self, path = '', city =''):       \n",
    "        torch.save(self.model, path)\n",
    "\n",
    "    def load_model_to(self, path = ''):       \n",
    "        return torch.load(path)\n",
    "\n",
    "    def evaluate_model(self, np_data=None, plot=True, scaled=True, city=None, k_steps=None, y_scaler =None):\n",
    "        assert scaled, \"data must be scaled\"\n",
    "        self.model.eval()\n",
    "        tensor_data = torch.from_numpy(np_data)\n",
    "        rmse_list = []\n",
    "        mae_list = [] \n",
    "        mape_list = []\n",
    "\n",
    "        y_predicted_list = []\n",
    "        y_true_list = []\n",
    "\n",
    "        for k_steps in range(1, args.n_predicted_month + 1):\n",
    "            y_predicted = []\n",
    "            y_true = []\n",
    "            for index in range(tensor_data.size(0) - args.look_back):\n",
    "                X = tensor_data[index: index + args.look_back, self.important_features]\n",
    "                # yhat = self.model.predict(X.unsqueeze(0)).squeeze()\n",
    "\n",
    "                yhat = self.model.predict(X.unsqueeze(0))\n",
    "                yhat = yhat.squeeze()\n",
    "\n",
    "                y_predicted.append(yhat.detach().cpu().numpy()[k_steps - 1])\n",
    "                y_true.append(tensor_data[index + args.look_back, -1].detach().cpu().numpy())\n",
    "\n",
    "            y_predicted = y_scaler.inverse_transform(np.array(y_predicted).reshape(-1, 1)).reshape(-1, )\n",
    "            y_true = y_scaler.inverse_transform(np.array(y_true).reshape(-1, 1)).reshape(-1, )\n",
    "\n",
    "            \n",
    "\n",
    "            rmse = mean_squared_error(y_true, y_predicted, squared=False)\n",
    "            mae = mean_absolute_error(y_true, y_predicted)\n",
    "            mape = mean_absolute_percentage_error(y_true, y_predicted)\n",
    "\n",
    "            rmse_list.append(rmse)\n",
    "            mae_list.append(mae)\n",
    "            mape_list.append(mape)\n",
    "\n",
    "            print('City: '+self.city+'  _algo:'+self.model_type+'  -RMSE: '+str(rmse))          \n",
    "            if plot==True:\n",
    "              plt.grid(True)\n",
    "              plt.plot(y_predicted, label='predicted')\n",
    "              plt.plot(y_true, label='actual')\n",
    "              plt.title(f\"k-steps = {k_steps} - city: \"+self.city+'  _algo:'+self.model_type+'  -RMSE: '+str(rmse))\n",
    "              plt.legend()\n",
    "              plt.show()\n",
    "\n",
    "              plt.show()\n",
    "            y_predicted_list.append(y_predicted)\n",
    "            y_true_list.append(y_true)\n",
    "\n",
    "        return y_true_list, y_predicted_list, rmse_list, mae_list, mape_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get optimized Hyper param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_str_num_filter_array(str_filter = ''):  \n",
    "  arr_filter = str_filter.split(',')  \n",
    "  int_arr_filter= [int(x) for x in arr_filter]\n",
    "  return int_arr_filter\n",
    "\n",
    "def concate_2_filter_str(listfilter = ''):\n",
    "  string_filter = ','.join(str(e) for e in listfilter)\n",
    "  return string_filter\n",
    "\n",
    "# opt_param_cnn = pd.read_excel(prj_path_opt+'diarrhoea_opt_hyperparam_cnn.xlsx', index_col = 'City')\n",
    "# opt_param_lstm = pd.read_excel(prj_path_opt+'diarrhoea_opt_hyperparam_lstm.xlsx', index_col = 'City')\n",
    "# opt_param_lstm_att = pd.read_excel(prj_path_opt+'diarrhoea_opt_hyperparam_lstm_att.xlsx', index_col = 'City')\n",
    "# opt_param_transformer = pd.read_excel(prj_path_opt+'diarrhoea_opt_hyperparam_transformer.xlsx', index_col = 'City')\n",
    "\n",
    "def get_opt_param_by_name(city = '', cnn_com ='', lstm_com= ''):\n",
    "  # An Giang_opt_CNN_with_lstm_att_pred-lstm_pred-Observed\n",
    "  if cnn_com != '':\n",
    "    opt_param_cnn = pd.read_excel(prj_opt_combination+city+'_opt_CNN_with_'+cnn_com+'.xlsx').reset_index(drop=True)\n",
    "    #   #load  param\n",
    "    n_layers = ''\n",
    "    hidden_size = ''\n",
    "    num_filters = convert_str_num_filter_array(opt_param_cnn['Num. filters'][0])\n",
    "    dropout = float(opt_param_cnn['Dropout rate'])\n",
    "    learning_rate = float(opt_param_cnn['Learning rate'])\n",
    "    epoch = int(opt_param_cnn['Epochs'])\n",
    "    lookback_window = int(opt_param_cnn['Lookback Window'])\n",
    "  elif lstm_com != '': \n",
    "    # tên file khác cnn vì lúc optimze đặt tên dư _opt_hyperparam\n",
    "    opt_param_lstm = pd.read_excel(prj_opt_combination+city+'_opt_hyperparam_LSTM_with_'+lstm_com+'.xlsx').reset_index(drop=True)  \n",
    "    n_layers = int(opt_param_lstm['n Layers'])\n",
    "    hidden_size = int(opt_param_lstm['Hiden Size'])\n",
    "    num_filters = ''\n",
    "    dropout = ''\n",
    "    learning_rate = float(opt_param_lstm['Learning rate'])\n",
    "    epoch = int(opt_param_lstm['Epochs'])\n",
    "    lookback_window = int(opt_param_lstm['Lookback Window'])\n",
    "\n",
    "  \n",
    "  return n_layers, hidden_size, num_filters, dropout, learning_rate, epoch, lookback_window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Đoạn này nhái Cross validation Train/Test set. Tạo 1 bộ validation duy nhất để chạy optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross validation chỉ trên chiều dài của bộ train = 212\n",
    "# tạo l_train_indices\\l_test_indices\n",
    "data = list(range(212))\n",
    "min_train_size = 176 # Chiều dài ban đầu train size = 6 năm = 72\n",
    "test_size = 36 # chiều dài test size 36 tháng\n",
    "l_train_indices= []\n",
    "l_test_indices = []\n",
    "for i in range(min_train_size, len(data)-test_size+1):\n",
    "    train = np.array(data[0:i])\n",
    "    test = np.array(data[i:i+test_size])    \n",
    "    l_train_indices.append(train)\n",
    "    l_test_indices.append(test)\n",
    "\n",
    "len(l_test_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Tạo và Predict. Important (epoch = 1 for test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_train_predict(city ='', algo = '', list_selected_features = '', origin_city_meta_test_set= '', y_scaler = '', s_selected_features= '', train ='', test ='', combination_idx= ''):\n",
    "    \n",
    "    metric_reslt = pd.DataFrame()\n",
    "    \n",
    "    # Get data to run model\n",
    "    important_features, train_loader, test_tensor = get_data(train, test, args.batch_size, list_selected_features, origin_city_meta_test_set)       \n",
    "    \n",
    "    if( algo =='cnn'): \n",
    "        n_layers, hidden_size, num_filters, dropout, learning_rate, epoch, lookback_window = get_opt_param_by_name(city = city, cnn_com=s_selected_features)\n",
    "        algo = 'CNN'    # upper case only\n",
    "        trainer = Trainer(\n",
    "                            model_type='cnn',  \n",
    "                            city = city,                    \n",
    "                            learning_rate=learning_rate,\n",
    "                            important_features=important_features,\n",
    "                            train_loader=train_loader,\n",
    "                            test_tensor=test_tensor,\n",
    "                            num_filters= num_filters, \n",
    "                            dropout=dropout )\n",
    "\n",
    "    if( algo =='lstm'):\n",
    "        n_layers, hidden_size, num_filters, dropout, learning_rate, epoch, lookback_window = get_opt_param_by_name(city = city, lstm_com=s_selected_features)\n",
    "        algo = 'LSTM'    # upper case only        \n",
    "        trainer = Trainer(\n",
    "                            model_type='lstm', \n",
    "                            city = city,\n",
    "                            learning_rate=learning_rate,\n",
    "                            important_features=important_features,\n",
    "                            train_loader=train_loader,\n",
    "                            test_tensor=test_tensor,\n",
    "                            n_layers=n_layers,\n",
    "                            hidden_size=hidden_size)\n",
    "\n",
    "#     epoch = 1 # comment when do experiment\n",
    "\n",
    "    # Train model with province-specific epochs\n",
    "    trainer.train(epochs=epoch)  \n",
    "    # Evaluate test set   \n",
    "    y_true_test, y_pred_test, rmse_list, mae_list, mape_list = trainer.evaluate_model(np_data=test, plot= False, scaled=True, city=city, y_scaler= y_scaler)\n",
    "        \n",
    "    com_reslt = pd.DataFrame({\n",
    "                              'City': city,\n",
    "                              'Meta Model': algo,   \n",
    "                              'Based Model':s_selected_features,                          \n",
    "                              'Observed': y_true_test[0], \n",
    "                              '1-month': y_pred_test[0], \n",
    "                              '2-months': y_pred_test[1],\n",
    "                              '3-months': y_pred_test[2],\n",
    "                              'RMSE_1-month': rmse_list[0],\n",
    "                              'RMSE_2-month': rmse_list[1],\n",
    "                              'RMSE_3-month': rmse_list[2],\n",
    "                              'MAE_1-month': mae_list[0],\n",
    "                              'MAE_2-month': mae_list[1],\n",
    "                              'MAE_3-month': mae_list[2],\n",
    "                              'MAPE_1-month': mape_list[0],\n",
    "                              'MAPE_2-month': mape_list[1],\n",
    "                              'MAPE_3-month': mape_list[2]}) \n",
    "  \n",
    "\n",
    "        \n",
    "    return com_reslt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Optimization\n",
    "Tạo ra 1 bộ validation. Xuất ra Metrics (RMSE và MAE) cho 5 bộ giải thuật chọn lựa. (10 cột cho 1 dòng validation )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0/210 - train_loss: 0.7107 - test_loss: 0.550699\n",
      "Epoch: 52/210 - train_loss: 0.1081 - test_loss: 0.094395\n",
      "Epoch: 104/210 - train_loss: 0.1082 - test_loss: 0.102279\n",
      "Epoch: 156/210 - train_loss: 0.1082 - test_loss: 0.104468\n",
      "Epoch: 208/210 - train_loss: 0.1093 - test_loss: 0.099609\n",
      "Epoch: 209/210 - train_loss: 0.1088 - test_loss: 0.106685\n",
      "City: Khánh Hòa  _algo:lstm  -RMSE: 45.984427395003614\n",
      "City: Khánh Hòa  _algo:lstm  -RMSE: 48.111121762193314\n",
      "City: Khánh Hòa  _algo:lstm  -RMSE: 74.84987213014257\n",
      "Epoch:  0/200 - train_loss: 0.7112 - test_loss: 0.689036\n",
      "Epoch: 50/200 - train_loss: 0.1192 - test_loss: 0.088877\n",
      "Epoch: 100/200 - train_loss: 0.0996 - test_loss: 0.051120\n",
      "Epoch: 150/200 - train_loss: 0.0968 - test_loss: 0.045217\n",
      "Epoch: 199/200 - train_loss: 0.0975 - test_loss: 0.048186\n",
      "City: Khánh Hòa  _algo:lstm  -RMSE: 15.319238512250687\n",
      "City: Khánh Hòa  _algo:lstm  -RMSE: 17.21750707096039\n",
      "City: Khánh Hòa  _algo:lstm  -RMSE: 21.930069755120286\n",
      "Epoch:  0/240 - train_loss: 0.6910 - test_loss: 0.610028\n",
      "Epoch: 60/240 - train_loss: 0.0837 - test_loss: 0.046231\n",
      "Epoch: 120/240 - train_loss: 0.0822 - test_loss: 0.044340\n",
      "Epoch: 180/240 - train_loss: 0.0802 - test_loss: 0.039694\n",
      "Epoch: 239/240 - train_loss: 0.0786 - test_loss: 0.042647\n",
      "City: Khánh Hòa  _algo:cnn  -RMSE: 15.37126958029223\n",
      "City: Khánh Hòa  _algo:cnn  -RMSE: 24.3898440818666\n",
      "City: Khánh Hòa  _algo:cnn  -RMSE: 21.840925769442137\n",
      "Epoch:  0/180 - train_loss: 0.6869 - test_loss: 0.662961\n",
      "Epoch: 45/180 - train_loss: 0.0976 - test_loss: 0.052698\n",
      "Epoch: 90/180 - train_loss: 0.0809 - test_loss: 0.040458\n",
      "Epoch: 135/180 - train_loss: 0.0807 - test_loss: 0.043896\n",
      "Epoch: 179/180 - train_loss: 0.0765 - test_loss: 0.045203\n",
      "City: Khánh Hòa  _algo:lstm  -RMSE: 21.06397644570615\n",
      "City: Khánh Hòa  _algo:lstm  -RMSE: 14.437739793379517\n",
      "City: Khánh Hòa  _algo:lstm  -RMSE: 14.854922596397577\n",
      "Epoch:  0/180 - train_loss: 0.6782 - test_loss: 0.626328\n",
      "Epoch: 45/180 - train_loss: 0.0847 - test_loss: 0.041543\n",
      "Epoch: 90/180 - train_loss: 0.0825 - test_loss: 0.037521\n",
      "Epoch: 135/180 - train_loss: 0.0810 - test_loss: 0.039007\n",
      "Epoch: 179/180 - train_loss: 0.0788 - test_loss: 0.037522\n",
      "City: Khánh Hòa  _algo:cnn  -RMSE: 15.420577586669438\n",
      "City: Khánh Hòa  _algo:cnn  -RMSE: 15.01082217448426\n",
      "City: Khánh Hòa  _algo:cnn  -RMSE: 18.106487540969226\n",
      "Epoch:  0/400 - train_loss: 0.7260 - test_loss: 0.632857\n",
      "Epoch: 100/400 - train_loss: 0.0970 - test_loss: 0.048121\n",
      "Epoch: 200/400 - train_loss: 0.0798 - test_loss: 0.040800\n",
      "Epoch: 300/400 - train_loss: 0.0709 - test_loss: 0.043366\n",
      "Epoch: 399/400 - train_loss: 0.0734 - test_loss: 0.041908\n",
      "City: Khánh Hòa  _algo:lstm  -RMSE: 20.447669176584807\n",
      "City: Khánh Hòa  _algo:lstm  -RMSE: 17.224252403479678\n",
      "City: Khánh Hòa  _algo:lstm  -RMSE: 21.20950025285361\n",
      "Epoch:  0/400 - train_loss: 0.6915 - test_loss: 0.670024\n",
      "Epoch: 100/400 - train_loss: 0.0787 - test_loss: 0.045214\n",
      "Epoch: 200/400 - train_loss: 0.0685 - test_loss: 0.037848\n",
      "Epoch: 300/400 - train_loss: 0.0767 - test_loss: 0.039941\n",
      "Epoch: 399/400 - train_loss: 0.0763 - test_loss: 0.040295\n",
      "City: Khánh Hòa  _algo:lstm  -RMSE: 17.849843924519128\n",
      "City: Khánh Hòa  _algo:lstm  -RMSE: 16.36550172945587\n",
      "City: Khánh Hòa  _algo:lstm  -RMSE: 21.891756971285762\n",
      "{\"ok\":true,\"result\":{\"message_id\":425,\"sender_chat\":{\"id\":-1001712314864,\"title\":\"PTN_Announcement\",\"username\":\"ptn_announcement\",\"type\":\"channel\"},\"chat\":{\"id\":-1001712314864,\"title\":\"PTN_Announcement\",\"username\":\"ptn_announcement\",\"type\":\"channel\"},\"date\":1675815224,\"text\":\"server Kh\\u00e1nh H\\u00f2a\"}}\n",
      "Epoch:  0/260 - train_loss: 0.7557 - test_loss: 0.723753\n",
      "Epoch: 65/260 - train_loss: 0.2107 - test_loss: 0.117288\n",
      "Epoch: 130/260 - train_loss: 0.1909 - test_loss: 0.100070\n",
      "Epoch: 195/260 - train_loss: 0.1793 - test_loss: 0.079916\n",
      "Epoch: 259/260 - train_loss: 0.1343 - test_loss: 0.080526\n",
      "City: Kiên Giang  _algo:lstm  -RMSE: 6.4184157884126405\n",
      "City: Kiên Giang  _algo:lstm  -RMSE: 5.155025529956808\n",
      "City: Kiên Giang  _algo:lstm  -RMSE: 11.036189020787845\n",
      "Epoch:  0/370 - train_loss: 0.7480 - test_loss: 0.732874\n",
      "Epoch: 92/370 - train_loss: 0.1974 - test_loss: 0.114897\n",
      "Epoch: 184/370 - train_loss: 0.1451 - test_loss: 0.072189\n",
      "Epoch: 276/370 - train_loss: 0.1139 - test_loss: 0.068631\n",
      "Epoch: 368/370 - train_loss: 0.1243 - test_loss: 0.082393\n",
      "Epoch: 369/370 - train_loss: 0.1250 - test_loss: 0.063384\n",
      "City: Kiên Giang  _algo:lstm  -RMSE: 6.825857042230716\n",
      "City: Kiên Giang  _algo:lstm  -RMSE: 7.200002201203647\n",
      "City: Kiên Giang  _algo:lstm  -RMSE: 6.429430312209249\n",
      "Epoch:  0/450 - train_loss: 0.7621 - test_loss: 0.712955\n",
      "Epoch: 112/450 - train_loss: 0.1660 - test_loss: 0.098466\n",
      "Epoch: 224/450 - train_loss: 0.1365 - test_loss: 0.087888\n",
      "Epoch: 336/450 - train_loss: 0.1240 - test_loss: 0.076905\n",
      "Epoch: 448/450 - train_loss: 0.1231 - test_loss: 0.063977\n",
      "Epoch: 449/450 - train_loss: 0.1220 - test_loss: 0.069744\n",
      "City: Kiên Giang  _algo:cnn  -RMSE: 6.675182788850407\n",
      "City: Kiên Giang  _algo:cnn  -RMSE: 5.906331487116441\n",
      "City: Kiên Giang  _algo:cnn  -RMSE: 8.841055703746747\n",
      "Epoch:  0/180 - train_loss: 0.8431 - test_loss: 0.678206\n",
      "Epoch: 45/180 - train_loss: 0.2091 - test_loss: 0.125892\n",
      "Epoch: 90/180 - train_loss: 0.1971 - test_loss: 0.112225\n",
      "Epoch: 135/180 - train_loss: 0.1931 - test_loss: 0.103915\n",
      "Epoch: 179/180 - train_loss: 0.1868 - test_loss: 0.095855\n",
      "City: Kiên Giang  _algo:lstm  -RMSE: 5.550936356963154\n",
      "City: Kiên Giang  _algo:lstm  -RMSE: 9.976452592474926\n",
      "City: Kiên Giang  _algo:lstm  -RMSE: 14.123675235798034\n",
      "Epoch:  0/100 - train_loss: 0.7848 - test_loss: 0.741632\n",
      "Epoch: 25/100 - train_loss: 0.4197 - test_loss: 0.381949\n",
      "Epoch: 50/100 - train_loss: 0.2460 - test_loss: 0.192092\n",
      "Epoch: 75/100 - train_loss: 0.1931 - test_loss: 0.112902\n",
      "Epoch: 99/100 - train_loss: 0.1801 - test_loss: 0.102253\n",
      "City: Kiên Giang  _algo:cnn  -RMSE: 5.473439397581812\n",
      "City: Kiên Giang  _algo:cnn  -RMSE: 9.60279656647822\n",
      "City: Kiên Giang  _algo:cnn  -RMSE: 11.904444305084464\n",
      "Epoch:  0/300 - train_loss: 0.8609 - test_loss: 0.732211\n",
      "Epoch: 75/300 - train_loss: 0.2874 - test_loss: 0.254754\n",
      "Epoch: 150/300 - train_loss: 0.2023 - test_loss: 0.133409\n",
      "Epoch: 225/300 - train_loss: 0.1999 - test_loss: 0.114956\n",
      "Epoch: 299/300 - train_loss: 0.1966 - test_loss: 0.116472\n",
      "City: Kiên Giang  _algo:lstm  -RMSE: 6.91440180137126\n",
      "City: Kiên Giang  _algo:lstm  -RMSE: 12.895426663632248\n",
      "City: Kiên Giang  _algo:lstm  -RMSE: 18.197767702928513\n",
      "Epoch:  0/230 - train_loss: 0.7477 - test_loss: 0.687088\n",
      "Epoch: 57/230 - train_loss: 0.2095 - test_loss: 0.127408\n",
      "Epoch: 114/230 - train_loss: 0.1936 - test_loss: 0.116117\n",
      "Epoch: 171/230 - train_loss: 0.1905 - test_loss: 0.114290\n",
      "Epoch: 228/230 - train_loss: 0.1658 - test_loss: 0.088297\n",
      "Epoch: 229/230 - train_loss: 0.1647 - test_loss: 0.100121\n",
      "City: Kiên Giang  _algo:lstm  -RMSE: 6.421020011448478\n",
      "City: Kiên Giang  _algo:lstm  -RMSE: 9.575097697015353\n",
      "City: Kiên Giang  _algo:lstm  -RMSE: 17.129804562879762\n",
      "{\"ok\":true,\"result\":{\"message_id\":427,\"sender_chat\":{\"id\":-1001712314864,\"title\":\"PTN_Announcement\",\"username\":\"ptn_announcement\",\"type\":\"channel\"},\"chat\":{\"id\":-1001712314864,\"title\":\"PTN_Announcement\",\"username\":\"ptn_announcement\",\"type\":\"channel\"},\"date\":1675815460,\"text\":\"server Ki\\u00ean Giang\"}}\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "dt_started = datetime.now()\n",
    "\n",
    "l_errCity = {} # có lỗi sẽ lưu vào đây, kiểm tra ngay cell sau\n",
    "\n",
    "# city_list = ['An Giang']\n",
    "# Với mỗi thành phố, loop qua từng bộ tr/validation, cho train/predict N tổ hợp models\n",
    "\n",
    "for city in cities:\n",
    "  # pd_all_rows_new_meta_data = pd.DataFrame()  \n",
    "  # pd_one_row_new_meta_data = pd.DataFrame()\n",
    "  df_one_city_all_result = pd.DataFrame()\n",
    "  combination_idx = 0\n",
    "  for combination_features in combs_features:            \n",
    "    list_selected_features = [x for x in combination_features] +['Observed'] # Lấy danh sách tên các features + cột Observed \n",
    "    s_selected_features = '-'.join([str(elem) for elem in list_selected_features])      \n",
    "\n",
    "    # Có trong danh sách chọn lựa thì thực hiện optimize\n",
    "    if((is_in_Selected_combination('CNN*'+s_selected_features)==True) or (is_in_Selected_combination('LSTM*'+s_selected_features) == True)):        \n",
    "      args.n_features = len(list_selected_features) \n",
    "      args.look_back = meta_lookback_window  \n",
    "\n",
    "      try:\n",
    "        # Lấy data từ train set và chia ra theo valiation\n",
    "        origin_city_meta_train_set = meta_train_set_city[city]\n",
    "        origin_city_meta_test_set = meta_test_set_city[city]          \n",
    "      \n",
    "        # Data train/test processing for all Meta model\n",
    "        train = meta_train_set_city[city].astype(float)\n",
    "        test = meta_test_set_city[city].astype(float)\n",
    "\n",
    "        train = train.replace((np.inf, -np.inf, np.nan), 0)\n",
    "        test = test.replace((np.inf, -np.inf, np.nan), 0) \n",
    "      \n",
    "        # Đoạn này impute các trường hợp rỗng\n",
    "        train = impute_missing_value(train)\n",
    "        test = impute_missing_value(test)\n",
    "\n",
    "        # Fit data scaler to training data\n",
    "        full_scaler = MinMaxScaler().fit(train)\n",
    "        y_scaler = MinMaxScaler().fit(train.values[:, -1].reshape(-1, 1))\n",
    "\n",
    "        # Scale train and test data\n",
    "        train = full_scaler.transform(train)\n",
    "        test = full_scaler.transform(test)\n",
    "\n",
    "        if(is_in_Selected_combination('CNN*'+s_selected_features)==True):\n",
    "          ########################################\n",
    "          ########## làm optimize cho CNN \n",
    "          ########################################\n",
    "\n",
    "          one_cnn_result = do_train_predict(city =city, algo='cnn', list_selected_features = list_selected_features,\n",
    "                                              origin_city_meta_test_set= origin_city_meta_test_set, y_scaler = y_scaler, \n",
    "                                              s_selected_features= s_selected_features, train = train, test = test, \n",
    "                                              combination_idx= combination_idx)\n",
    "          df_one_city_all_result = pd.concat([df_one_city_all_result,one_cnn_result])\n",
    "\n",
    "        if(is_in_Selected_combination('LSTM*'+s_selected_features)==True):\n",
    "          ########################################\n",
    "          ########## LSTM \n",
    "          ########################################\n",
    "          one_lstm_result = do_train_predict(city =city, algo='lstm', list_selected_features = list_selected_features,\n",
    "                                              origin_city_meta_test_set= origin_city_meta_test_set, y_scaler = y_scaler, \n",
    "                                              s_selected_features= s_selected_features, train = train, test = test, \n",
    "                                              combination_idx= combination_idx)\n",
    "          df_one_city_all_result = pd.concat([df_one_city_all_result,one_lstm_result])\n",
    "        #   one_lstm_opt_result.to_excel(prj_path_opt+'opt_combination\\\\'+city+'_opt_hyperparam_LSTM_with_'+s_selected_features+'.xlsx')           \n",
    "          \n",
    "      except Exception as e:\n",
    "        l_errCity[city] = e\n",
    "        break\n",
    "  df_one_city_all_result.to_excel(prj_path_result_top_simple_com+ city+'_combination_7.xlsx')\n",
    "  send_to_telegram(\"server \" + city)\n",
    "print(l_errCity)\n",
    "# Telegram vào"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "fe0c8757cd5c56d3f14ea3dfb2cdcbe263e0c7c6548ba4aed616ce8d5c9e5f40"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
