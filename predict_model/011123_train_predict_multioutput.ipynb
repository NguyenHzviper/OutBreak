{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import pickle\n",
    "import torch\n",
    "from xgboost import XGBRegressor\n",
    "from darts import TimeSeries\n",
    "from darts.models import  RandomForest, LinearRegressionModel,  \\\n",
    "                        LightGBMModel, CatBoostModel, XGBModel,  \\\n",
    "                        BlockRNNModel, NBEATSModel, NHiTSModel, \\\n",
    "                        TCNModel, TFTModel\n",
    "\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import PoissonRegressor\n",
    "\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from darts.utils.likelihood_models import GaussianLikelihood\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "prj_path = '../'\n",
    "data_path = prj_path + \"data/new_data/DH/squeezed/\"\n",
    "prj_path_opt= prj_path + \"optimize_hyperparam/opt_results/\"\n",
    "output_process = prj_path + \"data/new_data/DH/processed_data/\"\n",
    "output_featureselection = prj_path + \"data/new_data/DH/feature_selection/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cities = [\n",
    "        'An Giang', 'BR Vũng Tàu', 'Bình Phước', 'Bình Thuận', 'Bình Định',\n",
    "        'Bạc Liêu', 'Bắc Kạn', 'Bắc Giang', 'Cao Bằng', 'Cà Mau',\n",
    "        'Cần Thơ', 'Gia Lai', 'Hà Giang', 'Hà Nội', 'Hà Tĩnh',\n",
    "        'Hòa Bình','Hưng Yên', 'Hải Dương', 'Hải Phòng', 'Khánh Hòa', 'Kiên Giang',\n",
    "        'Kon Tum', 'Lai Châu', 'Long An', 'Lào Cai', 'Lâm Đồng',\n",
    "        'Lạng Sơn','Nam Định', 'Nghệ An', 'Ninh Bình', 'Ninh Thuận',\n",
    "        'Phú Thọ', 'Phú Yên', 'Quảng Bình', 'Quảng Nam', 'Quảng Ngãi',\n",
    "        'Quảng Ninh', 'Quảng Trị', 'Sóc Trăng', 'Sơn La', 'TT Huế',\n",
    "        'Thanh Hóa', 'Thái Bình', 'Thái Nguyên', 'Tiền Giang', 'Trà Vinh',\n",
    "        'Tuyên Quang', 'Tây Ninh', 'Vĩnh Phúc', 'Yên Bái', 'Điện Biên',\n",
    "        'Đà Nẵng', 'Đắk Nông', 'Đắk Lắk', 'Đồng Tháp'\n",
    "]\n",
    "# cities = ['Hà Nội','Hải Phòng','Quảng Ninh','Nam Định','Thái Bình','Quảng Nam','Quảng Ngãi', 'Phú Yên',\n",
    "#           'Ninh Thuận', 'Bình Thuận', 'Tây Ninh', 'Bình Phước', 'An Giang', 'Tiền Giang','Cần Thơ', 'Trà Vinh']\n",
    "\n",
    "cities = ['An Giang','Quảng Ninh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters as args using the Configuration class\n",
    "class Configuration():\n",
    "    def __init__(self):\n",
    "        # lấy bộ test dài 36 tháng = 3 năm\n",
    "        self.test_size = 36\n",
    "        # là nhìn vào dữ liệu trước 3 tháng và dự phóng        \n",
    "        self.look_back = 3\n",
    "        # dự phóng n-step trong 6 tháng\n",
    "        self.n_predicted_period_months = 6\n",
    "        self.n_features = 3\n",
    "        self.seed = 42\n",
    "        # mỗi phần tử x trong tập suppervise có độ lớn là 16 = 16 tháng\n",
    "        self.batch_size = 16\n",
    "        self.device = torch.device(\"cuda\")\n",
    "        self.epochs = 300\n",
    "        #others\n",
    "        self.labels = \"Dengue_fever_rates\"\n",
    "        # Input param for Optimize Run\n",
    "        self.ntry = 1\n",
    "        self.njob = 1\n",
    "\n",
    "args = Configuration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    import random\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    \n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "seed_everything(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supporting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def send_to_telegram(message):\n",
    "\n",
    "    apiToken = '5908735099:AAGVSLrW62aXPBP-GrMvxoVgMsuJxXJpP1Q'\n",
    "    chatID = '@ptn_announcement'\n",
    "    apiURL = f'https://api.telegram.org/bot{apiToken}/sendMessage'\n",
    "\n",
    "    try:\n",
    "        response = requests.post(apiURL, json={'chat_id': chatID, 'text': message})\n",
    "        print(response.text)\n",
    "    except Exception as e:\n",
    "        message_error = \"Bị lỗi rùi: \"+str(e)\n",
    "        response = requests.post(apiURL, json={'chat_id': chatID, 'text': message_error})\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict_all_city_data():\n",
    "  cities_data = {}  \n",
    "  for city in cities:\n",
    "    city_result = pd.read_excel(prj_path+'data/new_data/DH/squeezed/squeezed_'+city+'.xlsx')  \n",
    "    \"\"\"Get all data from all city in 1997 - 2016\"\"\" \n",
    "    city_result = city_result.loc[city_result['year_month'] < '2017-1-1'] \n",
    "    cities_data[city] = city_result\n",
    "  return cities_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data (pre-)processing functions\n",
    "# modification\n",
    "def get_city_data(city_name, dict_full_data):\n",
    "    \"\"\"Returns Diarrhoea rate and climate data\"\"\" \n",
    "    city_data = dict_full_data[city_name].drop(columns=['Diarrhoea_cases','Diarrhoea_rates', 'province',\n",
    "                                                        'Influenza_rates','Influenza_cases',\n",
    "                                                        'Dengue_fever_cases', 'year', 'month'], \n",
    "                                                                  axis=1, \n",
    "                                                                  inplace=False)    \n",
    "    return city_data\n",
    "\n",
    "def convert_to_stationary(city_data):\n",
    "    \"\"\"Subtracts previous value for all cols except disease rates\"\"\"\n",
    "    for col_name in city_data.columns:\n",
    "        if col_name != 'Diarrhoea_rates':\n",
    "            try:\n",
    "                city_data[col_name] = city_data[col_name] - city_data[col_name].shift()\n",
    "            except:\n",
    "                print(col_name)\n",
    "    return city_data\n",
    "\n",
    "def impute_missing_value(city_data):\n",
    "    \"\"\"\n",
    "    Imputes 0 for first 12 months, \n",
    "    last year's value for months 12-24, \n",
    "    and minimum value of last two years for months 25+\n",
    "    \"\"\"\n",
    "    for col in city_data.columns:\n",
    "        for index in range(len(city_data[col])):\n",
    "            if np.isnan(city_data[col].iloc[index]):\n",
    "                if index < 12:\n",
    "                    city_data[col].iloc[index] = 0\n",
    "                elif index >= 12 and index <= 24:\n",
    "                    city_data[col].iloc[index] = city_data[col].iloc[index - 12]\n",
    "                else:\n",
    "                    city_data[col].iloc[index] = min(city_data[col].iloc[index - 12], city_data[col].iloc[index - 24])\n",
    "    return city_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_full_data(dict_full_data):\n",
    "    climate_and_disease_feats = ['Total_Evaporation',\n",
    "       'Total_Rainfall', 'Max_Daily_Rainfall', 'n_raining_days',\n",
    "       'Average_temperature', 'Max_Average_Temperature',\n",
    "       'Min_Average_Temperature', 'Max_Absolute_Temperature',\n",
    "       'Min_Absolute_Temperature', 'Average_Humidity', 'Min_Humidity',\n",
    "       'n_hours_sunshine', 'Dengue_fever_rates']\n",
    "    for city in cities:\n",
    "        city_data = get_city_data(city_name=city,dict_full_data = dict_full_data)\n",
    "        city_data_features = city_data[climate_and_disease_feats]\n",
    "        city_data_features = impute_missing_value(city_data_features)\n",
    "        city_data_features = convert_to_stationary(city_data_features)\n",
    "        city_data_features.dropna(inplace=True)\n",
    "        city_data_features.loc[:, \"year_month\"] = city_data[\"year_month\"]\n",
    "        dict_full_data[city] = city_data_features\n",
    "    return dict_full_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, look_back, n_nextstep = args.n_predicted_period_months):\n",
    "    \"\"\"Splits data into train and test sets based on args (Configuration class)\"\"\"\n",
    "    train = data[: -args.test_size]    \n",
    "    test = data[-args.test_size - look_back-(n_nextstep - 1): ]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_supervised(data,  d_out, d_in, features_list=[]):\n",
    "    \"\"\"\n",
    "    Frames time-series as supervised learning dataset.\n",
    "    \n",
    "    Args:\n",
    "      d_in: lookback window\n",
    "      d_out: number of predicted months\n",
    "      features_list: list of all features **where last col is the disease incidence**\n",
    "\n",
    "    Returns:\n",
    "      Numpy arrays of disease incidence (y) and other predictors (X)\n",
    "    \"\"\"\n",
    "    X, y = list(), list()\n",
    "    for index, _ in enumerate(data):\n",
    "        in_end = index + d_in\n",
    "        out_end = in_end + d_out\n",
    "        if out_end <= len(data):\n",
    "            if len(features_list) == 0 :\n",
    "                X.append(data[index: in_end, :-1])\n",
    "            else:\n",
    "                X.append(data[index: in_end, features_list])\n",
    "            y.append(data[out_end-1: out_end, -1])\n",
    "    return np.array(X), np.array(y).reshape(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataWithSelectedFeature(city, next_predicted_month):\n",
    "  selected_feature = []\n",
    "  df = pd.read_csv(output_featureselection+str(next_predicted_month)+\"step_feature_selection_3_most.csv\")\n",
    "  for row in range(len(df)):\n",
    "    if (df[\"City\"][row] == city):\n",
    "      selected_feature.append(df[\"1st_Feature\"][row])\n",
    "      selected_feature.append(df[\"2nd_Feature\"][row])\n",
    "      selected_feature.append(df[\"3rd_Feature\"][row])\n",
    "  return selected_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(df_train, df_eval, model, location, feature_list, labels, scaler):\n",
    "    \"\"\"\n",
    "    $df: pandas.DataFrame object containing data for training and testing model:\n",
    "    $model: darts model object\n",
    "    $feature_list: Names of the features used as model input\n",
    "    $label: the value the model will be trained to predict\n",
    "    $scaler: scaler object. Note: the scaler will be fitted on training data and applied to test data\n",
    "    $lags: how much to look back into the past to output prediction\n",
    "    $split_index: the point at which to divide train and test_data\n",
    "\n",
    "    \"\"\"    \n",
    "    print(\"🍋🍋🍋🍋🍋Check var feature selection: \",feature_list)\n",
    "    x_train = TimeSeries.from_dataframe(df_train, \"year_month\", feature_list)\n",
    "    y_train = TimeSeries.from_dataframe(df_train, \"year_month\", labels)\n",
    "    x_test = TimeSeries.from_dataframe(df_eval, \"year_month\", feature_list)\n",
    "    y_test = TimeSeries.from_dataframe(df_eval, \"year_month\", labels)\n",
    "\n",
    "    #Cách một khoảng phần này cho x_train thì mới đưa đc chuỗi x_test vừa đủ để dự đoán 36 lần 6step\n",
    "        # ValueError: For the given forecasting horizon `n=xxx`, the provided \n",
    "        # past covariates at dataset index `0` do not extend far enough into \n",
    "        # the future. As `n > output_chunk_length` the past covariates must \n",
    "        # start at time step `xxxxxx`, whereas now they start at \n",
    "        # time step `xxxxx`.\n",
    "\n",
    "    x_train_back = x_train[:-(args.n_predicted_period_months-1)]\n",
    "    y_train_back = y_train[:-(args.n_predicted_period_months-1)]\n",
    "\n",
    "    predict_list = []\n",
    "    for i in range (args.test_size+(args.n_predicted_period_months-1)):\n",
    "        print(\"🍭🍭🍭🍭🍭🍭🍭🍭🍭\")\n",
    "        print(\"y_train_back: \",len(y_train_back))\n",
    "        print(\"x_train_back: \",len(x_train_back))\n",
    "        model.fit(y_train_back, past_covariates = x_train_back)\n",
    "        prediction = model.predict(6)\n",
    "        x_train_back = x_train_back.append(x_test[i+3])\n",
    "        predict_list.append(np.array(prediction._xa).squeeze())\n",
    "    y_pred_list = []\n",
    "    for step in range(args.n_predicted_period_months):\n",
    "        moving = args.n_predicted_period_months-1-step\n",
    "        y_pred_list.append([x[step] for x in predict_list][moving:args.test_size+moving])\n",
    "\n",
    "    y_true = df_eval[labels][-args.test_size:].values\n",
    "\n",
    "    y_pred_inverse_list = []\n",
    "    for step in range(args.n_predicted_period_months):\n",
    "        df_eval_pred_inverse = df_eval[-args.test_size:]\n",
    "        df_eval_pred_inverse[args.labels]= y_pred_list[step] #step 1\n",
    "        y_pred_inverse = scaler.inverse_transform(df_eval_pred_inverse.iloc[:,:-1])[:,[-1]].reshape(args.test_size)\n",
    "        y_pred_inverse_list.append(y_pred_inverse)\n",
    "\n",
    "    y_pred_inverse_list\n",
    "    df_compare_test_predict = pd.DataFrame({\n",
    "        'y_true':y_true,\n",
    "        'y_pred_1step':y_pred_inverse_list[0],\n",
    "        'y_pred_2step':y_pred_inverse_list[1],\n",
    "        'y_pred_3step':y_pred_inverse_list[2],\n",
    "        'y_pred_4step':y_pred_inverse_list[3],\n",
    "        'y_pred_5step':y_pred_inverse_list[4],\n",
    "        'y_pred_6step':y_pred_inverse_list[5],\n",
    "        })\n",
    "    df_compare_test_predict.plot()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    mse_nstep = []\n",
    "    mae_nstep = []\n",
    "    rmse_nstep = []\n",
    "    mape_nstep = []\n",
    "    for step in range(args.n_predicted_period_months):\n",
    "        mse_nstep.append(mean_squared_error(y_true, y_pred_inverse_list[step]))\n",
    "        mae_nstep.append(mean_absolute_error(y_true, y_pred_inverse_list[step]))\n",
    "        rmse_nstep.append(mse_nstep[step]**0.5)\n",
    "        mape_nstep.append(mean_absolute_percentage_error(y_true, y_pred_inverse_list[step]))\n",
    "    return model, y_true, y_pred_inverse_list, mse_nstep, mae_nstep, rmse_nstep, mape_nstep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_prediction_for_location(df_train, df_eval, model, location, feature_list, \n",
    "                                                labels, scaler):\n",
    "    \"\"\"train and generate prediction for a province\n",
    "    df: DataFrame object containing features and label(s) for training model\n",
    "    localtion: location_name\n",
    "    feature_list: list of features used as model input,  must be among the column names of df\n",
    "    labels: the values model will be trained to predict\n",
    "    scaler: sklearn scaler object\n",
    "    lags: how long into the past to look back when making prediction\n",
    "    split_index: the point at which to divide data into the train and test subsets.\n",
    "    \"\"\"\n",
    "    model, y_true, y_pred_inverse_list, mse_nstep, mae_nstep, rmse_nstep, mape_nstep = train_and_evaluate(df_train, df_eval, model, location, feature_list, labels, scaler)\n",
    "    \n",
    "    df_prediction = pd.DataFrame({\"Date\": df_eval[\"year_month\"][-args.test_size:],\n",
    "                                  \"Observed\": y_true[-args.test_size:],\n",
    "                                  f\"{1}-month\": y_pred_inverse_list[0],\n",
    "                                  f\"{2}-month\": y_pred_inverse_list[1],\n",
    "                                  f\"{3}-month\": y_pred_inverse_list[2],\n",
    "                                  f\"{4}-month\": y_pred_inverse_list[3],\n",
    "                                  f\"{5}-month\": y_pred_inverse_list[4],\n",
    "                                  f\"{6}-month\": y_pred_inverse_list[5]})    \n",
    "\n",
    "    df_prediction[\"City\"] = location\n",
    "    for step in range(args.n_predicted_period_months):\n",
    "        df_prediction[f\"RMSE_{step+1}-month\"] = rmse_nstep[step]\n",
    "        df_prediction[f\"MAE_{step+1}-month\"] = mae_nstep[step]\n",
    "        df_prediction[f\"MAPE_{step+1}-month\"] = mape_nstep[step]\n",
    "        df_prediction[f\"MSE_{step+1}-month\"] = mse_nstep[step]\n",
    "    print(\"⭐️⭐️⭐️⭐️⭐️⭐️⭐️\")\n",
    "\n",
    "    return df_prediction, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHyperParams(model_name, city):\n",
    "  folder_path = f'../optimize_hyperparam/opt_results/opt_res_ml_26102023/{model_name}/'\n",
    "  file_path = folder_path+ f'261023_DF_opt_hyperparam_{model_name}_multi-nstep.xlsx'\n",
    "  df_optimized = pd.read_excel(file_path)\n",
    "  df_optimized_params = df_optimized.loc[(df_optimized['City'] == city)]\n",
    "  return df_optimized_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_list = [\n",
    "   \"SARIMA\",\n",
    "]\n",
    "\n",
    "for model_name in model_name_list:   \n",
    "    print(f\"✨✨✨✨✨✨✨✨{model_name}✨✨✨✨✨✨✨✨✨✨\")\n",
    "    for city in cities:\n",
    "        print(f\"✨✨✨✨✨✨✨✨{city}✨✨✨✨✨✨✨✨✨✨\")\n",
    "        df_results = pd.DataFrame()\n",
    "        df_train = pd.read_csv(output_process+city+'_train_preprocessed.csv', parse_dates=True, index_col= None, encoding = 'unicode_escape')\n",
    "        df_eval = pd.read_csv(output_process+city+'_test_preprocessed_normed.csv', parse_dates=True, index_col= None, encoding = 'unicode_escape')\n",
    "        df_full = pd.concat([df_train, df_eval[args.n_predicted_period_months + args.look_back -1 :]], ignore_index=True, sort=False)\n",
    "\n",
    "        scaler = joblib.load(output_process+city+'_train_scalerMinMaxNorm.save') #ok\n",
    "        nstep = args.n_predicted_period_months\n",
    "        selected_features = getDataWithSelectedFeature(city, nstep)\n",
    "        \n",
    "        df_hyper_params = getHyperParams(model_name, city)\n",
    "\n",
    "        if model_name == \"SARIMA\":\n",
    "            p = df_hyper_params['p'].values[0]\n",
    "            d = df_hyper_params['d'].values[0]\n",
    "            q = df_hyper_params['q'].values[0]\n",
    "            t = df_hyper_params['t'].values[0]\n",
    "            P = df_hyper_params['P'].values[0]\n",
    "            D = df_hyper_params['D'].values[0]\n",
    "            Q = df_hyper_params['Q'].values[0]\n",
    "            m = df_hyper_params['m'].values[0]\n",
    "            cfg = [(p, d, q), (P, D, Q, m), t]\n",
    "            order, sorder, trend = cfg\n",
    "        \n",
    "        trainlist = [x for x in df_full.Dengue_fever_rates]\n",
    "        nstep = args.n_predicted_period_months\n",
    "\n",
    "        predict_list = []\n",
    "        for i in range (args.test_size+(nstep-1)):\n",
    "            print(\"🍒\",i)\n",
    "            history = trainlist[:-args.test_size-(nstep-1-i)]\n",
    "            model = SARIMAX(history, \n",
    "                            order=order, \n",
    "                            seasonal_order=sorder, \n",
    "                            trend = trend,\n",
    "                            enforce_stationarity=False,\n",
    "                            enforce_invertibility=False)\n",
    "            model_fit = model.fit(disp=False)\n",
    "            yhat = model_fit.predict(len(history), len(history) + nstep - 1)\n",
    "            predict_list.append(yhat)\n",
    "\n",
    "        y_pred_list = []\n",
    "        for step in range(nstep):\n",
    "            moving = nstep-1-step\n",
    "            y_pred_list.append([x[step] for x in predict_list][moving:args.test_size+moving])\n",
    "\n",
    "        df_eval_true_inverse = df_full[-args.test_size:]\n",
    "        y_true = scaler.inverse_transform(df_eval_true_inverse.iloc[:,:-1])[:,[-1]].reshape(args.test_size)\n",
    "\n",
    "        y_pred_inverse_list = []\n",
    "        for step in range(nstep):\n",
    "            df_eval_pred_inverse = df_full[-args.test_size:]\n",
    "            df_eval_pred_inverse[args.labels]= y_pred_list[step] #step 1\n",
    "            y_pred_inverse = scaler.inverse_transform(df_eval_pred_inverse.iloc[:,:-1])[:,[-1]].reshape(args.test_size)\n",
    "            y_pred_inverse_list.append(y_pred_inverse)\n",
    "\n",
    "            df_compare_test_predict = pd.DataFrame({'y_true':y_true, 'y_pred':y_pred_inverse})\n",
    "            df_compare_test_predict.plot()\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "        df_results[\"Date\"] = df_eval[\"year_month\"][- args.test_size:]\n",
    "        df_results[\"City\"] = city\n",
    "        df_results[\"Observed\"] = y_true\n",
    "\n",
    "        for step in range(args.n_predicted_period_months):\n",
    "            df_results[f\"MSE_{step+1}-month\"] = mean_squared_error(y_true, y_pred_inverse_list[step])\n",
    "            df_results[f\"MAE_{step+1}-month\"] = mean_absolute_error(y_true, y_pred_inverse_list[step])\n",
    "            df_results[f\"RMSE_{step+1}-month\"] = mean_squared_error(y_true, y_pred_inverse_list[step])**0.5\n",
    "            df_results[f\"MAPE_{step+1}-month\"] = mean_absolute_percentage_error(y_true, y_pred_inverse_list[step])\n",
    "            df_results[f\"{step+1}-month\"] = y_pred_inverse_list[step]\n",
    "\n",
    "        result_columns = ['Date', 'City', 'Observed',\n",
    "                            'MSE_1-month', 'MAE_1-month', 'RMSE_1-month', 'MAPE_1-month',  \n",
    "                            'MSE_2-month', 'MAE_2-month', 'RMSE_2-month', 'MAPE_2-month',  \n",
    "                            'MSE_3-month', 'MAE_3-month', 'RMSE_3-month', 'MAPE_3-month',\n",
    "                            'MSE_4-month', 'MAE_4-month', 'RMSE_4-month', 'MAPE_4-month', \n",
    "                            'MSE_5-month', 'MAE_5-month', 'RMSE_5-month', 'MAPE_5-month',  \n",
    "                            'MSE_6-month', 'MAE_6-month', 'RMSE_6-month', 'MAPE_6-month',\n",
    "                            '1-month', '2-month', '3-month', '4-month', '5-month', '6-month', \n",
    "                            ]\n",
    "        df_results = df_results.reset_index()[result_columns]\n",
    "        folder_save_path = f\"./metric_results/{model_name}/\"\n",
    "        if os.path.isdir(folder_save_path):\n",
    "        df_results.to_excel(f\"./metric_results/{model_name}/0_train_6nstep_denguefever_prediction_results_by_{model_name}_in_{city}.xlsx\")\n",
    "        \n",
    "        \n",
    "        # df.to_excel(f\"./predict_results/{model_name}/0_train_{nstep}nstep_denguefever_prediction_results_by_{model_name}_in_{city}.xlsx\")\n",
    "        # pickle.dump(model, open(f\"./trained_models/{model_name}/{nstep}nstep_denguefever_{model_name}_in_{city}.sav\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Darts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✨✨✨✨✨✨✨✨NBEATSModel✨✨✨✨✨✨✨✨✨✨\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cities' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/trinhtruc/Documents/STUDY/NCKH/Source/Source_14012023_v4/predict_model/\u001d011123_train_predict_multioutput.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/trinhtruc/Documents/STUDY/NCKH/Source/Source_14012023_v4/predict_model/%1D011123_train_predict_multioutput.ipynb#X25sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m model_name \u001b[39min\u001b[39;00m model_name_list:   \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/trinhtruc/Documents/STUDY/NCKH/Source/Source_14012023_v4/predict_model/%1D011123_train_predict_multioutput.ipynb#X25sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m✨✨✨✨✨✨✨✨\u001b[39m\u001b[39m{\u001b[39;00mmodel_name\u001b[39m}\u001b[39;00m\u001b[39m✨✨✨✨✨✨✨✨✨✨\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/trinhtruc/Documents/STUDY/NCKH/Source/Source_14012023_v4/predict_model/%1D011123_train_predict_multioutput.ipynb#X25sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39mfor\u001b[39;00m city \u001b[39min\u001b[39;00m cities:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/trinhtruc/Documents/STUDY/NCKH/Source/Source_14012023_v4/predict_model/%1D011123_train_predict_multioutput.ipynb#X25sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m✨✨✨✨✨✨✨✨\u001b[39m\u001b[39m{\u001b[39;00mcity\u001b[39m}\u001b[39;00m\u001b[39m✨✨✨✨✨✨✨✨✨✨\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/trinhtruc/Documents/STUDY/NCKH/Source/Source_14012023_v4/predict_model/%1D011123_train_predict_multioutput.ipynb#X25sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         df_train \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(output_process\u001b[39m+\u001b[39mcity\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m_train_preprocessed.csv\u001b[39m\u001b[39m'\u001b[39m, parse_dates\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, index_col\u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, encoding \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39municode_escape\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cities' is not defined"
     ]
    }
   ],
   "source": [
    "model_name_list = [\n",
    "    #  \"BlockRNNModel\",\n",
    "     \"NBEATSModel\",\n",
    "    #  \"NHiTSModel\",\n",
    "    #  \"TFTModel\",\n",
    "    # \"TCNModel\", #still bug\n",
    "]\n",
    "\n",
    "for model_name in model_name_list:   \n",
    "    print(f\"✨✨✨✨✨✨✨✨{model_name}✨✨✨✨✨✨✨✨✨✨\")\n",
    "    for city in cities:\n",
    "        print(f\"✨✨✨✨✨✨✨✨{city}✨✨✨✨✨✨✨✨✨✨\")\n",
    "        df_train = pd.read_csv(output_process+city+'_train_preprocessed.csv', parse_dates=True, index_col= None, encoding = 'unicode_escape')\n",
    "        df_eval = pd.read_csv(output_process+city+'_test_preprocessed.csv', parse_dates=True, index_col= None, encoding = 'unicode_escape')\n",
    "        scaler = joblib.load(output_process+city+'_train_scalerMinMaxNorm.save') #ok\n",
    "        nstep = args.n_predicted_period_months\n",
    "        selected_features = getDataWithSelectedFeature(city, nstep)\n",
    "\n",
    "        lags_by_nstep = args.look_back + nstep - 1\n",
    "        lags_past_covariates_by_nstep = [-lags_by_nstep+2,-lags_by_nstep+1,-lags_by_nstep] #Mảng này chứa ba giá trị tương ứng cho args.lookback 3\n",
    "\n",
    "        pl_trainer_kwargs = {\n",
    "                    \"accelerator\": \"cpu\",\n",
    "                    # \"devices\": -1,\n",
    "                    # \"auto_select_gpus\": True,\n",
    "                }\n",
    "\n",
    "        df_hyper_params = getHyperParams(model_name, city)\n",
    "\n",
    "        if model_name == \"BlockRNNModel\":\n",
    "            #suggest hyperparams\n",
    "            hidden_dim = df_hyper_params['hidden_dim'].values[0]\n",
    "            n_rnn_layers = df_hyper_params['n_rnn_layers'].values[0]\n",
    "            dropout = df_hyper_params['dropout'].values[0]\n",
    "            n_epochs = df_hyper_params['n_epochs'].values[0]\n",
    "            random_state = df_hyper_params['random_state'].values[0]\n",
    "\n",
    "            model = BlockRNNModel(\n",
    "                                input_chunk_length = args.look_back,\n",
    "                                output_chunk_length = args.n_predicted_period_months,\n",
    "                                hidden_dim = hidden_dim,\n",
    "                                n_rnn_layers = n_rnn_layers,\n",
    "                                dropout = dropout,\n",
    "                                n_epochs = 2,\n",
    "                                pl_trainer_kwargs = pl_trainer_kwargs,\n",
    "                                random_state=random_state)\n",
    "        elif model_name == 'TFTModel':\n",
    "            # Define the hyperparameters to optimize\n",
    "            dropout = df_hyper_params['dropout'].values[0]\n",
    "            n_epochs = df_hyper_params['n_epochs'].values[0]\n",
    "            random_state = df_hyper_params['random_state'].values[0]\n",
    "            # Create the TFTModel model\n",
    "            model = TFTModel(\n",
    "                            input_chunk_length = args.look_back,\n",
    "                            output_chunk_length = args.n_predicted_period_months,\n",
    "                            add_relative_index = True,\n",
    "                            dropout = dropout,\n",
    "                            n_epochs = 2,\n",
    "                            pl_trainer_kwargs = pl_trainer_kwargs,\n",
    "                            random_state=random_state)\n",
    "        elif model_name == 'NHiTSModel':\n",
    "            #suggest hyperparams\n",
    "            MaxPool1d = bool(df_hyper_params['MaxPool1d'].values[0])\n",
    "            dropout = df_hyper_params['dropout'].values[0]\n",
    "            n_epochs = df_hyper_params['n_epochs'].values[0]\n",
    "            random_state = df_hyper_params['random_state'].values[0]\n",
    "            model = NHiTSModel(\n",
    "                                input_chunk_length = args.look_back,\n",
    "                                output_chunk_length = args.n_predicted_period_months,\n",
    "                                MaxPool1d = MaxPool1d,\n",
    "                                dropout = dropout,\n",
    "                                n_epochs = 2 ,\n",
    "                                pl_trainer_kwargs = pl_trainer_kwargs,\n",
    "                                random_state=random_state)\n",
    "        elif model_name == \"NBEATSModel\":\n",
    "            dropout = df_hyper_params['dropout'].values[0]\n",
    "            n_epochs = df_hyper_params['n_epochs'].values[0]\n",
    "            random_state = df_hyper_params['random_state'].values[0]\n",
    "            model = NBEATSModel(\n",
    "                                    input_chunk_length = args.look_back,\n",
    "                                    output_chunk_length = args.n_predicted_period_months,\n",
    "                                    dropout = dropout,\n",
    "                                    n_epochs = 2 ,\n",
    "                                    pl_trainer_kwargs = pl_trainer_kwargs,\n",
    "                                    random_state=random_state)\n",
    "        elif model_name == \"TCNModel\":\n",
    "            params = {\n",
    "                'kernel_size': df_hyper_params[\"kernel_size\"].values[0],\n",
    "                'num_filters': df_hyper_params[\"num_filters\"].values[0],\n",
    "                'weight_norm': df_hyper_params[\"weight_norm\"].values[0],\n",
    "                'dilation_base': df_hyper_params[\"dilation_base\"].values[0],\n",
    "                'dropout': df_hyper_params[\"dropout\"].values[0],\n",
    "                'learning_rate': df_hyper_params[\"learning_rate\"].values[0],\n",
    "                'include_year': df_hyper_params[\"year\"].values[0],\n",
    "                'n_epochs': df_hyper_params[\"n_epochs\"].values[0],\n",
    "            }\n",
    "            # select input and output chunk lengths\n",
    "            params['input_chunk_length'] = args.look_back\n",
    "            params['output_chunk_length'] = args.n_predicted_period_months  \n",
    "            # optionally also add the (scaled) year value as a past covariate\n",
    "            if params['include_year']:\n",
    "                encoders = {\"datetime_attribute\": {\"past\": [\"year\"]},\n",
    "                            \"transformer\": Scaler()}\n",
    "            else:\n",
    "                encoders = None\n",
    "            params['encoders'] = encoders\n",
    "            param = params\n",
    "            model = TCNModel(\n",
    "                input_chunk_length=param['input_chunk_length'],\n",
    "                output_chunk_length=param['output_chunk_length'],\n",
    "                batch_size=16,\n",
    "                n_epochs=param['n_epochs'],\n",
    "                nr_epochs_val_period=1,\n",
    "                kernel_size=param['kernel_size'],\n",
    "                num_filters=param['num_filters'],\n",
    "                weight_norm=param['weight_norm'],\n",
    "                dilation_base=param['dilation_base'],\n",
    "                dropout=param['dropout'],\n",
    "                optimizer_kwargs={\"lr\": param['learning_rate']},\n",
    "                add_encoders=param['encoders'],\n",
    "                likelihood=GaussianLikelihood(),\n",
    "                pl_trainer_kwargs=pl_trainer_kwargs,\n",
    "                model_name=\"tcn_model\",\n",
    "                force_reset=True,\n",
    "                save_checkpoints=True,\n",
    "            )\n",
    "              \n",
    "        df, model = output_prediction_for_location(df_train, df_eval, model, location=city, feature_list=selected_features,\n",
    "                                                            labels=args.labels, scaler=scaler)\n",
    "        df.to_excel(f\"./predict_results/{model_name}/0_train_{nstep}nstep_denguefever_prediction_results_by_{model_name}_in_{city}.xlsx\")\n",
    "        pickle.dump(model, open(f\"./trained_models/{model_name}/{nstep}nstep_denguefever_{model_name}_in_{city}.sav\", 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
