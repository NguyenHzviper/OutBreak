{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: ftfy in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (6.1.1)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from ftfy) (0.2.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: optuna in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (3.4.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from optuna) (1.12.0)\n",
      "Requirement already satisfied: colorlog in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from optuna) (6.7.0)\n",
      "Requirement already satisfied: numpy in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from optuna) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from optuna) (23.1)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from optuna) (2.0.22)\n",
      "Requirement already satisfied: tqdm in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from optuna) (4.66.1)\n",
      "Requirement already satisfied: PyYAML in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from optuna) (6.0.1)\n",
      "Requirement already satisfied: Mako in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from alembic>=1.5.0->optuna) (1.2.4)\n",
      "Requirement already satisfied: typing-extensions>=4 in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from alembic>=1.5.0->optuna) (4.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: darts in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (0.26.0)\n",
      "Requirement already satisfied: holidays>=0.11.1 in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from darts) (0.35)\n",
      "Requirement already satisfied: joblib>=0.16.0 in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from darts) (1.3.2)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from darts) (3.8.0)\n",
      "Requirement already satisfied: nfoursid>=1.0.0 in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from darts) (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from darts) (1.24.3)\n",
      "Requirement already satisfied: pmdarima>=1.8.0 in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from darts) (2.0.4)\n",
      "Requirement already satisfied: pyod>=0.9.5 in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from darts) (1.1.1)\n",
      "Requirement already satisfied: requests>=2.22.0 in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from darts) (2.31.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.1 in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from darts) (1.3.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from darts) (1.11.2)\n",
      "Requirement already satisfied: shap>=0.40.0 in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from darts) (0.43.0)\n",
      "Requirement already satisfied: statsforecast>=1.4 in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from darts) (1.6.0)\n",
      "Requirement already satisfied: statsmodels>=0.14.0 in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from darts) (0.14.0)\n",
      "Requirement already satisfied: tbats>=1.1.0 in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from darts) (1.1.3)\n",
      "Requirement already satisfied: tqdm>=4.60.0 in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from darts) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from darts) (4.5.0)\n",
      "Requirement already satisfied: xarray>=0.17.0 in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from darts) (2023.10.1)\n",
      "Requirement already satisfied: xgboost>=1.6.0 in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from darts) (2.0.0)\n",
      "Requirement already satisfied: pytorch-lightning>=1.5.0 in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from darts) (2.1.0)\n",
      "Requirement already satisfied: tensorboardX>=2.1 in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from darts) (2.6.2.2)\n",
      "Requirement already satisfied: torch>=1.8.0 in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from darts) (1.13.1)\n",
      "Requirement already satisfied: pandas>=1.0.5 in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from darts) (2.1.0)\n",
      "Requirement already satisfied: python-dateutil in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from holidays>=0.11.1->darts) (2.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from matplotlib>=3.3.0->darts) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from matplotlib>=3.3.0->darts) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from matplotlib>=3.3.0->darts) (4.42.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from matplotlib>=3.3.0->darts) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from matplotlib>=3.3.0->darts) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from matplotlib>=3.3.0->darts) (10.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from matplotlib>=3.3.0->darts) (3.1.1)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from matplotlib>=3.3.0->darts) (6.0.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from pandas>=1.0.5->darts) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from pandas>=1.0.5->darts) (2023.3)\n",
      "Requirement already satisfied: Cython!=0.29.18,!=0.29.31,>=0.29 in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from pmdarima>=1.8.0->darts) (3.0.4)\n",
      "Requirement already satisfied: urllib3 in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from pmdarima>=1.8.0->darts) (1.26.16)\n",
      "Requirement already satisfied: setuptools!=50.0.0,>=38.6.0 in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from pmdarima>=1.8.0->darts) (57.0.0)\n",
      "Requirement already satisfied: numba>=0.51 in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from pyod>=0.9.5->darts) (0.58.0)\n",
      "Requirement already satisfied: six in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from pyod>=0.9.5->darts) (1.15.0)\n",
      "Requirement already satisfied: PyYAML>=5.4 in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from pytorch-lightning>=1.5.0->darts) (6.0.1)\n",
      "Requirement already satisfied: fsspec>2021.06.0 in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from fsspec[http]>2021.06.0->pytorch-lightning>=1.5.0->darts) (2023.9.2)\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from pytorch-lightning>=1.5.0->darts) (1.2.0)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from pytorch-lightning>=1.5.0->darts) (0.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from requests>=2.22.0->darts) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from requests>=2.22.0->darts) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from requests>=2.22.0->darts) (2023.7.22)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from scikit-learn>=1.0.1->darts) (3.2.0)\n",
      "Requirement already satisfied: slicer==0.0.7 in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from shap>=0.40.0->darts) (0.0.7)\n",
      "Requirement already satisfied: cloudpickle in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from shap>=0.40.0->darts) (3.0.0)\n",
      "Requirement already satisfied: polars in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from statsforecast>=1.4->darts) (0.19.11)\n",
      "Requirement already satisfied: fugue>=0.8.1 in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from statsforecast>=1.4->darts) (0.8.6)\n",
      "Requirement already satisfied: patsy>=0.5.2 in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from statsmodels>=0.14.0->darts) (0.5.3)\n",
      "Requirement already satisfied: protobuf>=3.20 in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from tensorboardX>=2.1->darts) (4.23.4)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from fsspec[http]>2021.06.0->pytorch-lightning>=1.5.0->darts) (3.8.6)\n",
      "Requirement already satisfied: triad>=0.9.1 in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from fugue>=0.8.1->statsforecast>=1.4->darts) (0.9.1)\n",
      "Requirement already satisfied: adagio>=0.2.4 in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from fugue>=0.8.1->statsforecast>=1.4->darts) (0.2.4)\n",
      "Requirement already satisfied: pyarrow>=0.15.1 in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from fugue>=0.8.1->statsforecast>=1.4->darts) (13.0.0)\n",
      "Requirement already satisfied: qpd>=0.4.4 in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from fugue>=0.8.1->statsforecast>=1.4->darts) (0.4.4)\n",
      "Requirement already satisfied: fugue-sql-antlr>=0.1.6 in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from fugue>=0.8.1->statsforecast>=1.4->darts) (0.1.8)\n",
      "Requirement already satisfied: sqlglot in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from fugue>=0.8.1->statsforecast>=1.4->darts) (18.16.1)\n",
      "Requirement already satisfied: jinja2 in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from fugue>=0.8.1->statsforecast>=1.4->darts) (3.1.2)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from importlib-resources>=3.2.0->matplotlib>=3.3.0->darts) (3.16.2)\n",
      "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from numba>=0.51->pyod>=0.9.5->darts) (0.41.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.5.0->darts) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.5.0->darts) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.5.0->darts) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.5.0->darts) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.5.0->darts) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.5.0->darts) (1.3.1)\n",
      "Requirement already satisfied: antlr4-python3-runtime<4.12 in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from fugue-sql-antlr>=0.1.6->fugue>=0.8.1->statsforecast>=1.4->darts) (4.11.1)\n",
      "Requirement already satisfied: fs in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from triad>=0.9.1->fugue>=0.8.1->statsforecast>=1.4->darts) (2.4.16)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from jinja2->fugue>=0.8.1->statsforecast>=1.4->darts) (2.1.3)\n",
      "Requirement already satisfied: appdirs~=1.4.3 in /Users/trinhtruc/Library/Python/3.9/lib/python/site-packages (from fs->triad>=0.9.1->fugue>=0.8.1->statsforecast>=1.4->darts) (1.4.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install packages\n",
    "# %pip install -U scikit-learn\n",
    "%pip install ftfy\n",
    "%pip install optuna\n",
    "%pip install darts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from darts import TimeSeries\n",
    "from darts.models import KalmanForecaster, RandomForest, LinearRegressionModel, LightGBMModel, \\\n",
    "                        CatBoostModel, RNNModel, BlockRNNModel, NBEATSModel, NHiTSModel, \\\n",
    "                        TCNModel, TransformerModel, TFTModel\n",
    "from darts.metrics import mae\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "\n",
    "from darts.utils.likelihood_models import GaussianLikelihood\n",
    "\n",
    "from ftfy import fix_text\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prj_path = '/home/mlworker/Quang/HealthCare/Source_14012023_v4/'\n",
    "# data_path = prj_path + \"data/new_data/DH/squeezed/\"\n",
    "# prj_path_opt= prj_path + \"optimize_hyperparam/opt_results/LinearRegression_27022023/\"\n",
    "\n",
    "# os.chdir(prj_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pwd\n",
    "# /Source_14012023_v4/preprocessing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "prj_path = '../'\n",
    "data_path = prj_path + \"data/new_data/DH/squeezed/\"\n",
    "prj_path_opt= prj_path + \"optimize_hyperparam/opt_results/\"\n",
    "output_process = prj_path + \"data/new_data/DH/processed_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = {0 : \"train\", 1 : \"test\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cities = [\n",
    "        'An Giang', 'BR Vũng Tàu', 'Bình Phước', 'Bình Thuận', 'Bình Định',\n",
    "        'Bạc Liêu', 'Bắc Kạn', 'Bắc Giang', 'Cao Bằng', 'Cà Mau',\n",
    "        'Cần Thơ', 'Gia Lai', 'Hà Giang', 'Hà Nội', 'Hà Tĩnh',\n",
    "        'Hòa Bình','Hưng Yên', 'Hải Dương', 'Hải Phòng', 'Khánh Hòa', 'Kiên Giang',\n",
    "        'Kon Tum', 'Lai Châu', 'Long An', 'Lào Cai', 'Lâm Đồng',\n",
    "        'Lạng Sơn','Nam Định', 'Nghệ An', 'Ninh Bình', 'Ninh Thuận',\n",
    "        'Phú Thọ', 'Phú Yên', 'Quảng Bình', 'Quảng Nam', 'Quảng Ngãi',\n",
    "        'Quảng Ninh', 'Quảng Trị', 'Sóc Trăng', 'Sơn La', 'TT Huế',\n",
    "        'Thanh Hóa', 'Thái Bình', 'Thái Nguyên', 'Tiền Giang', 'Trà Vinh',\n",
    "        'Tuyên Quang', 'Tây Ninh', 'Vĩnh Phúc', 'Yên Bái', 'Điện Biên',\n",
    "        'Đà Nẵng', 'Đắk Nông', 'Đắk Lắk', 'Đồng Tháp'\n",
    "]\n",
    "cities = ['An Giang']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters as args using the Configuration class\n",
    "class Configuration():\n",
    "    def __init__(self):\n",
    "      # lấy bộ test dài 36 tháng = 3 năm\n",
    "        self.test_size = 36\n",
    "        # là nhìn vào dữ liệu trước 3 tháng và dự phóng        \n",
    "        self.look_back = 3\n",
    "        # dự phóng n-step trong 6 tháng\n",
    "        self.n_predicted_period_months = 6\n",
    "        self.n_features = 3\n",
    "        self.seed = 42\n",
    "        # mỗi phần tử x trong tập suppervise có độ lớn là 16 = 16 tháng\n",
    "        self.batch_size = 16\n",
    "        self.device = torch.device(\"cuda\")\n",
    "        self.epochs = 300\n",
    "\n",
    "args = Configuration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    import random, os\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    \n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supporting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict_all_city_data():\n",
    "  cities_data = {}  \n",
    "  for city in cities:\n",
    "    city_result = pd.read_excel(prj_path+'data/new_data/DH/squeezed/squeezed_'+city+'.xlsx')  \n",
    "    \"\"\"Get all data from all city in 1997 - 2016\"\"\" \n",
    "    city_result = city_result.loc[city_result['year_month'] < '2017-1-1'] \n",
    "    cities_data[city] = city_result\n",
    "  return cities_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data (pre-)processing functions\n",
    "# modification\n",
    "def get_city_data(city_name, dict_full_data):\n",
    "    \"\"\"Returns Diarrhoea rate and climate data\"\"\" \n",
    "    city_data = dict_full_data[city_name].drop(columns=['Diarrhoea_cases','Diarrhoea_rates', 'province',\n",
    "                                                        'Influenza_rates','Influenza_cases',\n",
    "                                                        'Dengue_fever_cases', 'year', 'month'], \n",
    "                                                                  axis=1, \n",
    "                                                                  inplace=False)    \n",
    "    return city_data\n",
    "\n",
    "def convert_to_stationary(city_data):\n",
    "    \"\"\"Subtracts previous value for all cols except disease rates\"\"\"\n",
    "    for col_name in city_data.columns:\n",
    "        if col_name != 'Diarrhoea_rates':\n",
    "            try:\n",
    "                city_data[col_name] = city_data[col_name] - city_data[col_name].shift()\n",
    "            except:\n",
    "                print(col_name)\n",
    "    return city_data\n",
    "\n",
    "def impute_missing_value(city_data):\n",
    "    \"\"\"\n",
    "    Imputes 0 for first 12 months, \n",
    "    last year's value for months 12-24, \n",
    "    and minimum value of last two years for months 25+\n",
    "    \"\"\"\n",
    "    for col in city_data.columns:\n",
    "        for index in range(len(city_data[col])):\n",
    "            if np.isnan(city_data[col].iloc[index]):\n",
    "                if index < 12:\n",
    "                    city_data[col].iloc[index] = 0\n",
    "                elif index >= 12 and index <= 24:\n",
    "                    city_data[col].iloc[index] = city_data[col].iloc[index - 12]\n",
    "                else:\n",
    "                    city_data[col].iloc[index] = min(city_data[col].iloc[index - 12], city_data[col].iloc[index - 24])\n",
    "    return city_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_full_data(dict_full_data):\n",
    "    climate_and_disease_feats = ['Total_Evaporation',\n",
    "       'Total_Rainfall', 'Max_Daily_Rainfall', 'n_raining_days',\n",
    "       'Average_temperature', 'Max_Average_Temperature',\n",
    "       'Min_Average_Temperature', 'Max_Absolute_Temperature',\n",
    "       'Min_Absolute_Temperature', 'Average_Humidity', 'Min_Humidity',\n",
    "       'n_hours_sunshine', 'Dengue_fever_rates']\n",
    "    for city in cities:\n",
    "        city_data = get_city_data(city_name=city,dict_full_data = dict_full_data)\n",
    "        city_data_features = city_data[climate_and_disease_feats]\n",
    "        city_data_features = impute_missing_value(city_data_features)\n",
    "        city_data_features = convert_to_stationary(city_data_features)\n",
    "        city_data_features.dropna(inplace=True)\n",
    "        city_data_features.loc[:, \"year_month\"] = city_data[\"year_month\"]\n",
    "        dict_full_data[city] = city_data_features\n",
    "    return dict_full_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, look_back ):\n",
    "    \"\"\"Splits data into train and test sets based on args (Configuration class)\"\"\"\n",
    "    train = data[: -args.test_size]    \n",
    "    test = data[-args.test_size - look_back: ]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_supervised(data,  d_out, d_in, features_list=[]):\n",
    "    \"\"\"\n",
    "    Frames time-series as supervised learning dataset.\n",
    "    \n",
    "    Args:\n",
    "      d_in: lookback window\n",
    "      d_out: number of predicted months\n",
    "      features_list: list of all features **where last col is the disease incidence**\n",
    "\n",
    "    Returns:\n",
    "      Numpy arrays of disease incidence (y) and other predictors (X)\n",
    "    \"\"\"\n",
    "    X, y = list(), list()\n",
    "    for index, _ in enumerate(data):\n",
    "        in_end = index + d_in\n",
    "        out_end = in_end + d_out\n",
    "        if out_end <= len(data):\n",
    "            if len(features_list) == 0 :\n",
    "                X.append(data[index: in_end, :])\n",
    "            else:\n",
    "                X.append(data[index: in_end, features_list])\n",
    "            y.append(data[out_end-1: out_end, -1])\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_feature(train, next_predicted_month):\n",
    "    \"\"\"Selects args.n_features top features using RFE\"\"\"\n",
    "    train_X, train_y = to_supervised(train, d_out=next_predicted_month, d_in=args.look_back )\n",
    "    print(\"Shape train_X: \", train_X.shape)\n",
    "    print(\"Shape train_y: \", train_y.shape)\n",
    "    train_X, train_y = np.squeeze(train_X), np.squeeze(train_y)\n",
    "    rfe = RFE(RandomForestRegressor(n_estimators=500, random_state=args.seed), n_features_to_select=args.n_features)\n",
    "    fit = rfe.fit(train_X, train_y)\n",
    "    important_features = list()\n",
    "    for i in range(len(fit.support_)):\n",
    "        if fit.support_[i]:\n",
    "            important_features.append(i)\n",
    "    return np.array(important_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_y = to_supervised(train, d_out=next_predicted_month, d_in=args.look_back )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureSelection(Xtrain, Ytrain, n_selection_feature, n_selection_feature_by_day = 18):\n",
    "  D = Xtrain.shape[2]\n",
    "  \n",
    "  rfe = RFE(RandomForestRegressor(), n_features_to_select=n_selection_feature_by_day)\n",
    "\n",
    "  fit = rfe.fit(Xtrain.reshape(len(Xtrain),D*args.look_back), Ytrain)\n",
    "  important_features = list()\n",
    "  for i in range(len(fit.support_)):\n",
    "      if fit.support_[i]:\n",
    "          important_features.append(i)\n",
    "  result = np.array(important_features)\n",
    "  calMostFeature = [0]*D\n",
    "  for i in result:\n",
    "    calMostFeature[i%D] = calMostFeature[i%D] + 1\n",
    "  top_idx =  np.sort(np.argsort(calMostFeature)[-n_selection_feature:])\n",
    "  return top_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(train_np, test_np, batch_size, specific_data):\n",
    "    \"\"\"\n",
    "    Returns important feature list and data formatted for input into Pytorch \n",
    "    models\n",
    "    \"\"\"\n",
    "    important_features = select_feature(train_np, specific_data)\n",
    "\n",
    "    train_X, train_y = to_supervised(train_np, features_list=important_features)\n",
    "\n",
    "    test_X, test_y = to_supervised(test_np, features_list=important_features)\n",
    "    train_tensor = TensorDataset(torch.from_numpy(train_X), torch.from_numpy(train_y))\n",
    "    test_tensor = (torch.from_numpy(test_X), torch.from_numpy(test_y))\n",
    "\n",
    "    train_loader = DataLoader(train_tensor, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return important_features, train_loader, test_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizationMinMax(df,city,data_set_index):\n",
    "    date_index = df.index\n",
    "    norm_set = data_set[data_set_index]\n",
    "    scaler = MinMaxScaler()\n",
    "    if norm_set == \"train\":\n",
    "        scaler.fit(df)\n",
    "        series = scaler.transform(df)\n",
    "        df_scaled = pd.DataFrame(data = series, columns = df.columns)\n",
    "        joblib.dump(scaler, output_process+city+'_train_scalerMinMaxNorm.save')\n",
    "\n",
    "    else:\n",
    "        true_incidence = df.iloc[:, [-1]]\n",
    "        scaler = joblib.load(output_process+city+'_train_scalerMinMaxNorm.save') #ok\n",
    "        series = scaler.transform(df)\n",
    "        df_scaled = pd.DataFrame(data = series, columns = df.columns)\n",
    "        df_scaled.iloc[:, [-1]] = true_incidence\n",
    "    df_scaled[\"year_month\"] = date_index\n",
    "    \"\"\"Save data as csv, when load data as a dataframe, use command df.iloc[:,:-1].to_numpy() to convert to an array to use\"\"\"\n",
    "    df_scaled.to_csv(output_process+city+'_'+norm_set+'_preprocessed.csv', float_format='%.7f',index=False)\n",
    "    return df_scaled,scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_full_data = get_dict_all_city_data()\n",
    "full_data = clean_full_data(dict_full_data=dict_full_data)\n",
    "for city in cities:\n",
    "    specific_data = full_data[city]\n",
    "    specific_data = specific_data.set_index(\"year_month\")\n",
    "    train, test = split_data(specific_data, args.look_back)\n",
    "    df_train,scaler = normalizationMinMax(train,city, 0)\n",
    "    df_test,scaler = normalizationMinMax(test,city, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape train_X:  (199, 3, 13)\n",
      "Shape train_y:  (199, 2)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 3. RFE expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/trinhtruc/Documents/STUDY/NCKH/Source/Source_14012023_v4/preprocessing_data/preprocessing_data.ipynb Cell 25\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/trinhtruc/Documents/STUDY/NCKH/Source/Source_14012023_v4/preprocessing_data/preprocessing_data.ipynb#Y133sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m train \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39miloc[:,:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mto_numpy()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/trinhtruc/Documents/STUDY/NCKH/Source/Source_14012023_v4/preprocessing_data/preprocessing_data.ipynb#Y133sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m next_predicted_month \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/trinhtruc/Documents/STUDY/NCKH/Source/Source_14012023_v4/preprocessing_data/preprocessing_data.ipynb#Y133sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m important_features \u001b[39m=\u001b[39m select_feature(train, next_predicted_month)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/trinhtruc/Documents/STUDY/NCKH/Source/Source_14012023_v4/preprocessing_data/preprocessing_data.ipynb#Y133sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(important_features)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/trinhtruc/Documents/STUDY/NCKH/Source/Source_14012023_v4/preprocessing_data/preprocessing_data.ipynb#Y133sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m list_features \u001b[39m=\u001b[39m full_data[city]\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mtolist()\n",
      "\u001b[1;32m/Users/trinhtruc/Documents/STUDY/NCKH/Source/Source_14012023_v4/preprocessing_data/preprocessing_data.ipynb Cell 25\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/trinhtruc/Documents/STUDY/NCKH/Source/Source_14012023_v4/preprocessing_data/preprocessing_data.ipynb#Y133sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m train_X, train_y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msqueeze(train_X), np\u001b[39m.\u001b[39msqueeze(train_y)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/trinhtruc/Documents/STUDY/NCKH/Source/Source_14012023_v4/preprocessing_data/preprocessing_data.ipynb#Y133sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m rfe \u001b[39m=\u001b[39m RFE(RandomForestRegressor(n_estimators\u001b[39m=\u001b[39m\u001b[39m500\u001b[39m, random_state\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mseed), n_features_to_select\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mn_features)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/trinhtruc/Documents/STUDY/NCKH/Source/Source_14012023_v4/preprocessing_data/preprocessing_data.ipynb#Y133sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m fit \u001b[39m=\u001b[39m rfe\u001b[39m.\u001b[39;49mfit(train_X, train_y)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/trinhtruc/Documents/STUDY/NCKH/Source/Source_14012023_v4/preprocessing_data/preprocessing_data.ipynb#Y133sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m important_features \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/trinhtruc/Documents/STUDY/NCKH/Source/Source_14012023_v4/preprocessing_data/preprocessing_data.ipynb#Y133sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(fit\u001b[39m.\u001b[39msupport_)):\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/feature_selection/_rfe.py:249\u001b[0m, in \u001b[0;36mRFE.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[39m@_fit_context\u001b[39m(\n\u001b[1;32m    226\u001b[0m     \u001b[39m# RFE.estimator is not validated yet\u001b[39;00m\n\u001b[1;32m    227\u001b[0m     prefer_skip_nested_validation\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    228\u001b[0m )\n\u001b[1;32m    229\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params):\n\u001b[1;32m    230\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Fit the RFE model and then the underlying estimator on the selected features.\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \n\u001b[1;32m    232\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 249\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/feature_selection/_rfe.py:258\u001b[0m, in \u001b[0;36mRFE._fit\u001b[0;34m(self, X, y, step_score, **fit_params)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_fit\u001b[39m(\u001b[39mself\u001b[39m, X, y, step_score\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params):\n\u001b[1;32m    252\u001b[0m     \u001b[39m# Parameter step_score controls the calculation of self.scores_\u001b[39;00m\n\u001b[1;32m    253\u001b[0m     \u001b[39m# step_score is not exposed to users\u001b[39;00m\n\u001b[1;32m    254\u001b[0m     \u001b[39m# and is used when implementing RFECV\u001b[39;00m\n\u001b[1;32m    255\u001b[0m     \u001b[39m# self.scores_ will not be calculated when calling _fit through fit\u001b[39;00m\n\u001b[1;32m    257\u001b[0m     tags \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_tags()\n\u001b[0;32m--> 258\u001b[0m     X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[1;32m    259\u001b[0m         X,\n\u001b[1;32m    260\u001b[0m         y,\n\u001b[1;32m    261\u001b[0m         accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsc\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    262\u001b[0m         ensure_min_features\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m,\n\u001b[1;32m    263\u001b[0m         force_all_finite\u001b[39m=\u001b[39;49m\u001b[39mnot\u001b[39;49;00m tags\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mallow_nan\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mTrue\u001b[39;49;00m),\n\u001b[1;32m    264\u001b[0m         multi_output\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    265\u001b[0m     )\n\u001b[1;32m    267\u001b[0m     \u001b[39m# Initialization\u001b[39;00m\n\u001b[1;32m    268\u001b[0m     n_features \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/base.py:621\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    619\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[1;32m    620\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 621\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    622\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[1;32m    624\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:1147\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1142\u001b[0m         estimator_name \u001b[39m=\u001b[39m _check_estimator_name(estimator)\n\u001b[1;32m   1143\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1144\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m requires y to be passed, but the target y is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1145\u001b[0m     )\n\u001b[0;32m-> 1147\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m   1148\u001b[0m     X,\n\u001b[1;32m   1149\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[1;32m   1150\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49maccept_large_sparse,\n\u001b[1;32m   1151\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m   1152\u001b[0m     order\u001b[39m=\u001b[39;49morder,\n\u001b[1;32m   1153\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m   1154\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[1;32m   1155\u001b[0m     ensure_2d\u001b[39m=\u001b[39;49mensure_2d,\n\u001b[1;32m   1156\u001b[0m     allow_nd\u001b[39m=\u001b[39;49mallow_nd,\n\u001b[1;32m   1157\u001b[0m     ensure_min_samples\u001b[39m=\u001b[39;49mensure_min_samples,\n\u001b[1;32m   1158\u001b[0m     ensure_min_features\u001b[39m=\u001b[39;49mensure_min_features,\n\u001b[1;32m   1159\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[1;32m   1160\u001b[0m     input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1161\u001b[0m )\n\u001b[1;32m   1163\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[1;32m   1165\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:953\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    949\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdtype=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnumeric\u001b[39m\u001b[39m'\u001b[39m\u001b[39m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    950\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    951\u001b[0m     )\n\u001b[1;32m    952\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_nd \u001b[39mand\u001b[39;00m array\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m3\u001b[39m:\n\u001b[0;32m--> 953\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    954\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound array with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m expected <= 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    955\u001b[0m         \u001b[39m%\u001b[39m (array\u001b[39m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    956\u001b[0m     )\n\u001b[1;32m    958\u001b[0m \u001b[39mif\u001b[39;00m force_all_finite:\n\u001b[1;32m    959\u001b[0m     _assert_all_finite(\n\u001b[1;32m    960\u001b[0m         array,\n\u001b[1;32m    961\u001b[0m         input_name\u001b[39m=\u001b[39minput_name,\n\u001b[1;32m    962\u001b[0m         estimator_name\u001b[39m=\u001b[39mestimator_name,\n\u001b[1;32m    963\u001b[0m         allow_nan\u001b[39m=\u001b[39mforce_all_finite \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mallow-nan\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    964\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with dim 3. RFE expected <= 2."
     ]
    }
   ],
   "source": [
    "dict_full_data = get_dict_all_city_data()\n",
    "full_data = clean_full_data(dict_full_data=dict_full_data)\n",
    "for city in cities:\n",
    "    df = pd.read_csv(output_process+city+'_train_preprocessed.csv', parse_dates=True, index_col= None, encoding = 'unicode_escape')\n",
    "    train = df.iloc[:,:-1].to_numpy()\n",
    "    next_predicted_month = 2\n",
    "    important_features = select_feature(train, next_predicted_month)\n",
    "    print(important_features)\n",
    "    list_features = full_data[city].columns.tolist()\n",
    "    selected_features = [\n",
    "        list_features[important_features[0]],\n",
    "        list_features[important_features[1]],\n",
    "        list_features[important_features[2]]\n",
    "    ]\n",
    "    print(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_predicted_month = 2\n",
    "important_features = select_feature(train, next_predicted_month)\n",
    "list_features = full_data[city].columns.tolist()\n",
    "selected_features = [\n",
    "    list_features[important_features[0]],\n",
    "    list_features[important_features[1]],\n",
    "    list_features[important_features[2]]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['n_raining_days', 'Min_Absolute_Temperature', 'Dengue_fever_rates']\n"
     ]
    }
   ],
   "source": [
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_feature(train, next_predicted_month):\n",
    "    \"\"\"Selects args.n_features top features using RFE\"\"\"\n",
    "    train_X, train_y = to_supervised(train, d_out=next_predicted_month, d_in=1 )\n",
    "    train_X, train_y = np.squeeze(train_X), np.squeeze(train_y)\n",
    "    rfe = RFE(RandomForestRegressor(n_estimators=500, random_state=args.seed), n_features_to_select=args.n_features)\n",
    "    fit = rfe.fit(train_X, train_y)\n",
    "    important_features = list()\n",
    "    for i in range(len(fit.support_)):\n",
    "        if fit.support_[i]:\n",
    "            important_features.append(i)\n",
    "    return np.array(important_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_feature(train, next_predicted_month):\n",
    "    \"\"\"Selects args.n_features top features using RFE\"\"\"\n",
    "    train_X, train_y = to_supervised(train, d_in=1, d_out=1)\n",
    "    train_X, train_y = np.squeeze(train_X), np.squeeze(train_y)\n",
    "    rfe = RFE(RandomForestRegressor(n_estimators=500, random_state=args.seed), n_features_to_select=args.n_features)\n",
    "    fit = rfe.fit(train_X, train_y)\n",
    "    important_features = list()\n",
    "    for i in range(len(fit.support_)):\n",
    "        if fit.support_[i]:\n",
    "            important_features.append(i)\n",
    "    return np.array(important_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# norm\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "list_features = full_data[city].columns.tolist()\n",
    "labels = [\"Dengue_fever_rates\"]\n",
    "df_features = full_data[city]\n",
    "\n",
    "# SELECT FETURES\n",
    "specific_data = full_data[city]\n",
    "specific_data = specific_data.set_index(\"year_month\")\n",
    "\n",
    "train, test = split_data(specific_data, args.look_back)\n",
    "\n",
    "# Fit data scaler to training data\n",
    "full_scaler = MinMaxScaler().fit(train)\n",
    "y_scaler = MinMaxScaler().fit(train.values[:, -1].reshape(-1, 1))\n",
    "\n",
    "# Scale train and test data\n",
    "train = full_scaler.transform(train)\n",
    "test = full_scaler.transform(test)\n",
    "\n",
    "# Get data to run model\n",
    "important_features = select_feature(train, specific_data)\n",
    "\n",
    "# selected_features = [\n",
    "#     list_features[important_features[0]],\n",
    "#     list_features[important_features[1]]\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  4, 12])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "important_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.36724138, 0.46628065, 0.42813589, ..., 0.57894737, 0.203     ,\n",
       "        0.51903695],\n",
       "       [0.56954023, 0.46280654, 0.41811847, ..., 0.36842105, 0.9885    ,\n",
       "        0.51402799],\n",
       "       [0.52011494, 0.49645777, 0.50043554, ..., 0.23684211, 0.28      ,\n",
       "        0.5530263 ],\n",
       "       ...,\n",
       "       [0.46551724, 0.47479564, 0.47560976, ..., 0.39473684, 0.83      ,\n",
       "        0.53487389],\n",
       "       [0.5       , 0.35490463, 0.24477352, ..., 0.39473684, 0.65      ,\n",
       "        0.56060385],\n",
       "       [0.57471264, 0.45367847, 0.52351916, ..., 0.55263158, 0.61      ,\n",
       "        0.53153234]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.iloc[:,:-1].to_numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.36724138, 0.46628065, 0.42813589, ..., 0.57894737, 0.203     ,\n",
       "        0.51903695],\n",
       "       [0.56954023, 0.46280654, 0.41811847, ..., 0.36842105, 0.9885    ,\n",
       "        0.51402799],\n",
       "       [0.52011494, 0.49645777, 0.50043554, ..., 0.23684211, 0.28      ,\n",
       "        0.5530263 ],\n",
       "       ...,\n",
       "       [0.46551724, 0.47479564, 0.47560976, ..., 0.39473684, 0.83      ,\n",
       "        0.53487389],\n",
       "       [0.5       , 0.35490463, 0.24477352, ..., 0.39473684, 0.65      ,\n",
       "        0.56060385],\n",
       "       [0.57471264, 0.45367847, 0.52351916, ..., 0.55263158, 0.61      ,\n",
       "        0.53153234]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(model_name, trial, city):   \n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    # ==================================================================\n",
    "    list_features = full_data[city].columns.tolist()\n",
    "    labels = [\"Diarrhoea_rates\"]\n",
    "    df_features = full_data[city]\n",
    "\n",
    "    # SELECT FETURES\n",
    "    specific_data = full_data[city]\n",
    "    specific_data = specific_data.set_index(\"year_month\")\n",
    "\n",
    "    train, test = split_data(specific_data, args.look_back)\n",
    "\n",
    "    # Fit data scaler to training data\n",
    "    full_scaler = MinMaxScaler().fit(train)\n",
    "    y_scaler = MinMaxScaler().fit(train.values[:, -1].reshape(-1, 1))\n",
    "\n",
    "    # Scale train and test data\n",
    "    train = full_scaler.transform(train)\n",
    "    test = full_scaler.transform(test)\n",
    "\n",
    "    # Get data to run model\n",
    "    important_features = select_feature(train, specific_data)\n",
    "\n",
    "    selected_features = [\n",
    "        list_features[important_features[0]],\n",
    "        list_features[important_features[1]]\n",
    "    ]\n",
    "\n",
    "    if model_name == \"RandomForest\":\n",
    "      random_state = trial.suggest_int('random_state', 0, 42)\n",
    "      n_estimators = trial.suggest_int('n_estimators', 50, 200)\n",
    "      max_depth = trial.suggest_int('max_depth', 1, 15)\n",
    "      output_chunk_length = trial.suggest_int('output_chunk_length', 1, 3)\n",
    "      # Create the RandomForest model\n",
    "      model = RandomForest(\n",
    "                    lags = 3,\n",
    "                    lags_past_covariates = 3,\n",
    "                    output_chunk_length = output_chunk_length,\n",
    "                    n_estimators = n_estimators,\n",
    "                    max_depth = max_depth,\n",
    "                    random_state=random_state)\n",
    "    elif model_name == 'TFTModel':\n",
    "      # Define the hyperparameters to optimize\n",
    "      random_state = trial.suggest_int('random_state', 0, 42)\n",
    "      dropout = trial.suggest_uniform('dropout', 0.01, 0.8)\n",
    "      n_epochs = trial.suggest_int('n_epochs', 50, 200)\n",
    "\n",
    "      # Create the TFTModel model\n",
    "      model = TFTModel(\n",
    "                    input_chunk_length = 3,\n",
    "                    output_chunk_length = 1,\n",
    "                    add_relative_index = True,\n",
    "                    dropout = dropout,\n",
    "                    n_epochs = n_epochs ,\n",
    "                    random_state=random_state)\n",
    "    elif model_name == 'NHiTSModel':\n",
    "      #suggest hyperparams\n",
    "      random_state = trial.suggest_int('random_state', 0, 42)\n",
    "      dropout = trial.suggest_uniform('dropout', 0.01, 0.80)\n",
    "      n_epochs = trial.suggest_int('n_epochs', 100, 500, step=10)\n",
    "      MaxPool1d = trial.suggest_categorical('MaxPool1d', [True, False])\n",
    "\n",
    "      model = NHiTSModel(\n",
    "                          input_chunk_length = 3,\n",
    "                          output_chunk_length = 1,\n",
    "                          MaxPool1d = MaxPool1d,\n",
    "                          dropout = dropout,\n",
    "                          n_epochs = n_epochs ,\n",
    "                          random_state=random_state)\n",
    "    elif model_name == 'LinearRegressionModel':\n",
    "      random_state = trial.suggest_int('random_state', 0, 43)\n",
    "      output_chunk_length = trial.suggest_int('output_chunk_length', 1, 5)\n",
    "      \n",
    "      # Create the  model\n",
    "      model = LinearRegressionModel(\n",
    "                      lags = output_chunk_length,\n",
    "                      lags_past_covariates = 3,\n",
    "                      output_chunk_length = output_chunk_length,\n",
    "                      random_state=random_state)\n",
    "    elif model_name == \"BlockRNNModel\":\n",
    "      #suggest hyperparams\n",
    "      random_state = trial.suggest_int('random_state', 0, 1000)\n",
    "      n_rnn_layers = trial.suggest_int('n_rnn_layers', 1, 3)\n",
    "      dropout = trial.suggest_uniform('dropout', 0.1, 0.5)\n",
    "      hidden_dim = trial.suggest_int('n_rnn_layers', 5, 20)\n",
    "      n_epochs = trial.suggest_int('n_epochs', 50, 200)\n",
    "\n",
    "      model = BlockRNNModel(\n",
    "                          input_chunk_length = 3,\n",
    "                          output_chunk_length = 1,\n",
    "                          hidden_dim = hidden_dim,\n",
    "                          n_rnn_layers = n_rnn_layers,\n",
    "                          dropout = dropout,\n",
    "                          n_epochs = n_epochs,\n",
    "                          random_state=random_state)\n",
    "      \n",
    "    elif model_name == \"CatBoostModel\":\n",
    "      #suggest hyperparams\n",
    "      learning_rate = trial.suggest_float('learning_rate', 0.001, 0.1)\n",
    "      n_estimators = trial.suggest_int('n_estimators', 50, 200)\n",
    "      max_depth = trial.suggest_int('max_depth', 1, 15)\n",
    "      random_state = trial.suggest_int('random_state', 0, 1000)\n",
    "      likelihood = trial.suggest_categorical('likelihood', ['quantile'])\n",
    "      quantiles =  trial.suggest_categorical('quantiles', [None, [0.1, 0.5, 0.9]])\n",
    "      output_chunk_length = trial.suggest_int('output_chunk_length', 1, 4)\n",
    "      bagging_temperature = trial.suggest_float('bagging_temperature', 0.01, 100.0)\n",
    "      border_count = trial.suggest_int('border_count', 1, 255)\n",
    "      l2_leaf_reg = trial.suggest_float('l2_leaf_reg', 0.1, 10)\n",
    "      random_strength = trial.suggest_float('random_strength', 0.1, 10)\n",
    "      model = CatBoostModel(lags=3,\n",
    "                          lags_past_covariates=3, \n",
    "                          learning_rate=learning_rate,\n",
    "                          n_estimators=n_estimators,\n",
    "                          max_depth=max_depth, \n",
    "                          output_chunk_length = output_chunk_length,\n",
    "                          likelihood = likelihood,\n",
    "                          quantiles = quantiles,\n",
    "                          bagging_temperature = bagging_temperature,\n",
    "                          border_count = border_count,\n",
    "                          l2_leaf_reg = l2_leaf_reg,\n",
    "                          random_strength = random_strength,\n",
    "                          random_state=random_state)\n",
    "    \n",
    "    elif model_name == \"NBEATSModel\":\n",
    "      random_state = trial.suggest_int('random_state', 0, 42)\n",
    "      dropout = trial.suggest_uniform('dropout', 0.01, 0.80)\n",
    "      n_epochs = trial.suggest_int('n_epochs', 50, 200)\n",
    "\n",
    "      pl_trainer_kwargs = {\n",
    "              \"accelerator\": \"gpu\",\n",
    "              \"devices\": -1,\n",
    "              \"auto_select_gpus\": True,\n",
    "          }\n",
    "      model = NBEATSModel(\n",
    "                            input_chunk_length = 3,\n",
    "                            output_chunk_length = 1,\n",
    "                            dropout = dropout,\n",
    "                            n_epochs = n_epochs ,\n",
    "                            pl_trainer_kwargs = pl_trainer_kwargs,\n",
    "                            random_state=random_state)\n",
    "  \n",
    "    elif model_name == \"TCNModel\":\n",
    "      params = {\n",
    "        'kernel_size': trial.suggest_int(\"kernel_size\", 2, 5),\n",
    "        'num_filters': trial.suggest_int(\"num_filters\", 1, 5),\n",
    "        'weight_norm': trial.suggest_categorical(\"weight_norm\", [False, True]),\n",
    "        'dilation_base': trial.suggest_int(\"dilation_base\", 2, 4),\n",
    "        'dropout': trial.suggest_float(\"dropout\", 0.0, 0.4),\n",
    "        'learning_rate': trial.suggest_float(\"learning_rate\", 5e-5, 1e-3, log=True),\n",
    "        'include_year': trial.suggest_categorical(\"year\", [False, True]),\n",
    "        'n_epochs': trial.suggest_int(\"n_epochs\", 100, 300),\n",
    "      }\n",
    "\n",
    "      # select input and output chunk lengths\n",
    "      in_len = trial.suggest_int(\"input_chunk_length\", 12, 36)\n",
    "      out_len = trial.suggest_int(\"output_chunk_length\", 1, in_len-1)\n",
    "\n",
    "      params['input_chunk_length'] = in_len\n",
    "      params['lags'] = in_len\n",
    "      params['output_chunk_length'] = out_len    \n",
    "   \n",
    "\n",
    "      # optionally also add the (scaled) year value as a past covariate\n",
    "      if params['include_year']:\n",
    "          encoders = {\"datetime_attribute\": {\"past\": [\"year\"]},\n",
    "                      \"transformer\": Scaler()}\n",
    "      else:\n",
    "          encoders = None\n",
    "      params['encoders'] = encoders\n",
    "\n",
    "      pl_trainer_kwargs = {\n",
    "            \"accelerator\": \"gpu\",\n",
    "            \"devices\": -1,\n",
    "            \"auto_select_gpus\": True,\n",
    "        }\n",
    "     \n",
    "\n",
    "      param = params\n",
    "      model = TCNModel(\n",
    "          input_chunk_length=param['input_chunk_length'],\n",
    "          output_chunk_length=param['output_chunk_length'],\n",
    "          batch_size=16,\n",
    "          n_epochs=param['n_epochs'],\n",
    "          nr_epochs_val_period=1,\n",
    "          kernel_size=param['kernel_size'],\n",
    "          num_filters=param['num_filters'],\n",
    "          weight_norm=param['weight_norm'],\n",
    "          dilation_base=param['dilation_base'],\n",
    "          dropout=param['dropout'],\n",
    "          optimizer_kwargs={\"lr\": param['learning_rate']},\n",
    "          add_encoders=param['encoders'],\n",
    "          likelihood=GaussianLikelihood(),\n",
    "          pl_trainer_kwargs=pl_trainer_kwargs,\n",
    "          model_name=\"tcn_model\",\n",
    "          force_reset=True,\n",
    "          save_checkpoints=True,\n",
    "      )\n",
    "    elif model_name == \"LightGBMModel\":\n",
    "      params = {\n",
    "        \"lags\": trial.suggest_categorical(\"lags\", [3]),\n",
    "        \"lags_past_covariates\": trial.suggest_categorical(\"lags_past_covariates\", [3]),\n",
    "        \"random_state\": trial.suggest_int(\"random_state\", 0, 999),\n",
    "        \"multi_models\": trial.suggest_categorical(\"multi_models\", [True, False]),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
    "        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.1, 1.0),\n",
    "        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.1, 1.0),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n",
    "        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n",
    "        'verbose': -1,\n",
    "        'likelihood' : trial.suggest_categorical(\"likelihood\", [\"quantile\"])\n",
    "      }\n",
    "\n",
    "      param = params\n",
    "      model = LightGBMModel(\n",
    "          lags = param['lags'],\n",
    "          lags_past_covariates = param['lags_past_covariates'],\n",
    "          output_chunk_length = 1,\n",
    "          random_state = param['random_state'],\n",
    "          multi_models = param['multi_models'],\n",
    "          likelihood = param['likelihood'],\n",
    "          num_leaves = param['num_leaves'],\n",
    "          learning_rate = param['learning_rate'],\n",
    "          feature_fraction = param['feature_fraction'],\n",
    "          bagging_fraction = param['bagging_fraction'],\n",
    "          min_child_samples = param['min_child_samples'],\n",
    "          lambda_l1 = param['lambda_l1'],\n",
    "          verbose = param['verbose']\n",
    "      )\n",
    "\n",
    "    mae_error = output_prediction_for_location(df_features, model, location=city, feature_list=selected_features,\n",
    "                                                       labels=labels, scaler=scaler, lags= 3 if param.get('input_chunk_length') is None else param['input_chunk_length'],\n",
    "                                                       split_index=-36)\n",
    "\n",
    "    return mae_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main run optimize and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# Main Cell for optimize\n",
    "#########################\n",
    "\n",
    "# Input param for Optimize Run\n",
    "ntry = 50\n",
    "njob = 1\n",
    "\n",
    "model_name_list = [\n",
    "     \"RandomForest\",\n",
    "     \"LinearRegressionModel\",\n",
    "     \"LightGBMModel\",\n",
    "     \"CatBoostModel\",\n",
    "     \"BlockRNNModel\",\n",
    "     \"NBEATSModel\",\n",
    "     \"NHiTSModel\",\n",
    "     \"TCNModel\",\n",
    "     \"TFTModel\"\n",
    "]\n",
    "\n",
    "# Lưu thông tin traceback study và error city trong quá trình optimize\n",
    "l_study_city ={}\n",
    "l_errCity =[]\n",
    "\n",
    "if __name__ == '__main__':  \n",
    "  for model_name in model_name_list: \n",
    "    print(model_name)\n",
    "    best_param = pd.DataFrame()\n",
    "    for city in cities:\n",
    "      # Use Tree-structured Parzen Estimator sampler to minimise RMSE\n",
    "      sampler = optuna.samplers.TPESampler()\n",
    "      study = optuna.create_study(sampler=sampler, direction='minimize', study_name = model_name)\n",
    "\n",
    "      # truyền multiple param vào trong biến trial\n",
    "      obj_func = lambda trial: objective(model_name, trial, city)\n",
    "\n",
    "      try:\n",
    "        # Optimise over 100 trials\n",
    "        study.optimize(obj_func, n_trials=ntry, n_jobs=njob)\n",
    "\n",
    "        # Print results\n",
    "        print(\"Study statistics for : \")\n",
    "        print(\"  Number of finished trials: \", len(study.trials))\n",
    "    \n",
    "        \n",
    "        print(\"Best trial of city: \",city)\n",
    "        best_trial = study.best_trial\n",
    "        print(\"  Value: \", best_trial.value)   \n",
    "\n",
    "        # lưu best param vào trong biến toàn cục\n",
    "\n",
    "        if model_name == \"LinearRegressionModel\": \n",
    "          one_city_param = pd.DataFrame({\n",
    "                              'City': city,\n",
    "                              'Alg_name': 'LinearRegressionModel',\n",
    "                              'Best_value': best_trial.value,\n",
    "                              'n_try_opt': ntry,\n",
    "                              'lags' : best_trial.params['output_chunk_length'],\n",
    "                              'lags_past_covariates': 3,\n",
    "                              'output_chunk_length': best_trial.params['output_chunk_length'],\n",
    "                              'random_state':best_trial.params['random_state'],\n",
    "                              }, index=[0])\n",
    "          one_city_param.to_excel(prj_path_opt+'diarrhoea_opt_hyperparam_' + model_name + \"_\" +city+'.xlsx')\n",
    "\n",
    "        elif model_name == \"LightGBMModel\":\n",
    "          one_city_param = pd.DataFrame({\n",
    "                              'City': city,\n",
    "                              'Alg_name': 'LightGBMModel',\n",
    "                              'Best_value': best_trial.value,\n",
    "                              'n_try_opt': ntry,\n",
    "                              'lags': best_trial.params['lags'],\n",
    "                              'lags_past_covariates': best_trial.params['lags_past_covariates'],\n",
    "                              'multi_models': best_trial.params['multi_models'],\n",
    "                              'num_leaves': best_trial.params['num_leaves'], \n",
    "                              'feature_fraction': best_trial.params['feature_fraction'], \n",
    "                              'min_child_samples': best_trial.params['min_child_samples'], \n",
    "                              'lambda_l1': best_trial.params['lambda_l1'], \n",
    "                              'lambda_l2': best_trial.params['lambda_l2'], \n",
    "                              'likelihood': best_trial.params['likelihood'], \n",
    "                              'learning_rate': best_trial.params['learning_rate']}, index=[0])\n",
    "          one_city_param.to_excel(prj_path_opt+'diarrhoea_opt_hyperparam_' + model_name + \"_\" +city+'.xlsx')\n",
    "           \n",
    "        elif model_name == \"CatBoostModel\":\n",
    "          one_city_param = pd.DataFrame({\n",
    "                              'City': city,\n",
    "                              'Alg_name': 'CatBoost',\n",
    "                              'Best_value': best_trial.value,\n",
    "                              'n_try_opt': ntry,\n",
    "                              'lags' : 3,\n",
    "                              'lags_past_covariates': 3,\n",
    "                              'output_chunk_length': best_trial.params['output_chunk_length'],\n",
    "                              'likelihood': best_trial.params['likelihood'],\n",
    "                              'learning_rate': best_trial.params['learning_rate'],\n",
    "                              'n_estimators': best_trial.params['n_estimators'],\n",
    "                              'max_depth': best_trial.params['max_depth'],\n",
    "                              'bagging_temperature': best_trial.params['bagging_temperature'],\n",
    "                              'l2_leaf_reg': best_trial.params['l2_leaf_reg'],\n",
    "                              'random_strength':best_trial.params['random_strength'],\n",
    "                              }, index=[0])\n",
    "\n",
    "          one_city_param.to_excel(prj_path_opt+'diarrhoea_opt_hyperparam_' + model_name + \"_\" +city+'.xlsx')\n",
    "          \n",
    "        elif model_name == \"NHiTSModel\":\n",
    "          one_city_param = pd.DataFrame({\n",
    "                              'City': city,\n",
    "                              'Alg_name': 'N-HiTS',\n",
    "                              'Best_value': best_trial.value,\n",
    "                              'n_try_opt': ntry,\n",
    "                              'input_chunk_length' : 3,\n",
    "                              'output_chunk_length' : 1,\n",
    "                              'MaxPool1d' : best_trial.params['MaxPool1d'],\n",
    "                              'dropout' : best_trial.params['dropout'],\n",
    "                              'n_epochs' : best_trial.params['n_epochs'],\n",
    "                              'random_state' : best_trial.params['random_state'],\n",
    "                              }, index=[0])\n",
    "          one_city_param.to_excel(prj_path_opt+'diarrhoea_opt_hyperparam_' + model_name + \"_\" +city+'.xlsx')\n",
    "\n",
    "        elif model_name == \"TCNModel\":\n",
    "          one_city_param = pd.DataFrame({\n",
    "                              'City': city,\n",
    "                              'Alg_name': 'TCNModel',\n",
    "                              'Best_value': best_trial.value,\n",
    "                              'n_try_opt': ntry,\n",
    "                              'lags' : best_trial.params['input_chunk_length'],\n",
    "                              'input_chunk_length': best_trial.params['input_chunk_length'],\n",
    "                              'output_chunk_length': best_trial.params['output_chunk_length'],\n",
    "                              'n_epochs':best_trial.params['n_epochs'],\n",
    "                              'num_filters':best_trial.params['num_filters'],\n",
    "                              'weight_norm':best_trial.params['weight_norm'],\n",
    "                              'dilation_base':best_trial.params['dilation_base'],\n",
    "                              'dropout':best_trial.params['dropout'],\n",
    "                              'learning_rate':best_trial.params['learning_rate'],\n",
    "                              'year':best_trial.params['year'],\n",
    "                              }, index=[0])\n",
    "          one_city_param.to_excel(prj_path_opt+'diarrhoea_opt_hyperparam_' + model_name + \"_\" +city+'.xlsx')\n",
    "          \n",
    "        elif model_name == \"NBEATSModel\":\n",
    "          one_city_param = pd.DataFrame({\n",
    "                              'City': city,\n",
    "                              'Alg_name': 'NBeatsModel',\n",
    "                              'Best_value': best_trial.value,\n",
    "                              'n_try_opt': ntry,\n",
    "                              'output_chunk_length': 3,\n",
    "                              'input_chunk_length': 1,\n",
    "                              'n_epochs':best_trial.params['n_epochs'],\n",
    "                              'dropout':best_trial.params['dropout'],\n",
    "                              'random_state':best_trial.params['random_state'],\n",
    "                              }, index=[0])    \n",
    "          one_city_param.to_excel(prj_path_opt+'diarrhoea_opt_hyperparam_' + model_name + \"_\" +city+'.xlsx')\n",
    "        elif model_name == \"TFTModel\":\n",
    "          one_city_param = pd.DataFrame({\n",
    "                              'City': city,\n",
    "                              'Alg_name': 'TFTModel',\n",
    "                              'Best_value': best_trial.value,\n",
    "                              'n_try_opt': ntry,\n",
    "                              'output_chunk_length': 3,\n",
    "                              'input_chunk_length': 1,\n",
    "                              'add_relative_index': True,\n",
    "                              'random_state':best_trial.params['random_state'],\n",
    "                              'n_epochs':best_trial.params['n_epochs'],\n",
    "                              'dropout':best_trial.params['dropout']\n",
    "                              }, index=[0])\n",
    "          one_city_param.to_excel(prj_path_opt+'diarrhoea_opt_hyperparam_' + model_name + \"_\" +city+'.xlsx')\n",
    "        elif model_name == \"RandomForest\":\n",
    "          one_city_param = pd.DataFrame({\n",
    "                              'City': city,\n",
    "                              'Alg_name': 'RandomForest',\n",
    "                              'Best_value': best_trial.value,\n",
    "                              'n_try_opt': ntry,\n",
    "                              'lags' : best_trial.params['output_chunk_length'],\n",
    "                              'lags_past_covariates': 3,\n",
    "                              'output_chunk_length': best_trial.params['output_chunk_length'],\n",
    "                              'n_estimators': best_trial.params['n_estimators'],\n",
    "                              'max_depth': best_trial.params['max_depth'],\n",
    "                              'random_state':best_trial.params['random_state'],\n",
    "                              }, index=[0])\n",
    "          one_city_param.to_excel(prj_path_opt+'diarrhoea_opt_hyperparam_' + model_name + \"_\" +city+'.xlsx')\n",
    "           \n",
    "        elif model_name == \"BlockRNNModel\":\n",
    "          one_city_param = pd.DataFrame({\n",
    "                              'City': city,\n",
    "                              'Alg_name': 'BlockRNNModel',\n",
    "                              'Best_value': best_trial.value,\n",
    "                              'n_try_opt': ntry,\n",
    "                              'input_chunk_length': 3,\n",
    "                              'output_chunk_length': 1,\n",
    "                              'random_state':best_trial.params['random_state'],\n",
    "                              'n_epochs':best_trial.params['n_epochs'],\n",
    "                              'hidden_dim': best_trial.params['hidden_dim'],\n",
    "                              'n_rnn_layers': best_trial.params['n_rnn_layers'],\n",
    "                              'dropout':best_trial.params['dropout']\n",
    "                              }, index=[0])\n",
    "          one_city_param.to_excel(prj_path_opt+'diarrhoea_opt_hyperparam_' + model_name + \"_\" +city+'.xlsx')\n",
    "\n",
    "        best_param = best_param.append(one_city_param)\n",
    "      except:# có error thì lưu vào l_errCity để check lại sau \n",
    "        l_errCity.append(city)\n",
    "    # lưu kết quả vào file \n",
    "    best_param.to_excel(prj_path_opt+'diarrhoea_opt_hyperparam_' + model_name + '.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "\n",
    "# def send_to_telegram(message):\n",
    "\n",
    "#     apiToken = '5908735099:AAGVSLrW62aXPBP-GrMvxoVgMsuJxXJpP1Q'\n",
    "#     chatID = '@ptn_announcement'\n",
    "#     apiURL = f'https://api.telegram.org/bot{apiToken}/sendMessage'\n",
    "\n",
    "#     try:\n",
    "#         response = requests.post(apiURL, json={'chat_id': chatID, 'text': message})\n",
    "#         print(response.text)\n",
    "#     except Exception as e:\n",
    "#         print(e)\n",
    "\n",
    "# send_to_telegram(\"Server Chạy Xong optimize\" )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
