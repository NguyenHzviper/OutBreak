{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import csv\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pwd\n",
    "# /Source_14012023_v4/preprocessing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prj_path = '../'\n",
    "data_path = prj_path + \"data/new_data/DH/squeezed/\"\n",
    "prj_path_opt= prj_path + \"optimize_hyperparam/opt_results/\"\n",
    "output_process = prj_path + \"data/new_data/DH/processed_data/\"\n",
    "output_featureselection = prj_path + \"data/new_data/DH/feature_selection/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = {0 : \"train\", 1 : \"test\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cities = [\n",
    "        'An Giang', 'BR V≈©ng T√†u', 'B√¨nh Ph∆∞·ªõc', 'B√¨nh Thu·∫≠n', 'B√¨nh ƒê·ªãnh',\n",
    "        'B·∫°c Li√™u', 'B·∫Øc K·∫°n', 'B·∫Øc Giang', 'Cao B·∫±ng', 'C√† Mau',\n",
    "        'C·∫ßn Th∆°', 'Gia Lai', 'H√† Giang', 'H√† N·ªôi', 'H√† Tƒ©nh',\n",
    "        'H√≤a B√¨nh','H∆∞ng Y√™n', 'H·∫£i D∆∞∆°ng', 'H·∫£i Ph√≤ng', 'Kh√°nh H√≤a', 'Ki√™n Giang',\n",
    "        'Kon Tum', 'Lai Ch√¢u', 'Long An', 'L√†o Cai', 'L√¢m ƒê·ªìng',\n",
    "        'L·∫°ng S∆°n','Nam ƒê·ªãnh', 'Ngh·ªá An', 'Ninh B√¨nh', 'Ninh Thu·∫≠n',\n",
    "        'Ph√∫ Th·ªç', 'Ph√∫ Y√™n', 'Qu·∫£ng B√¨nh', 'Qu·∫£ng Nam', 'Qu·∫£ng Ng√£i',\n",
    "        'Qu·∫£ng Ninh', 'Qu·∫£ng Tr·ªã', 'S√≥c TrƒÉng', 'S∆°n La', 'TT Hu·∫ø',\n",
    "        'Thanh H√≥a', 'Th√°i B√¨nh', 'Th√°i Nguy√™n', 'Ti·ªÅn Giang', 'Tr√† Vinh',\n",
    "        'Tuy√™n Quang', 'T√¢y Ninh', 'Vƒ©nh Ph√∫c', 'Y√™n B√°i', 'ƒêi·ªán Bi√™n',\n",
    "        'ƒê√† N·∫µng', 'ƒê·∫Øk N√¥ng', 'ƒê·∫Øk L·∫Øk', 'ƒê·ªìng Th√°p'\n",
    "]\n",
    "cities = all_cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters as args using the Configuration class\n",
    "class Configuration():\n",
    "    def __init__(self):\n",
    "      # l·∫•y b·ªô test d√†i 36 th√°ng = 3 nƒÉm\n",
    "        self.test_size = 36\n",
    "        # l√† nh√¨n v√†o d·ªØ li·ªáu tr∆∞·ªõc 3 th√°ng v√† d·ª± ph√≥ng        \n",
    "        self.look_back = 3\n",
    "        # d·ª± ph√≥ng n-step trong 6 th√°ng\n",
    "        self.n_predicted_period_months = 6\n",
    "        self.n_features = 3\n",
    "        self.seed = 42\n",
    "        # m·ªói ph·∫ßn t·ª≠ x trong t·∫≠p suppervise c√≥ ƒë·ªô l·ªõn l√† 16 = 16 th√°ng\n",
    "        self.batch_size = 16\n",
    "        self.epochs = 300\n",
    "\n",
    "args = Configuration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supporting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict_all_city_data():\n",
    "  cities_data = {}  \n",
    "  for city in cities:\n",
    "    city_result = pd.read_excel(prj_path+'data/new_data/DH/squeezed/squeezed_'+city+'.xlsx')  \n",
    "    \"\"\"Get all data from all city in 1997 - 2016\"\"\" \n",
    "    city_result = city_result.loc[city_result['year_month'] < '2017-1-1'] \n",
    "    cities_data[city] = city_result\n",
    "  return cities_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data (pre-)processing functions\n",
    "# modification\n",
    "def get_city_data(city_name, dict_full_data):\n",
    "    \"\"\"Returns Diarrhoea rate and climate data\"\"\" \n",
    "    city_data = dict_full_data[city_name].drop(columns=['Diarrhoea_cases','Diarrhoea_rates', 'province',\n",
    "                                                        'Influenza_rates','Influenza_cases',\n",
    "                                                        'Dengue_fever_cases', 'year', 'month'], \n",
    "                                                                  axis=1, \n",
    "                                                                  inplace=False)    \n",
    "    return city_data\n",
    "\n",
    "def convert_to_stationary(city_data):\n",
    "    \"\"\"Subtracts previous value for all cols except disease rates\"\"\"\n",
    "    for col_name in city_data.columns:\n",
    "        if col_name != 'Dengue_fever_rates':\n",
    "            try:\n",
    "                city_data[col_name] = city_data[col_name] - city_data[col_name].shift()\n",
    "            except:\n",
    "                print(col_name)\n",
    "    return city_data\n",
    "\n",
    "def impute_missing_value(city_data):\n",
    "    \"\"\"\n",
    "    Imputes 0 for first 12 months, \n",
    "    last year's value for months 12-24, \n",
    "    and minimum value of last two years for months 25+\n",
    "    \"\"\"\n",
    "    for col in city_data.columns:\n",
    "        for index in range(len(city_data[col])):\n",
    "            if np.isnan(city_data[col].iloc[index]):\n",
    "                if index < 12:\n",
    "                    city_data[col].iloc[index] = 0\n",
    "                elif index >= 12 and index <= 24:\n",
    "                    city_data[col].iloc[index] = city_data[col].iloc[index - 12]\n",
    "                else:\n",
    "                    city_data[col].iloc[index] = min(city_data[col].iloc[index - 12], city_data[col].iloc[index - 24])\n",
    "    return city_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_full_data(dict_full_data):\n",
    "    climate_and_disease_feats = ['Total_Evaporation',\n",
    "       'Total_Rainfall', 'Max_Daily_Rainfall', 'n_raining_days',\n",
    "       'Average_temperature', 'Max_Average_Temperature',\n",
    "       'Min_Average_Temperature', 'Max_Absolute_Temperature',\n",
    "       'Min_Absolute_Temperature', 'Average_Humidity', 'Min_Humidity',\n",
    "       'n_hours_sunshine', 'Dengue_fever_rates']\n",
    "    for city in cities:\n",
    "        city_data = get_city_data(city_name=city,dict_full_data = dict_full_data)\n",
    "        city_data_features = city_data[climate_and_disease_feats]\n",
    "        city_data_features = impute_missing_value(city_data_features)\n",
    "        city_data_features = convert_to_stationary(city_data_features)\n",
    "        city_data_features.dropna(inplace=True)\n",
    "        city_data_features.loc[:, \"year_month\"] = city_data[\"year_month\"]\n",
    "        dict_full_data[city] = city_data_features\n",
    "    return dict_full_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, look_back, n_nextstep = args.n_predicted_period_months):\n",
    "    \"\"\"Splits data into train and test sets based on args (Configuration class)\"\"\"\n",
    "    train = data[: -args.test_size]    \n",
    "    test = data[-args.test_size - look_back-(n_nextstep - 1): ]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_supervised(data,  d_out, d_in, features_list=[]):\n",
    "    \"\"\"\n",
    "    Frames time-series as supervised learning dataset.\n",
    "    \n",
    "    Args:\n",
    "      d_in: lookback window\n",
    "      d_out: number of predicted months\n",
    "      features_list: list of all features **where last col is the disease incidence**\n",
    "\n",
    "    Returns:\n",
    "      Numpy arrays of disease incidence (y) and other predictors (X)\n",
    "    \"\"\"\n",
    "    X, y = list(), list()\n",
    "    for index, _ in enumerate(data):\n",
    "        in_end = index + d_in\n",
    "        out_end = in_end + d_out\n",
    "        if out_end <= len(data):\n",
    "            if len(features_list) == 0 :\n",
    "                X.append(data[index: in_end, :-1])\n",
    "            else:\n",
    "                X.append(data[index: in_end, features_list])\n",
    "            y.append(data[out_end-1: out_end, -1])\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureSelection(train, next_predicted_month, n_selection_feature, n_selection_feature_by_day = 18): #max args.look_back * num_feature = 36 => take half\n",
    "  train_X, train_y = to_supervised(train, d_out=next_predicted_month, d_in=args.look_back )\n",
    "\n",
    "  D = train_X.shape[2]\n",
    "  rfe = RFE(RandomForestRegressor(), n_features_to_select=n_selection_feature_by_day)\n",
    "  fit = rfe.fit(train_X.reshape(len(train_X),D*args.look_back), train_y)\n",
    "  important_features = list()\n",
    "\n",
    "  for i in range(len(fit.support_)):\n",
    "      if fit.support_[i]:\n",
    "          important_features.append(i)\n",
    "  result = np.array(important_features)\n",
    "  calMostFeature = [0]*D\n",
    "  for i in result:\n",
    "    calMostFeature[i%D] = calMostFeature[i%D] + 1\n",
    "  top_idx =  np.sort(np.argsort(calMostFeature)[-n_selection_feature:])\n",
    "  return top_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizationMinMax(df,city,data_set_index):\n",
    "    date_index = df.index\n",
    "    norm_set = data_set[data_set_index]\n",
    "    scaler = MinMaxScaler()\n",
    "    if norm_set == \"train\":\n",
    "        print(\"üçíCh·∫øt cha d√≠nh train √≤i!\")\n",
    "        scaler.fit(df)\n",
    "        series = scaler.transform(df)\n",
    "        df_scaled = pd.DataFrame(data = series, columns = df.columns)\n",
    "        joblib.dump(scaler, output_process+city+'_train_scalerMinMaxNorm.save')\n",
    "\n",
    "    else:\n",
    "        print(\"üå∑ Ch·ªâ m·∫ßn l·∫°i c√°i test thui!\")\n",
    "        true_incidence = df.iloc[:, [-1]]\n",
    "        scaler = joblib.load(output_process+city+'_train_scalerMinMaxNorm.save') #ok\n",
    "        series = scaler.transform(df)\n",
    "        df_scaled = pd.DataFrame(data = series, columns = df.columns) \n",
    "        # df_scaled.iloc[:, [-1]] = true_incidence\n",
    "    df_scaled[\"year_month\"] = date_index\n",
    "    \"\"\"Save data as csv, when load data as a dataframe, use command df.iloc[:,:-1].to_numpy() to convert to an array to use\"\"\"\n",
    "    df_scaled.to_csv(output_process+city+'_'+norm_set+'_preprocessed_normed.csv', float_format='%.7f',index=False)\n",
    "    return df_scaled,scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üå∑ Ch·ªâ m·∫ßn l·∫°i c√°i test thui!\n",
      "üå∑ Ch·ªâ m·∫ßn l·∫°i c√°i test thui!\n",
      "üå∑ Ch·ªâ m·∫ßn l·∫°i c√°i test thui!\n",
      "üå∑ Ch·ªâ m·∫ßn l·∫°i c√°i test thui!\n",
      "üå∑ Ch·ªâ m·∫ßn l·∫°i c√°i test thui!\n",
      "üå∑ Ch·ªâ m·∫ßn l·∫°i c√°i test thui!\n",
      "üå∑ Ch·ªâ m·∫ßn l·∫°i c√°i test thui!\n",
      "üå∑ Ch·ªâ m·∫ßn l·∫°i c√°i test thui!\n",
      "üå∑ Ch·ªâ m·∫ßn l·∫°i c√°i test thui!\n",
      "üå∑ Ch·ªâ m·∫ßn l·∫°i c√°i test thui!\n",
      "üå∑ Ch·ªâ m·∫ßn l·∫°i c√°i test thui!\n",
      "üå∑ Ch·ªâ m·∫ßn l·∫°i c√°i test thui!\n",
      "üå∑ Ch·ªâ m·∫ßn l·∫°i c√°i test thui!\n",
      "üå∑ Ch·ªâ m·∫ßn l·∫°i c√°i test thui!\n",
      "üå∑ Ch·ªâ m·∫ßn l·∫°i c√°i test thui!\n",
      "üå∑ Ch·ªâ m·∫ßn l·∫°i c√°i test thui!\n",
      "üå∑ Ch·ªâ m·∫ßn l·∫°i c√°i test thui!\n",
      "üå∑ Ch·ªâ m·∫ßn l·∫°i c√°i test thui!\n",
      "üå∑ Ch·ªâ m·∫ßn l·∫°i c√°i test thui!\n",
      "üå∑ Ch·ªâ m·∫ßn l·∫°i c√°i test thui!\n",
      "üå∑ Ch·ªâ m·∫ßn l·∫°i c√°i test thui!\n",
      "üå∑ Ch·ªâ m·∫ßn l·∫°i c√°i test thui!\n",
      "üå∑ Ch·ªâ m·∫ßn l·∫°i c√°i test thui!\n",
      "üå∑ Ch·ªâ m·∫ßn l·∫°i c√°i test thui!\n",
      "üå∑ Ch·ªâ m·∫ßn l·∫°i c√°i test thui!\n",
      "üå∑ Ch·ªâ m·∫ßn l·∫°i c√°i test thui!\n",
      "üå∑ Ch·ªâ m·∫ßn l·∫°i c√°i test thui!\n",
      "üå∑ Ch·ªâ m·∫ßn l·∫°i c√°i test thui!\n",
      "üå∑ Ch·ªâ m·∫ßn l·∫°i c√°i test thui!\n",
      "üå∑ Ch·ªâ m·∫ßn l·∫°i c√°i test thui!\n",
      "üå∑ Ch·ªâ m·∫ßn l·∫°i c√°i test thui!\n",
      "üå∑ Ch·ªâ m·∫ßn l·∫°i c√°i test thui!\n",
      "üå∑ Ch·ªâ m·∫ßn l·∫°i c√°i test thui!\n",
      "üå∑ Ch·ªâ m·∫ßn l·∫°i c√°i test thui!\n",
      "üå∑ Ch·ªâ m·∫ßn l·∫°i c√°i test thui!\n",
      "üå∑ Ch·ªâ m·∫ßn l·∫°i c√°i test thui!\n",
      "üå∑ Ch·ªâ m·∫ßn l·∫°i c√°i test thui!\n",
      "üå∑ Ch·ªâ m·∫ßn l·∫°i c√°i test thui!\n",
      "üå∑ Ch·ªâ m·∫ßn l·∫°i c√°i test thui!\n",
      "üå∑ Ch·ªâ m·∫ßn l·∫°i c√°i test thui!\n",
      "üå∑ Ch·ªâ m·∫ßn l·∫°i c√°i test thui!\n",
      "üå∑ Ch·ªâ m·∫ßn l·∫°i c√°i test thui!\n",
      "üå∑ Ch·ªâ m·∫ßn l·∫°i c√°i test thui!\n",
      "üå∑ Ch·ªâ m·∫ßn l·∫°i c√°i test thui!\n",
      "üå∑ Ch·ªâ m·∫ßn l·∫°i c√°i test thui!\n",
      "üå∑ Ch·ªâ m·∫ßn l·∫°i c√°i test thui!\n",
      "üå∑ Ch·ªâ m·∫ßn l·∫°i c√°i test thui!\n",
      "üå∑ Ch·ªâ m·∫ßn l·∫°i c√°i test thui!\n",
      "üå∑ Ch·ªâ m·∫ßn l·∫°i c√°i test thui!\n",
      "üå∑ Ch·ªâ m·∫ßn l·∫°i c√°i test thui!\n",
      "üå∑ Ch·ªâ m·∫ßn l·∫°i c√°i test thui!\n",
      "üå∑ Ch·ªâ m·∫ßn l·∫°i c√°i test thui!\n",
      "üå∑ Ch·ªâ m·∫ßn l·∫°i c√°i test thui!\n",
      "üå∑ Ch·ªâ m·∫ßn l·∫°i c√°i test thui!\n",
      "üå∑ Ch·ªâ m·∫ßn l·∫°i c√°i test thui!\n"
     ]
    }
   ],
   "source": [
    "dict_full_data = get_dict_all_city_data()\n",
    "full_data = clean_full_data(dict_full_data=dict_full_data)\n",
    "for city in cities:\n",
    "    specific_data = full_data[city]\n",
    "    specific_data = specific_data.set_index(\"year_month\")\n",
    "    train, test = split_data(specific_data, args.look_back)\n",
    "    # df_train,scaler = normalizationMinMax(train,city, 0)\n",
    "    df_test,scaler = normalizationMinMax(test,city, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fields = ['City', 'Target', '1st_Feature', '2nd_Feature', '3rd_Feature'] #, '3rd_Feature'\n",
    "# for next_predicted_month in range(1,args.n_predicted_period_months+1):\n",
    "#     filename = output_featureselection+str(next_predicted_month)+\"step_feature_selection_3_most.csv\"\n",
    "#     with open(filename, 'w') as csvfile:\n",
    "#         rows = []\n",
    "#         csvwriter = csv.writer(csvfile)\n",
    "#         csvwriter.writerow(fields)\n",
    "#         for city in cities:\n",
    "#             df = pd.read_csv(output_process+city+'_train_preprocessed.csv', parse_dates=True, index_col= None, encoding = 'unicode_escape')\n",
    "#             train = df.iloc[:,:-1].to_numpy()\n",
    "#             important_features = featureSelection(train, next_predicted_month, args.n_features, n_selection_feature_by_day = 18)\n",
    "#             list_features = full_data[city].columns.tolist()\n",
    "#             selected_features = [\n",
    "#                 list_features[important_features[0]],\n",
    "#                 list_features[important_features[1]],\n",
    "#                 list_features[important_features[2]]\n",
    "#             ]\n",
    "#             rows=[city,'Dengue_fever_rates',selected_features[0],selected_features[1],selected_features[2]] #,third_feature\n",
    "#             csvwriter.writerow(rows)\n",
    "#             print(\"-------------Feature selection processing------------\")\n",
    "#             print(\"----> \",next_predicted_month,\" step\")\n",
    "#             print(\"--> \",city)\n",
    "#             print(selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inverse scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ƒê·ªìng Th√°p'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(output_process+city+'_train_preprocessed.csv', parse_dates=True, index_col= None, encoding = 'unicode_escape')\n",
    "df_valid = pd.read_csv(output_process+city+'_test_preprocessed.csv', parse_dates=True, index_col= None, encoding = 'unicode_escape')\n",
    "scaler = joblib.load(output_process+city+'_train_scalerMinMaxNorm.save') #ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3.1321351 ,   2.41436035,   2.34909895,   3.45840435,\n",
       "         0.45676998,   6.85154422,  15.53017807,  38.62969408,\n",
       "        61.92495655,  49.39641593,  58.85807336,  41.56761371,\n",
       "        12.72084476,   9.25152888,  22.74333523,  21.07292015,\n",
       "        12.7851002 ,  36.68486678,  72.9842607 ,  70.22165406,\n",
       "        68.10151378,  48.24927402,  22.22935459,  10.71291888,\n",
       "         5.3564547 ,   5.48399734,   5.61152741,   5.61152741,\n",
       "        10.5216112 ,  15.68678027,  16.64328095,  21.17077452,\n",
       "        23.1475535 ,  20.40555637,  13.26360832,   5.82057764,\n",
       "         3.03682077,   1.96127777,   2.72048533,   6.57978521,\n",
       "         5.69404095,   8.09818776,  14.61470782,  12.7166952 ,\n",
       "        16.51272044,   9.36352951,   6.32671183, 120.65066815,\n",
       "       110.28764901, 112.36025787, 122.72327701, 117.38475657,\n",
       "       110.53887395, 119.95980272, 118.45245311, 104.63509426,\n",
       "        99.54779875, 125.23552636, 120.65066815,   3.42999867,\n",
       "         1.8709052 ,   1.18490608,   1.43435802,   4.17835448,\n",
       "         3.86654585,   1.8085485 ,   7.67071933,  16.2768992 ,\n",
       "        21.20361898,  13.4081642 ,   8.98036087,   6.9999353 ,\n",
       "         4.39819359,   5.63711642,  10.09726123,   9.35389748,\n",
       "        12.07953524,  14.12376357,  20.8139745 ,  28.55727047,\n",
       "        39.27398374,  32.89351923,  26.26525004,  12.84889039,\n",
       "         9.09873674,  16.96790295,  27.29620404,  17.76712241,\n",
       "        12.60297187,  24.59117579,  24.83708174,  35.47276525,\n",
       "        33.99729187,  32.58330755,  29.14053633,   1.58585499,\n",
       "         6.16041473,  12.44282368,  16.59041796,  18.481238  ,\n",
       "        17.07838207,  33.66879694,  49.10033794,  87.95364037,\n",
       "        97.77370733,  68.92345527,   5.85544659,  16.33470395,\n",
       "         9.47291465,   9.83725933,  10.38377008,  14.63443713,\n",
       "        23.80373859,  37.46660776,  46.81807527,  74.14378846,\n",
       "        74.62958975,  74.44741112,  63.63857743,  26.05016407,\n",
       "        12.14868214,  20.06648633,  37.77576318,  59.89725105,\n",
       "       106.49743316, 115.08008624, 126.20126927, 120.27802433,\n",
       "        85.70565565,  46.90238696,  29.857959  ,  13.29323373,\n",
       "         5.23308677,   4.09022016,   3.30826542,   4.33083226,\n",
       "         7.51879483,  10.9473695 ,  11.00751309,  16.96240792,\n",
       "        20.45112618,  19.06767262,  17.08270768,   8.22032339,\n",
       "         6.60026899,   6.00024138,   8.16033069,  11.88046913,\n",
       "        20.94083813,  25.20101147,  29.161171  ,  38.40153068,\n",
       "        32.64130092,  29.58118278,  24.78098712,  16.05175368,\n",
       "         8.50503409,   7.726399  ,   6.8878718 ,   7.96597999,\n",
       "        13.77575309,  34.73885748,  43.96263146,  47.61620076,\n",
       "        41.38715778,  24.31720368,  12.39818438,  12.50223795,\n",
       "         6.4604914 ,   7.71669152,   6.75958727,   8.85325833,\n",
       "        10.6478461 ,  16.80923854,  24.16701445,  25.00448539,\n",
       "        24.10719779,  22.61171846,  13.5789881 ,   6.38806257,\n",
       "         7.16418277,   8.35821481,  11.10447467,  17.61194113,\n",
       "        23.34328738,  40.53731358,  54.20895971,  39.46268851,\n",
       "        33.97014363,  34.9253743 ,  20.47760797,   9.65205026,\n",
       "         5.60057507,   6.37512346,   3.4556757 ,   4.28980183,\n",
       "         4.82602667,   6.13679991,   5.00477248,   5.36225152,\n",
       "         7.2092496 ,   8.22212154,   6.49427895])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_test c·ªôt dengue fever rate l√† ƒë√£ l√† gi√° tr·ªã g·ªëc r ko c·∫ßn inverse g√¨ n·ªØa\n",
    "df_train_inverse = scaler.inverse_transform(df_train.iloc[:,:-1])[:,[-1]].reshape(len(df_train))\n",
    "df_train_inverse\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
