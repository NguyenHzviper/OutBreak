{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ja18nXEglgwF",
    "outputId": "f3edd346-8abb-4463-bb2d-53aac7fd7fc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Feb  1 02:28:18 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.60.13    Driver Version: 525.60.13    CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro RTX 6000     On   | 00000000:21:00.0 Off |                  Off |\n",
      "| 34%   35C    P8    23W / 260W |      1MiB / 24576MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VW4v2IEcTYHa",
    "outputId": "d42b58ac-62a9-4bbe-a0cb-05926ecef44e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /home/mlworker/anaconda3/envs/climate_diseases_py39/lib/python3.9/site-packages (1.2.0)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.2.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /home/mlworker/anaconda3/envs/climate_diseases_py39/lib/python3.9/site-packages (from scikit-learn) (1.23.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/mlworker/anaconda3/envs/climate_diseases_py39/lib/python3.9/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/mlworker/anaconda3/envs/climate_diseases_py39/lib/python3.9/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /home/mlworker/anaconda3/envs/climate_diseases_py39/lib/python3.9/site-packages (from scikit-learn) (1.8.1)\n",
      "Installing collected packages: scikit-learn\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.2.0\n",
      "    Uninstalling scikit-learn-1.2.0:\n",
      "      Successfully uninstalled scikit-learn-1.2.0\n",
      "Successfully installed scikit-learn-1.2.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.9 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: ftfy in /home/mlworker/anaconda3/envs/climate_diseases_py39/lib/python3.9/site-packages (6.1.1)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /home/mlworker/anaconda3/envs/climate_diseases_py39/lib/python3.9/site-packages (from ftfy) (0.2.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.9 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: optuna in /home/mlworker/anaconda3/envs/climate_diseases_py39/lib/python3.9/site-packages (3.0.2)\n",
      "Requirement already satisfied: colorlog in /home/mlworker/anaconda3/envs/climate_diseases_py39/lib/python3.9/site-packages (from optuna) (6.7.0)\n",
      "Requirement already satisfied: scipy<1.9.0,>=1.7.0 in /home/mlworker/anaconda3/envs/climate_diseases_py39/lib/python3.9/site-packages (from optuna) (1.8.1)\n",
      "Requirement already satisfied: cmaes>=0.8.2 in /home/mlworker/anaconda3/envs/climate_diseases_py39/lib/python3.9/site-packages (from optuna) (0.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/mlworker/.local/lib/python3.9/site-packages (from optuna) (21.3)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /home/mlworker/anaconda3/envs/climate_diseases_py39/lib/python3.9/site-packages (from optuna) (1.8.1)\n",
      "Requirement already satisfied: PyYAML in /home/mlworker/anaconda3/envs/climate_diseases_py39/lib/python3.9/site-packages (from optuna) (6.0)\n",
      "Requirement already satisfied: cliff in /home/mlworker/anaconda3/envs/climate_diseases_py39/lib/python3.9/site-packages (from optuna) (4.0.0)\n",
      "Requirement already satisfied: tqdm in /home/mlworker/anaconda3/envs/climate_diseases_py39/lib/python3.9/site-packages (from optuna) (4.64.1)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in /home/mlworker/anaconda3/envs/climate_diseases_py39/lib/python3.9/site-packages (from optuna) (1.4.41)\n",
      "Requirement already satisfied: numpy in /home/mlworker/anaconda3/envs/climate_diseases_py39/lib/python3.9/site-packages (from optuna) (1.23.3)\n",
      "Requirement already satisfied: Mako in /home/mlworker/anaconda3/envs/climate_diseases_py39/lib/python3.9/site-packages (from alembic>=1.5.0->optuna) (1.2.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/mlworker/.local/lib/python3.9/site-packages (from packaging>=20.0->optuna) (3.0.9)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/mlworker/anaconda3/envs/climate_diseases_py39/lib/python3.9/site-packages (from sqlalchemy>=1.3.0->optuna) (1.1.3)\n",
      "Requirement already satisfied: cmd2>=1.0.0 in /home/mlworker/anaconda3/envs/climate_diseases_py39/lib/python3.9/site-packages (from cliff->optuna) (2.4.2)\n",
      "Requirement already satisfied: stevedore>=2.0.1 in /home/mlworker/anaconda3/envs/climate_diseases_py39/lib/python3.9/site-packages (from cliff->optuna) (4.0.0)\n",
      "Requirement already satisfied: autopage>=0.4.0 in /home/mlworker/anaconda3/envs/climate_diseases_py39/lib/python3.9/site-packages (from cliff->optuna) (0.5.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/mlworker/.local/lib/python3.9/site-packages (from cliff->optuna) (5.0.0)\n",
      "Requirement already satisfied: PrettyTable>=0.7.2 in /home/mlworker/anaconda3/envs/climate_diseases_py39/lib/python3.9/site-packages (from cliff->optuna) (3.4.1)\n",
      "Requirement already satisfied: attrs>=16.3.0 in /home/mlworker/anaconda3/envs/climate_diseases_py39/lib/python3.9/site-packages (from cmd2>=1.0.0->cliff->optuna) (22.1.0)\n",
      "Requirement already satisfied: pyperclip>=1.6 in /home/mlworker/anaconda3/envs/climate_diseases_py39/lib/python3.9/site-packages (from cmd2>=1.0.0->cliff->optuna) (1.8.2)\n",
      "Requirement already satisfied: wcwidth>=0.1.7 in /home/mlworker/anaconda3/envs/climate_diseases_py39/lib/python3.9/site-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/mlworker/.local/lib/python3.9/site-packages (from importlib-metadata>=4.4->cliff->optuna) (3.8.1)\n",
      "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /home/mlworker/anaconda3/envs/climate_diseases_py39/lib/python3.9/site-packages (from stevedore>=2.0.1->cliff->optuna) (5.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /home/mlworker/anaconda3/envs/climate_diseases_py39/lib/python3.9/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.9 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install packages\n",
    "%pip install -U scikit-learn\n",
    "%pip install ftfy\n",
    "%pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "b8I8S9koHjT_"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from ftfy import fix_text\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MNAuUk6_HVs1",
    "outputId": "edf798b0-1fe3-4a65-da90-18e86aa7a993"
   },
   "outputs": [],
   "source": [
    "# Attach Google Drive for reading and saving files\n",
    "\n",
    "prj_path = '/home/mlworker/Quang/HealthCare/Source_14012023_v4/'\n",
    "data_path = prj_path + \"/data/\"\n",
    "prj_path_opt= prj_path + \"/optimize_hyperparam/opt_results/\"\n",
    "os.chdir(prj_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "smSb4hloMoQW"
   },
   "source": [
    "# Create Dict data for all cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "CFCMMJ5DMpL8"
   },
   "outputs": [],
   "source": [
    "cities = ['An Giang', 'BR Vũng Tàu', 'Bình Phước', 'Bình Thuận', 'Bình Định',\n",
    "        'Bạc Liêu', 'Bắc Kạn', 'Bắc Giang', 'Cao Bằng', 'Cà Mau',\n",
    "        'Cần Thơ', 'Gia Lai', 'Hà Giang', 'Hà Nội', 'Hà Tĩnh', 'Hòa Bình',\n",
    "        'Hưng Yên', 'Hải Dương', 'Hải Phòng', 'Khánh Hòa', 'Kiên Giang',\n",
    "        'Kon Tum', 'Lai Châu', 'Long An', 'Lào Cai', 'Lâm Đồng',\n",
    "        'Lạng Sơn', 'Nam Định', 'Nghệ An', 'Ninh Bình', 'Ninh Thuận',\n",
    "        'Phú Thọ', 'Phú Yên', 'Quảng Bình', 'Quảng Nam', 'Quảng Ngãi',\n",
    "        'Quảng Ninh', 'Quảng Trị', 'Sóc Trăng', 'Sơn La', 'TT Huế',\n",
    "        'Thanh Hóa', 'Thái Bình', 'Thái Nguyên', 'Tiền Giang', 'Trà Vinh',\n",
    "        'Tuyên Quang', 'Tây Ninh', 'Vĩnh Phúc', 'Yên Bái', 'Điện Biên',\n",
    "         'Đà Nẵng', 'Đắk Nông', 'Đắk Lắk', 'Đồng Tháp']\n",
    "\n",
    "def get_dict_all_city_data():\n",
    "  cities_data = {}  \n",
    "  for city in cities:\n",
    "    city_result = pd.read_excel(prj_path+'/data/new_data/DH/squeezed/squeezed_'+city+'.xlsx')  \n",
    "    # Đoạn này rất quan trọng. Vì việc optimize không được đụng vào 24 tháng (2016-2017) để dự báo. \n",
    "    # Dữ liệu optimize tính từ 1997- 30/12/2015. Sau đó tách ra train và test trên bộ này.\n",
    "    # lọc 2 năm cuối ra khỏi bộ dữ liệu trước khi chạy optimize \n",
    "    # đoạn này áp dụng cho tất cả các bước optimize trong project\n",
    "    city_result = city_result.loc[city_result['year_month'] < '2014-1-1'] \n",
    "    cities_data[city] = city_result\n",
    "  return cities_data\n",
    "\n",
    "dict_full_data = get_dict_all_city_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P9ezcs0VNarC"
   },
   "source": [
    "# Seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "fVkHGeUPO6cO"
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    import random, os\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    \n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "lEn56jmQtEmD"
   },
   "outputs": [],
   "source": [
    "# Set hyperparameters as args using the Configuration class\n",
    "class Configuration():\n",
    "    def __init__(self):\n",
    "        self.test_size = 36\n",
    "        self.look_back = 3\n",
    "        self.n_predicted_month = 3\n",
    "        self.n_features = 3\n",
    "        self.seed = 42\n",
    "        self.batch_size = 16\n",
    "        self.device = torch.device(\"cuda\")\n",
    "        self.epochs = 300\n",
    "\n",
    "args = Configuration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GT6wniFkNdmC"
   },
   "source": [
    "# Supporting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ukoPuS5CHvdl"
   },
   "outputs": [],
   "source": [
    "# Define data (pre-)processing functions\n",
    "# modification\n",
    "def get_city_data(city_name):\n",
    "    \"\"\"Returns Diarrhoea rate and climate data\"\"\" \n",
    "    city_data = dict_full_data[city_name].drop(columns=['Diarrhoea_cases', 'province', 'year_month',\n",
    "                                                        'Influenza_rates','Dengue_fever_rates',\n",
    "                                                        'Influenza_cases','Dengue_fever_cases', 'year', 'month'], \n",
    "                                                                  axis=1, \n",
    "                                                                  inplace=False)    \n",
    "    return city_data\n",
    "\n",
    "def convert_to_stationary(city_data):\n",
    "    \"\"\"Subtracts previous value for all cols except disease rates\"\"\"\n",
    "    for col_name in city_data.columns:\n",
    "        if col_name != 'Diarrhoea_rates':\n",
    "            try:\n",
    "                city_data[col_name] = city_data[col_name] - city_data[col_name].shift()\n",
    "            except:\n",
    "                print(col_name)\n",
    "    return city_data\n",
    "\n",
    "def impute_missing_value(city_data):\n",
    "    \"\"\"\n",
    "    Imputes 0 for first 12 months, \n",
    "    last year's value for months 12-24, \n",
    "    and minimum value of last two years for months 25+\n",
    "    \"\"\"\n",
    "    for col in city_data.columns:\n",
    "        for index in range(len(city_data[col])):\n",
    "            if np.isnan(city_data[col].iloc[index]):\n",
    "                if index < 12:\n",
    "                    city_data[col].iloc[index] = 0\n",
    "                elif index >= 12 and index <= 24:\n",
    "                    city_data[col].iloc[index] = city_data[col].iloc[index - 12]\n",
    "                else:\n",
    "                    city_data[col].iloc[index] = min(city_data[col].iloc[index - 12], city_data[col].iloc[index - 24])\n",
    "    return city_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "RT8LmtHts4fQ"
   },
   "outputs": [],
   "source": [
    "def split_data(data):\n",
    "    \"\"\"Splits data into train and test sets based on args (Configuration class)\"\"\"\n",
    "    train = data[: -args.test_size]\n",
    "    test = data[-args.test_size - args.look_back: ]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "6Wdc44U0uMbP"
   },
   "outputs": [],
   "source": [
    "def to_supervised(data, d_in=args.look_back, d_out=args.n_predicted_month, features_list=[]):\n",
    "    \"\"\"\n",
    "    Frames time-series as supervised learning dataset.\n",
    "    \n",
    "    Args:\n",
    "      d_in: lookback window\n",
    "      d_out: number of predicted months\n",
    "      features_list: list of all features **where last col is the disease incidence**\n",
    "\n",
    "    Returns:\n",
    "      Numpy arrays of disease incidence (y) and other predictors (X)\n",
    "    \"\"\"\n",
    "    X, y = list(), list()\n",
    "    for index, _ in enumerate(data):\n",
    "        in_end = index + d_in\n",
    "        out_end = in_end + d_out\n",
    "        if out_end <= len(data):\n",
    "            if len(features_list) == 0 :\n",
    "                X.append(data[index: in_end, :])\n",
    "            else:\n",
    "                X.append(data[index: in_end, features_list])\n",
    "            y.append(data[in_end: out_end, -1])\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "x_RfqYtVGTAk"
   },
   "outputs": [],
   "source": [
    "def select_feature(train, specific_data):\n",
    "    \"\"\"Selects args.n_features top features using RFE\"\"\"\n",
    "    train_X, train_y = to_supervised(train, d_in=1, d_out=1)\n",
    "    train_X, train_y = np.squeeze(train_X), np.squeeze(train_y)\n",
    "    rfe = RFE(RandomForestRegressor(n_estimators=500, random_state=args.seed), n_features_to_select=args.n_features)\n",
    "    fit = rfe.fit(train_X, train_y)\n",
    "    important_features = list()\n",
    "    for i in range(len(fit.support_)):\n",
    "        if fit.support_[i]:\n",
    "            important_features.append(i)\n",
    "    return np.array(important_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "OsgC3mbHV96C"
   },
   "outputs": [],
   "source": [
    "def get_data(train_np, test_np, batch_size, specific_data):\n",
    "    \"\"\"\n",
    "    Returns important feature list and data formatted for input into Pytorch \n",
    "    models\n",
    "    \"\"\"\n",
    "    important_features = select_feature(train_np, specific_data)\n",
    "\n",
    "    train_X, train_y = to_supervised(train_np, features_list=important_features)\n",
    "    test_X, test_y = to_supervised(test_np, features_list=important_features)\n",
    "    train_tensor = TensorDataset(torch.from_numpy(train_X), torch.from_numpy(train_y))\n",
    "    test_tensor = (torch.from_numpy(test_X), torch.from_numpy(test_y))\n",
    "\n",
    "    train_loader = DataLoader(train_tensor, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return important_features, train_loader, test_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "PW18DMZgF5K_"
   },
   "outputs": [],
   "source": [
    "#Define Pytorch LSTM model\n",
    "class MultiVariateLSTM(nn.Module):\n",
    "    def __init__(self, n_feature=3, n_layers=2, hidden_size=50):\n",
    "        super(MultiVariateLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=n_feature, hidden_size=hidden_size, num_layers=n_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, args.n_predicted_month)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.sigma = nn.Parameter(torch.ones(args.n_predicted_month))\n",
    "    \n",
    "    def forward(self, X_batch, y_batch=None):\n",
    "        output, (last_hidden, _) = self.lstm(X_batch)\n",
    "        last_hidden_vector = output[:, -1, :]\n",
    "        y_predicted = self.linear(last_hidden_vector)\n",
    "        if y_batch != None:\n",
    "            assert y_predicted.size() == y_batch.size()\n",
    "            loss = self.loss_fn(y_predicted, y_batch)\n",
    "            loss = 0.5 * loss / self.sigma**2\n",
    "            loss = loss.sum() + torch.log(1 + self.sigma.prod())\n",
    "            return y_predicted, loss\n",
    "            #return y_predicted, self.loss_fn(y_predicted, y_batch)\n",
    "        else:\n",
    "            return y_predicted\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = torch.tensor(X, device=args.device)\n",
    "        return self.forward(X)\n",
    "\n",
    "#Define Pytorch LSTM-ATT model\n",
    "class MultiVariateLSTM_Attention(nn.Module):\n",
    "    def __init__(self, n_feature=3, n_layers=2, hidden_size=50):\n",
    "        super(MultiVariateLSTM_Attention, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=n_feature, hidden_size=hidden_size, num_layers=n_layers, batch_first=True)\n",
    "        self.attention_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        # self.linear = nn.Linear(hidden_size*2, args.n_predicted_month)\n",
    "        self.linear = nn.Linear(hidden_size, args.n_predicted_month)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.sigma = nn.Parameter(torch.ones(args.n_predicted_month))\n",
    "    \n",
    "    def forward(self, X_batch, y_batch=None):\n",
    "        output, (last_hidden, _) = self.lstm(X_batch)\n",
    "        last_hidden_vector = last_hidden[-1]\n",
    "        remain_hidden_vector = output\n",
    "        e_t = remain_hidden_vector.bmm(self.attention_linear(last_hidden_vector).unsqueeze(2)).squeeze(-1)\n",
    "        alpha_t = F.softmax(e_t, dim=1)\n",
    "        attenion_vector = remain_hidden_vector.transpose(2, 1).bmm(alpha_t.unsqueeze(2)).squeeze(-1)\n",
    "        # combine_vector = torch.cat((last_hidden_vector, attenion_vector), dim=1)\n",
    "        # combine_vector = last_hidden_vector + attenion_vector\n",
    "        y_predicted = self.linear(attenion_vector)\n",
    "        if y_batch != None:\n",
    "            assert y_predicted.size() == y_batch.size()\n",
    "            loss = self.loss_fn(y_predicted, y_batch)\n",
    "            loss = 0.5 * loss / self.sigma**2\n",
    "            loss = loss.sum() + torch.log(1 + self.sigma.prod())\n",
    "            return y_predicted, loss\n",
    "            # return y_predicted, self.loss_fn(y_predicted, y_batch)\n",
    "        else:\n",
    "            return y_predicted\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = torch.tensor(X, device=args.device)\n",
    "        return self.forward(X)\n",
    "\n",
    "# Define Pytorch CNN model\n",
    "class MultivariateCNN(nn.Module):\n",
    "    def __init__(self, num_filters=[100, 100, 100], dropout=0.01):\n",
    "        super(MultivariateCNN, self).__init__()\n",
    "        self.loss_fn = loss = nn.MSELoss()\n",
    "        self.filter_sizes = [1, 2, 3]\n",
    "        self.conv1d_list = nn.ModuleList([nn.Conv1d(args.n_features, num_filters[i], self.filter_sizes[i]) for i in range(len(self.filter_sizes))])\n",
    "        self.linear = nn.Linear(np.sum(num_filters), args.n_predicted_month)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.sigma = nn.Parameter(torch.ones(args.n_predicted_month))\n",
    "\n",
    "    def forward(self, X_batch, y_batch=None):\n",
    "        X_batch = X_batch.permute(0, 2, 1)  #(batch_size, n_features, n_look_back)\n",
    "        X_conv_list = [F.relu(conv1d(X_batch)) for conv1d in self.conv1d_list]\n",
    "        X_pool_list = [F.max_pool1d(x_conv, kernel_size=x_conv.shape[2]) for x_conv in X_conv_list]\n",
    "        X_fc = torch.cat([x_pool.squeeze(dim=2) for x_pool in X_pool_list], dim=1)\n",
    "        y_predicted = self.linear(self.dropout(X_fc))\n",
    "        if y_batch != None:\n",
    "            assert y_predicted.size() == y_batch.size()\n",
    "            loss = self.loss_fn(y_predicted, y_batch)\n",
    "            loss = 0.5 * loss / self.sigma**2\n",
    "            loss = loss.sum() + torch.log(1 + self.sigma.prod())\n",
    "            return y_predicted, loss\n",
    "            # return y_predicted, self.loss_fn(y_predicted, y_batch)\n",
    "        else:\n",
    "            return y_predicted\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = torch.tensor(X, device=args.device)\n",
    "        return self.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "8TvH93W9mNWQ"
   },
   "outputs": [],
   "source": [
    "# Create class to train and evaluate models\n",
    "class Trainer():\n",
    "    def __init__(self, model_type, learning_rate, important_features, train_loader, test_tensor, n_layers=2, hidden_size=128, num_filters=[100, 100, 100], dropout=0.01):\n",
    "        \"\"\"\n",
    "        Initialise trainer, allowing input of LSTM, LSTM-ATT, or CNN \n",
    "        hyperparameters. Adam optimiser used for all models.\n",
    "        \"\"\"\n",
    "        self.model_type = model_type\n",
    "        self.model = self.init_model(model_type, n_layers, hidden_size, num_filters, dropout)\n",
    "        self.model.double().to(args.device)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        self.important_features, self.train_loader, self.test_tensor = important_features, train_loader, test_tensor\n",
    "    \n",
    "    def init_model(self, model_type, n_layers, hidden_size, num_filters, dropout):\n",
    "        \"\"\"Initialise a model based on whether LSTM, LSTM-ATT, or CNN is chosen.\"\"\"\n",
    "        if model_type.lower() == 'lstm':\n",
    "            model = MultiVariateLSTM(args.n_features, n_layers, hidden_size)\n",
    "        elif model_type.lower() == 'lstm_attention':\n",
    "            model = MultiVariateLSTM_Attention(args.n_features, n_layers, hidden_size)\n",
    "        elif model_type.lower() == 'cnn':\n",
    "            model = MultivariateCNN(num_filters, dropout)\n",
    "        return model\n",
    "\n",
    "    def step(self, batch):\n",
    "        self.model.train()\n",
    "        X_batch, y_batch = tuple(t.to(args.device) for t in batch)\n",
    "        self.optimizer.zero_grad()\n",
    "        y_pred, loss = self.model.forward(X_batch, y_batch)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.mean().item()\n",
    "\n",
    "    def validation(self):\n",
    "        self.model.eval()\n",
    "        eval_loss = 0.0\n",
    "\n",
    "        result = {}\n",
    "\n",
    "        y_true = np.array([])\n",
    "        y_pred = np.array([])\n",
    "\n",
    "        X_batch, y_batch = tuple(t.to(args.device) for t in self.test_tensor)\n",
    "        with torch.no_grad():\n",
    "            outputs, loss = self.model.forward(X_batch, y_batch)\n",
    "            eval_loss = loss.mean().item()\n",
    "\n",
    "        return eval_loss\n",
    "\n",
    "    def train(self, epochs=20):\n",
    "        best_lost = float(\"inf\")\n",
    "        best_model = None\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0.0\n",
    "            for batch in self.train_loader:\n",
    "                loss = self.step(batch)\n",
    "                total_loss += loss\n",
    "            train_loss = total_loss/len(self.train_loader)\n",
    "            eval_loss = self.validation()\n",
    "            if eval_loss < best_lost:\n",
    "                best_lost = eval_loss\n",
    "                best_model = copy.deepcopy(self.model)\n",
    "            if (epoch + 1) == epochs or (epoch + 1) in [c + 1 for c in range(epochs) if c % int(epochs/4) == 0]:\n",
    "                print(f\"Epoch: {epoch:2}/{epochs:2} - train_loss: {train_loss:.4f} - test_loss: {eval_loss:4f}\")\n",
    "        self.model = best_model\n",
    "        self.model.eval()\n",
    "        return None\n",
    "    \n",
    "    def evaluate_model(self, np_data=None, plot=True, scaled=True, city=None, k_steps=None, y_scaler = None):\n",
    "        assert scaled, \"data must be scaled\"\n",
    "        self.model.eval()\n",
    "        tensor_data = torch.from_numpy(np_data) # np_data = bộ dữ liệu đưa vào\n",
    "        rmse_list = []\n",
    "        mae_list = [] \n",
    "        mape_list = []\n",
    "\n",
    "        y_predicted_list = []\n",
    "        y_true_list = []\n",
    "\n",
    "        for k_steps in range(1, args.n_predicted_month + 1):\n",
    "            y_predicted = []\n",
    "            y_true = []\n",
    "            for index in range(tensor_data.size(0) - args.look_back):\n",
    "                X = tensor_data[index: index + args.look_back, self.important_features]\n",
    "                # yhat = self.model.predict(X.unsqueeze(0)).squeeze()\n",
    "\n",
    "                yhat = self.model.predict(X.unsqueeze(0))\n",
    "                yhat = yhat.squeeze()\n",
    "\n",
    "                y_predicted.append(yhat.detach().cpu().numpy()[k_steps - 1])\n",
    "                y_true.append(tensor_data[index + args.look_back, -1].detach().cpu().numpy())\n",
    "\n",
    "            y_predicted = y_scaler.inverse_transform(np.array(y_predicted).reshape(-1, 1)).reshape(-1, )\n",
    "            y_true = y_scaler.inverse_transform(np.array(y_true).reshape(-1, 1)).reshape(-1, )\n",
    "\n",
    "            if plot==True:\n",
    "                plt.plot(y_predicted, label='predicted')\n",
    "                plt.plot(y_true, label='actual')\n",
    "                plt.title(f\"k-steps = {k_steps}\")\n",
    "                plt.legend()\n",
    "                plt.show()\n",
    "\n",
    "                plt.show()\n",
    "\n",
    "            rmse = mean_squared_error(y_true, y_predicted, squared=False)\n",
    "            mae = mean_absolute_error(y_true, y_predicted)\n",
    "            mape = mean_absolute_percentage_error(y_true, y_predicted)\n",
    "\n",
    "            rmse_list.append(rmse)\n",
    "            mae_list.append(mae)\n",
    "            mape_list.append(mape)\n",
    "\n",
    "            y_predicted_list.append(y_predicted)\n",
    "            y_true_list.append(y_true)\n",
    "\n",
    "        return y_true_list, y_predicted_list, rmse_list, mae_list, mape_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "zwTWTB5sp26U"
   },
   "outputs": [],
   "source": [
    "def concate_2_filter_str(listfilter = ''):\n",
    "  string_filter = ','.join(str(e) for e in listfilter)\n",
    "  return string_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bfKNKZsHNvTk"
   },
   "source": [
    "# Objective and main run functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "VLj6VKl3_fDD"
   },
   "outputs": [],
   "source": [
    "def objective(trial, city):    \n",
    "    # Define search parameters   \n",
    "    args.epochs = trial.suggest_int('Epochs', 100, 500, step=10)\n",
    "    learning_rate = trial.suggest_loguniform('Learning rate', 1e-4, 1e-2)\n",
    "    num_filters = trial.suggest_categorical('Num. filters', [[64, 64, 64], [100, 100, 100], [128, 128, 128], [16, 32, 64], [32, 64, 128]])\n",
    "    dropout = trial.suggest_uniform('Dropout rate', 0.01, 0.80)\n",
    "\n",
    "    # Pre-process data\n",
    "    specific_data = get_city_data(fix_text(city))\n",
    "    specific_data = impute_missing_value(specific_data)\n",
    "    specific_data = convert_to_stationary(specific_data)\n",
    "    specific_data.dropna(inplace=True)\n",
    "    train, test = split_data(specific_data)\n",
    "\n",
    "    # Fit data scaler to training data\n",
    "    full_scaler = MinMaxScaler().fit(train)\n",
    "    y_scaler = MinMaxScaler().fit(train.values[:, -1].reshape(-1, 1))\n",
    "\n",
    "    # Scale train and test data\n",
    "    train = full_scaler.transform(train)\n",
    "    test = full_scaler.transform(test)\n",
    "\n",
    "    # Get data to run model\n",
    "    important_features, train_loader, test_tensor = get_data(train, test, args.batch_size, specific_data)\n",
    "\n",
    "    # CNN model\n",
    "    trainer = Trainer(model_type='cnn',\n",
    "                  learning_rate=learning_rate,\n",
    "                  important_features=important_features,\n",
    "                  train_loader=train_loader,\n",
    "                  test_tensor=test_tensor,\n",
    "                  num_filters=num_filters, \n",
    "                  dropout=dropout)\n",
    "\n",
    "    # Train model\n",
    "    # trainer.train(epochs=args.epochs, trial=trial)\n",
    "    trainer.train(epochs=args.epochs)\n",
    "\n",
    "    \n",
    "    # Evaluate model\n",
    "    y_true, y_pred, rmse_list, mae_list, mape_list = trainer.evaluate_model(np_data=test, plot= False, scaled=True, city=city, y_scaler = y_scaler)\n",
    "    # _, _, rmse, mae, = trainer.evaluate_model(np_data=test, plot=False, scaled=True, city=city, y_scaler=y_scaler)\n",
    "\n",
    "    return rmse_list[0]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ggBbhETfXbsk",
    "outputId": "6fd0923e-fa17-4a55-96aa-de121852e28e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-01 02:28:29,458]\u001b[0m A new study created in memory with name: no-name-10ce151b-d103-430e-a37d-270e567ebebf\u001b[0m\n",
      "<ipython-input-16-fed4c598d5d7>:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  learning_rate = trial.suggest_loguniform('Learning rate', 1e-4, 1e-2)\n",
      "<ipython-input-16-fed4c598d5d7>:6: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  dropout = trial.suggest_uniform('Dropout rate', 0.01, 0.80)\n",
      "<ipython-input-8-d24461057253>:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  city_data[col].iloc[index] = 0\n",
      "<ipython-input-8-d24461057253>:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  city_data[col].iloc[index] = city_data[col].iloc[index - 12]\n",
      "<ipython-input-8-d24461057253>:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  city_data[col].iloc[index] = min(city_data[col].iloc[index - 12], city_data[col].iloc[index - 24])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0/250 - train_loss: 0.7300 - test_loss: 0.627712\n",
      "Epoch: 62/250 - train_loss: 0.1215 - test_loss: 1.698870\n",
      "Epoch: 124/250 - train_loss: 0.1297 - test_loss: 2.422188\n",
      "Epoch: 186/250 - train_loss: 0.1112 - test_loss: 3.221865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-01 02:29:03,459]\u001b[0m Trial 0 finished with value: 70.33617310124798 and parameters: {'Epochs': 250, 'Learning rate': 0.007408567284229461, 'Num. filters': [100, 100, 100], 'Dropout rate': 0.37939243877115864}. Best is trial 0 with value: 70.33617310124798.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 248/250 - train_loss: 0.1688 - test_loss: 2.830805\n",
      "Epoch: 249/250 - train_loss: 0.1533 - test_loss: 2.987746\n",
      "Study statistics for : \n",
      "  Number of finished trials:  1\n",
      "Best trial of city:  An Giang\n",
      "  Value:  70.33617310124798\n",
      "kết thúc study trong: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-716ad362280d>:64: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  best_param = best_param.append(one_city_param)\n"
     ]
    }
   ],
   "source": [
    "#########################\n",
    "# Main Cell for optimize\n",
    "#########################\n",
    "dt_started = datetime.now()\n",
    "\n",
    "# Input param for Optimize Run\n",
    "ntry = 1\n",
    "njob = -1\n",
    "cities = [\n",
    "    'An Giang', 'BR Vũng Tàu', 'Bình Phước', 'Bình Thuận', 'Bình Định', 'Bạc Liêu', 'Bắc Kạn', 'Bắc Giang',\n",
    "#     'Cao Bằng', 'Cà Mau', 'Cần Thơ',\n",
    "#         'Gia Lai', 'Hà Giang', 'Hà Nội', 'Hà Tĩnh', 'Hòa Bình', 'Hưng Yên', 'Hải Dương', 'Hải Phòng', 'Khánh Hòa', 'Kiên Giang', 'Kon Tum',\n",
    "#         'Lai Châu', 'Long An', 'Lào Cai', 'Lâm Đồng', 'Lạng Sơn', 'Nam Định', 'Nghệ An', 'Ninh Bình', 'Ninh Thuận', 'Phú Thọ', 'Phú Yên',\n",
    "#         'Quảng Bình', 'Quảng Nam', 'Quảng Ngãi', 'Quảng Ninh', 'Quảng Trị', 'Sóc Trăng', 'Sơn La', 'TT Huế', 'Thanh Hóa', 'Thái Bình', 'Thái Nguyên', \n",
    "#         'Tiền Giang', 'Trà Vinh', 'Tuyên Quang', 'Tây Ninh', 'Vĩnh Phúc', 'Yên Bái', 'Điện Biên', 'Đà Nẵng', 'Đắk Nông', 'Đắk Lắk', 'Đồng Tháp'\n",
    "         ]\n",
    "\n",
    "\n",
    "# Lưu thông tin traceback study và error city trong quá trình optimize\n",
    "l_study_city ={}\n",
    "l_errCity =[]\n",
    " \n",
    "\n",
    "if __name__ == '__main__':  \n",
    "  best_param = pd.DataFrame()\n",
    "  for city in cities:\n",
    "    # Use Tree-structured Parzen Estimator sampler to minimise RMSE\n",
    "    sampler = optuna.samplers.TPESampler()\n",
    "    study = optuna.create_study(sampler=sampler, direction='minimize')\n",
    "\n",
    "    # truyền multiple param vào trong biến trial\n",
    "    obj_func = lambda trial: objective(trial, city)\n",
    "\n",
    "    try:\n",
    "      # Optimise over 100 trials\n",
    "      study.optimize(obj_func, n_trials=ntry, n_jobs=njob)\n",
    "\n",
    "      # Print results\n",
    "      print(\"Study statistics for : \")\n",
    "      print(\"  Number of finished trials: \", len(study.trials))\n",
    "  \n",
    "      \n",
    "      print(\"Best trial of city: \",city)\n",
    "      best_trial = study.best_trial\n",
    "      print(\"  Value: \", best_trial.value)   \n",
    "\n",
    "      # lưu best param vào trong biến toàn cục\n",
    "      one_city_param = pd.DataFrame({                     \n",
    "                              'City': city,\n",
    "                              'Alg_name': 'cnn',\n",
    "                              'Best_value': best_trial.value,\n",
    "                              'n_try_opt': ntry,\n",
    "                              'n Feature': 3, # dùng chung cho tất cả các model và algorithm\n",
    "                              'Batch Size': 16, # dùng chung cho tất cả các model và algorithm\n",
    "                              'Lookback Window': 3, # dùng chung cho tất cả các tỉnh của CNN\n",
    "                              'Epochs': best_trial.params['Epochs'],\n",
    "                              'Hiden Size': '', # CNN không quan tâm\n",
    "                              'n Layers': '', # CNN không quan tâm\n",
    "                              'Learning rate': best_trial.params['Learning rate'], \n",
    "                              'Num. filters': concate_2_filter_str(best_trial.params['Num. filters']), # đoạn này ép kiểu không sẽ thành x 3 do cấu trúc [] \n",
    "                              'Dropout rate': best_trial.params['Dropout rate']}, index=[0])\n",
    "      one_city_param.to_excel(prj_path_opt+'/opt_res_01022023/cnn/diarrhoea_opt_hyperparam_cnn_'+city+'.xlsx')\n",
    "      best_param = best_param.append(one_city_param)\n",
    "    except:# có error thì lưu vào l_errCity để check lại sau \n",
    "      l_errCity.append(city)\n",
    "  # lưu kết quả vào file CNN\n",
    "  best_param.to_excel(prj_path_opt+'/opt_res_01022023/diarrhoea_opt_hyperparam_cnn_1.xlsx')\n",
    "\n",
    "dt_ended = datetime.now()\n",
    "print('kết thúc study trong:', round((dt_ended - dt_started).total_seconds()/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "RqdsvOFIMHrE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"ok\":true,\"result\":{\"message_id\":215,\"sender_chat\":{\"id\":-1001712314864,\"title\":\"PTN_Announcement\",\"username\":\"ptn_announcement\",\"type\":\"channel\"},\"chat\":{\"id\":-1001712314864,\"title\":\"PTN_Announcement\",\"username\":\"ptn_announcement\",\"type\":\"channel\"},\"date\":1675218545,\"text\":\"Server Ch\\u1ea1y Xong CNN _test _server: islab\"}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def send_to_telegram(message):\n",
    "\n",
    "    apiToken = '5908735099:AAGVSLrW62aXPBP-GrMvxoVgMsuJxXJpP1Q'\n",
    "    chatID = '@ptn_announcement'\n",
    "    apiURL = f'https://api.telegram.org/bot{apiToken}/sendMessage'\n",
    "\n",
    "    try:\n",
    "        response = requests.post(apiURL, json={'chat_id': chatID, 'text': message})\n",
    "        print(response.text)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "send_to_telegram(\"Server Chạy Xong CNN _1 _server: islab\" )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "P9ezcs0VNarC"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "306px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "24f3299fa11d098f3e36f80146bbe61fdadb7bfe8872ee0c4a379787469f5f10"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
