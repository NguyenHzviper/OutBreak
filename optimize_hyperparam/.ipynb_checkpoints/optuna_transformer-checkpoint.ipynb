{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4Smx2B-csrq"
   },
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17063,
     "status": "ok",
     "timestamp": 1675101187981,
     "user": {
      "displayName": "Nhat Le",
      "userId": "00702307310725808810"
     },
     "user_tz": -420
    },
    "id": "VW4v2IEcTYHa",
    "outputId": "3eae95c8-1dab-4f6b-be39-9e8ffc15a25c"
   },
   "outputs": [],
   "source": [
    "# Install packages\n",
    "%pip install -U scikit-learn\n",
    "%pip install ftfy\n",
    "%pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2280,
     "status": "ok",
     "timestamp": 1675101190254,
     "user": {
      "displayName": "Nhat Le",
      "userId": "00702307310725808810"
     },
     "user_tz": -420
    },
    "id": "b8I8S9koHjT_"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "# from google.colab import drive\n",
    "import os\n",
    "\n",
    "import traceback\n",
    "import sys\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from ftfy import fix_text\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oA7yvJS1czVO"
   },
   "source": [
    "# Set Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3325,
     "status": "ok",
     "timestamp": 1675101193568,
     "user": {
      "displayName": "Nhat Le",
      "userId": "00702307310725808810"
     },
     "user_tz": -420
    },
    "id": "MNAuUk6_HVs1",
    "outputId": "9e3b5911-d2e8-4c20-9936-c0cbc4955010"
   },
   "outputs": [],
   "source": [
    "\n",
    "prj_path = '/home/mlworker/Quang/HealthCare/Source_14012023_v4/'\n",
    "data_path = prj_path + \"/data/\"\n",
    "prj_path_opt= prj_path + \"optimize_hyperparam/opt_results/opt_results_10022023_v4/\"\n",
    "\n",
    "os.chdir(prj_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X94Oj9inc8Fk"
   },
   "source": [
    "# Create Dict data for all cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 675,
     "status": "ok",
     "timestamp": 1675101194240,
     "user": {
      "displayName": "Nhat Le",
      "userId": "00702307310725808810"
     },
     "user_tz": -420
    },
    "id": "ONDHBTEPRMTu"
   },
   "outputs": [],
   "source": [
    "cities = [\n",
    "#     'An Giang', 'BR Vũng Tàu', 'Bình Phước', 'Bình Thuận', 'Bình Định',\n",
    "#         'Bạc Liêu', 'Bắc Kạn', 'Bắc Giang', 'Cao Bằng', 'Cà Mau',\n",
    "#         'Cần Thơ', 'Gia Lai', 'Hà Giang', 'Hà Nội', 'Hà Tĩnh', 'Hòa Bình',\n",
    "#         'Hưng Yên', 'Hải Dương', 'Hải Phòng', 'Khánh Hòa', 'Kiên Giang',\n",
    "#         'Kon Tum', 'Lai Châu', 'Long An', 'Lào Cai', 'Lâm Đồng',\n",
    "#         'Lạng Sơn', 'Nam Định', 'Nghệ An', 'Ninh Bình', 'Ninh Thuận',\n",
    "#         'Phú Thọ', 'Phú Yên', 'Quảng Bình', 'Quảng Nam', 'Quảng Ngãi',\n",
    "#         'Quảng Ninh', 'Quảng Trị', 'Sóc Trăng', 'Sơn La',\n",
    "          'TT Huế',\n",
    "        'Thanh Hóa', 'Thái Bình', 'Thái Nguyên', 'Tiền Giang', 'Trà Vinh',\n",
    "        'Tuyên Quang', 'Tây Ninh', 'Vĩnh Phúc', 'Yên Bái', 'Điện Biên',\n",
    "         'Đà Nẵng', 'Đắk Nông', 'Đắk Lắk', 'Đồng Tháp']\n",
    "\n",
    "def get_dict_all_city_data():\n",
    "  cities_data = {}  \n",
    "  for city in cities:\n",
    "    city_result = pd.read_excel(prj_path+'data/new_data/DH/squeezed/squeezed_'+city+'.xlsx')  \n",
    "    # Đoạn này rất quan trọng. Vì việc optimize không được đụng vào 24 tháng (2016-2017) để dự báo. \n",
    "    # Dữ liệu optimize tính từ 1997- 30/12/2015. Sau đó tách ra train và test trên bộ này.\n",
    "    # lọc 2 năm cuối ra khỏi bộ dữ liệu trước khi chạy optimize \n",
    "    # đoạn này áp dụng cho tất cả các bước optimize trong project\n",
    "    city_result = city_result.loc[city_result['year_month'] < '2015-1-1'] \n",
    "    cities_data[city] = city_result\n",
    "  return cities_data\n",
    "\n",
    "dict_full_data = get_dict_all_city_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OUPWuxbOdBlU"
   },
   "source": [
    "# Seed and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1675101194240,
     "user": {
      "displayName": "Nhat Le",
      "userId": "00702307310725808810"
     },
     "user_tz": -420
    },
    "id": "fVkHGeUPO6cO"
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    import random, os\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    \n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1675101194241,
     "user": {
      "displayName": "Nhat Le",
      "userId": "00702307310725808810"
     },
     "user_tz": -420
    },
    "id": "lEn56jmQtEmD"
   },
   "outputs": [],
   "source": [
    "# Set hyperparameters as args using the Configuration class\n",
    "class Configuration():\n",
    "    def __init__(self):\n",
    "        self.test_size = 36\n",
    "        self.look_back = 3\n",
    "        self.n_predicted_month = 3\n",
    "        self.n_features = 3\n",
    "        self.seed = 42\n",
    "        self.batch_size = 16\n",
    "        self.device = torch.device(\"cuda\")\n",
    "        self.epochs = 300\n",
    "\n",
    "args = Configuration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4OyQATbT2nRz"
   },
   "source": [
    "# Pre-processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1675101194241,
     "user": {
      "displayName": "Nhat Le",
      "userId": "00702307310725808810"
     },
     "user_tz": -420
    },
    "id": "ukoPuS5CHvdl"
   },
   "outputs": [],
   "source": [
    "# Define data (pre-)processing functions\n",
    "# modification\n",
    "def get_city_data(city_name):\n",
    "    \"\"\"Returns Diarrhoea rate and climate data\"\"\" \n",
    "    city_data = dict_full_data[city_name].drop(columns=['Diarrhoea_cases', 'province', 'year_month',\n",
    "                                                        'Influenza_rates','Dengue_fever_rates',\n",
    "                                                        'Influenza_cases','Dengue_fever_cases', 'year', 'month'], \n",
    "                                                                  axis=1, \n",
    "                                                                  inplace=False)    \n",
    "    return city_data\n",
    "\n",
    "\n",
    "def convert_to_stationary(city_data):\n",
    "    \"\"\"Subtracts previous value for all cols except disease rates\"\"\"\n",
    "    for col_name in city_data.columns:\n",
    "        if col_name != 'Diarrhoea_rates':\n",
    "            try:\n",
    "                city_data[col_name] = city_data[col_name] - city_data[col_name].shift()\n",
    "            except:\n",
    "                print(col_name)\n",
    "    return city_data\n",
    "\n",
    "def impute_missing_value(city_data):\n",
    "    \"\"\"\n",
    "    Cơ bản dữ liệu bị thiếu sót rất nhiều: Như Điện Biên 1997 -2003 là thiếu dữ liệu về bệnh\n",
    "    Hàm này sẽ tự sinh ra dữ liệu bị thiếu. Nếu tháng nào không có số liệu thì tính như sau:\n",
    "    12 tháng đầu không có số liệu thì gán = 0\n",
    "    tháng 13-24 không có số liệu, sẽ lấy giá trị của tháng cùng kỳ năm trước\n",
    "    tháng từ 24 trở đi sẽ lấy giá trị nhỏ nhất của 2 tháng cùng kỳ trong 2 năm gần nhất.\n",
    "    Do Điện Biên bằng 0 nên sau khi xử lý từ 1997 -2003 là đều = 0.  \n",
    "    \"\"\"\n",
    "    for col in city_data.columns:\n",
    "        for index in range(len(city_data[col])):\n",
    "            if np.isnan(city_data[col].iloc[index]):\n",
    "                if index < 12:\n",
    "                    city_data[col].iloc[index] = 0\n",
    "                elif index >= 12 and index <= 24:\n",
    "                    city_data[col].iloc[index] = city_data[col].iloc[index - 12]\n",
    "                else:\n",
    "                    city_data[col].iloc[index] = min(city_data[col].iloc[index - 12], city_data[col].iloc[index - 24])\n",
    "    return city_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1675101194241,
     "user": {
      "displayName": "Nhat Le",
      "userId": "00702307310725808810"
     },
     "user_tz": -420
    },
    "id": "RT8LmtHts4fQ"
   },
   "outputs": [],
   "source": [
    "def split_data(data, look_back ):\n",
    "    \"\"\"Splits data into train and test sets based on args (Configuration class)\"\"\"\n",
    "    train = data[: -args.test_size]\n",
    "    print('lookback', look_back)\n",
    "    test = data[-args.test_size - look_back: ]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1675101194242,
     "user": {
      "displayName": "Nhat Le",
      "userId": "00702307310725808810"
     },
     "user_tz": -420
    },
    "id": "6Wdc44U0uMbP"
   },
   "outputs": [],
   "source": [
    "def to_supervised(data, d_in=args.look_back, d_out=args.n_predicted_month, features_list=[]):\n",
    "    \"\"\"\n",
    "    Frames time-series as supervised learning dataset.\n",
    "    \n",
    "    Args:\n",
    "      d_in: lookback window\n",
    "      d_out: number of predicted months\n",
    "      features_list: list of all features **where last col is the disease incidence**\n",
    "\n",
    "    Returns:\n",
    "      Numpy arrays of disease incidence (y) and other predictors (X)\n",
    "    \"\"\"\n",
    "    X, y = list(), list()\n",
    "    for index, _ in enumerate(data):\n",
    "        in_end = index + d_in\n",
    "        out_end = in_end + d_out\n",
    "        if out_end <= len(data):\n",
    "            if len(features_list) == 0 :\n",
    "                X.append(data[index: in_end, :])\n",
    "            else:\n",
    "                X.append(data[index: in_end, features_list])\n",
    "            y.append(data[in_end: out_end, -1])\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1675101194242,
     "user": {
      "displayName": "Nhat Le",
      "userId": "00702307310725808810"
     },
     "user_tz": -420
    },
    "id": "x_RfqYtVGTAk"
   },
   "outputs": [],
   "source": [
    "def select_feature(train, specific_data):\n",
    "    \"\"\"Selects args.n_features top features using RFE\"\"\"\n",
    "    train_X, train_y = to_supervised(train, d_in=1, d_out=1)\n",
    "    train_X, train_y = np.squeeze(train_X), np.squeeze(train_y)\n",
    "    rfe = RFE(RandomForestRegressor(n_estimators=500, random_state=args.seed), n_features_to_select=args.n_features)\n",
    "    fit = rfe.fit(train_X, train_y)\n",
    "    important_features = list()\n",
    "    # print(\"Important Feature:\")\n",
    "    for i in range(len(fit.support_)):\n",
    "        if fit.support_[i]:\n",
    "            important_features.append(i)\n",
    "            # print(specific_data.columns[i])\n",
    "    return np.array(important_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1675101194242,
     "user": {
      "displayName": "Nhat Le",
      "userId": "00702307310725808810"
     },
     "user_tz": -420
    },
    "id": "OsgC3mbHV96C"
   },
   "outputs": [],
   "source": [
    "def get_data(train_np, test_np, batch_size, specific_data):\n",
    "    \"\"\"\n",
    "    Returns important feature list and data formatted for input into Pytorch \n",
    "    models\n",
    "    \"\"\"\n",
    "    important_features = select_feature(train_np, specific_data)\n",
    "\n",
    "    train_X, train_y = to_supervised(train_np, features_list=important_features)\n",
    "    test_X, test_y = to_supervised(test_np, features_list=important_features)\n",
    "    train_tensor = TensorDataset(torch.from_numpy(train_X), torch.from_numpy(train_y))\n",
    "    test_tensor = (torch.from_numpy(test_X), torch.from_numpy(test_y))\n",
    "\n",
    "    train_loader = DataLoader(train_tensor, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return important_features, train_loader, test_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1675101194242,
     "user": {
      "displayName": "Nhat Le",
      "userId": "00702307310725808810"
     },
     "user_tz": -420
    },
    "id": "PW18DMZgF5K_"
   },
   "outputs": [],
   "source": [
    "#Define Pytorch LSTM model\n",
    "class MultiVariateLSTM(nn.Module):\n",
    "    def __init__(self, n_feature=3, n_layers=2, hidden_size=50):\n",
    "        super(MultiVariateLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=n_feature, hidden_size=hidden_size, num_layers=n_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, args.n_predicted_month)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.sigma = nn.Parameter(torch.ones(args.n_predicted_month))\n",
    "    \n",
    "    def forward(self, X_batch, y_batch=None):\n",
    "        output, (last_hidden, _) = self.lstm(X_batch)\n",
    "        last_hidden_vector = output[:, -1, :]\n",
    "        y_predicted = self.linear(last_hidden_vector)\n",
    "        if y_batch != None:\n",
    "            assert y_predicted.size() == y_batch.size()\n",
    "            loss = self.loss_fn(y_predicted, y_batch)\n",
    "            loss = 0.5 * loss / self.sigma**2\n",
    "            loss = loss.sum() + torch.log(1 + self.sigma.prod())\n",
    "            return y_predicted, loss\n",
    "            #return y_predicted, self.loss_fn(y_predicted, y_batch)\n",
    "        else:\n",
    "            return y_predicted\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = torch.tensor(X, device=args.device)\n",
    "        return self.forward(X)\n",
    "\n",
    "#Define Pytorch LSTM-ATT model\n",
    "class MultiVariateLSTM_Attention(nn.Module):\n",
    "    def __init__(self, n_feature=3, n_layers=2, hidden_size=50):\n",
    "        super(MultiVariateLSTM_Attention, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=n_feature, hidden_size=hidden_size, num_layers=n_layers, batch_first=True)\n",
    "        self.attention_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        # self.linear = nn.Linear(hidden_size*2, args.n_predicted_month)\n",
    "        self.linear = nn.Linear(hidden_size, args.n_predicted_month)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.sigma = nn.Parameter(torch.ones(args.n_predicted_month))\n",
    "    \n",
    "    def forward(self, X_batch, y_batch=None):\n",
    "        output, (last_hidden, _) = self.lstm(X_batch)\n",
    "        last_hidden_vector = last_hidden[-1]\n",
    "        remain_hidden_vector = output\n",
    "        e_t = remain_hidden_vector.bmm(self.attention_linear(last_hidden_vector).unsqueeze(2)).squeeze(-1)\n",
    "        alpha_t = F.softmax(e_t, dim=1)\n",
    "        attenion_vector = remain_hidden_vector.transpose(2, 1).bmm(alpha_t.unsqueeze(2)).squeeze(-1)\n",
    "        # combine_vector = torch.cat((last_hidden_vector, attenion_vector), dim=1)\n",
    "        # combine_vector = last_hidden_vector + attenion_vector\n",
    "        y_predicted = self.linear(attenion_vector)\n",
    "        if y_batch != None:\n",
    "            assert y_predicted.size() == y_batch.size()\n",
    "            loss = self.loss_fn(y_predicted, y_batch)\n",
    "            loss = 0.5 * loss / self.sigma**2\n",
    "            loss = loss.sum() + torch.log(1 + self.sigma.prod())\n",
    "            return y_predicted, loss\n",
    "            # return y_predicted, self.loss_fn(y_predicted, y_batch)\n",
    "        else:\n",
    "            return y_predicted\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = torch.tensor(X, device=args.device)\n",
    "        return self.forward(X)\n",
    "\n",
    "# Define Pytorch CNN model\n",
    "class MultivariateCNN(nn.Module):\n",
    "    def __init__(self, num_filters=[100, 100, 100], dropout=0.01):\n",
    "        super(MultivariateCNN, self).__init__()\n",
    "        self.loss_fn = loss = nn.MSELoss()\n",
    "        self.filter_sizes = [1, 2, 3]\n",
    "        self.conv1d_list = nn.ModuleList([nn.Conv1d(args.n_features, num_filters[i], self.filter_sizes[i]) for i in range(len(self.filter_sizes))])\n",
    "        self.linear = nn.Linear(np.sum(num_filters), args.n_predicted_month)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.sigma = nn.Parameter(torch.ones(args.n_predicted_month))\n",
    "\n",
    "    def forward(self, X_batch, y_batch=None):\n",
    "        X_batch = X_batch.permute(0, 2, 1)  #(batch_size, n_features, n_look_back)\n",
    "        X_conv_list = [F.relu(conv1d(X_batch)) for conv1d in self.conv1d_list]\n",
    "        X_pool_list = [F.max_pool1d(x_conv, kernel_size=x_conv.shape[2]) for x_conv in X_conv_list]\n",
    "        X_fc = torch.cat([x_pool.squeeze(dim=2) for x_pool in X_pool_list], dim=1)\n",
    "        y_predicted = self.linear(self.dropout(X_fc))\n",
    "        if y_batch != None:\n",
    "            assert y_predicted.size() == y_batch.size()\n",
    "            loss = self.loss_fn(y_predicted, y_batch)\n",
    "            loss = 0.5 * loss / self.sigma**2\n",
    "            loss = loss.sum() + torch.log(1 + self.sigma.prod())\n",
    "            return y_predicted, loss\n",
    "            # return y_predicted, self.loss_fn(y_predicted, y_batch)\n",
    "        else:\n",
    "            return y_predicted\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = torch.tensor(X, device=args.device)\n",
    "        return self.forward(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zs3TB2E72MDb"
   },
   "source": [
    "# Define Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1675101194243,
     "user": {
      "displayName": "Nhat Le",
      "userId": "00702307310725808810"
     },
     "user_tz": -420
    },
    "id": "ZvmfYUt4vWeP"
   },
   "outputs": [],
   "source": [
    "# Define Pytorch Transformer model\n",
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, d_model=3, n_feature=3, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        pe = torch.zeros(n_feature, d_model)\n",
    "        for pos in range(n_feature):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = math.sin(pos/(10000**(2*i/d_model)))\n",
    "                if i + 1 < d_model:\n",
    "                    pe[pos, i+1] = math.cos(pos/(10000**((2*i+1)/d_model)))\n",
    "        pe = pe.unsqueeze(0)        \n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x*math.sqrt(self.d_model)\n",
    "        length = x.size(1)\n",
    "        pe = Variable(self.pe[:, :length], requires_grad=False)\n",
    "        if x.is_cuda:\n",
    "            pe.cuda()\n",
    "        x = x + pe\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, d_input=3, n_head=3, hidden_size=256, n_layers=3, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.pe = PositionalEncoder(dropout=dropout)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=d_input, nhead=n_head, dim_feedforward=hidden_size, dropout=dropout, activation='gelu')\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, n_layers)\n",
    "        self.decoder = nn.Linear(d_input*n_head, args.n_predicted_month)\n",
    "        self.sigma = nn.Parameter(torch.ones(args.n_predicted_month))\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "    \n",
    "    def forward(self, X_batch, y_batch=None):\n",
    "        X_batch = self.pe(X_batch)\n",
    "        X_batch = self.transformer_encoder(X_batch)\n",
    "        X_batch = X_batch.view(X_batch.size(0), -1)\n",
    "        \n",
    "        y_predicted = self.decoder(X_batch)\n",
    "        if y_batch != None:\n",
    "            assert y_predicted.size() == y_batch.size()\n",
    "            loss = self.loss_fn(y_predicted, y_batch)\n",
    "            loss = 0.5 * loss / self.sigma**2\n",
    "            loss = loss.sum() + torch.log(1 + self.sigma.prod())\n",
    "            return y_predicted, loss\n",
    "        else:\n",
    "            return y_predicted\n",
    "        return X_batch\n",
    "    \n",
    "    def predict(self, X):\n",
    "        with torch.no_grad():\n",
    "            X = torch.tensor(X, device=args.device)\n",
    "        return self.forward(X).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oaAY5LAp2Htc"
   },
   "source": [
    "# Class Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1675101194243,
     "user": {
      "displayName": "Nhat Le",
      "userId": "00702307310725808810"
     },
     "user_tz": -420
    },
    "id": "8TvH93W9mNWQ"
   },
   "outputs": [],
   "source": [
    "# Create class to train and evaluate models\n",
    "class Trainer():\n",
    "    def __init__(self, city, model_type, learning_rate, important_features, train_loader, test_tensor, n_layers=2, hidden_size=128, num_filters=[100, 100, 100], dropout=0.01):\n",
    "        \"\"\"\n",
    "        Initialise trainer, allowing input of LSTM, LSTM-ATT, or CNN \n",
    "        hyperparameters. Adam optimiser used for all models.\n",
    "        \"\"\"\n",
    "        self.model_type = model_type    \n",
    "        self.city = city    \n",
    "        self.Model = self.init_model(model_type, n_layers, hidden_size, num_filters, dropout, city)\n",
    "        self.Model.double().to(args.device)\n",
    "        self.optimizer = torch.optim.Adam(self.Model.parameters(), lr=learning_rate)\n",
    "        self.important_features, self.train_loader, self.test_tensor = important_features, train_loader, test_tensor\n",
    "    \n",
    "    def init_model(self, model_type, n_layers, hidden_size, num_filters, dropout, city):\n",
    "        \"\"\"Initialise a model based on whether LSTM, LSTM-ATT, CNN or Transformer is chosen.\"\"\"\n",
    "        model = TransformerModel()\n",
    "        if model_type.lower() == 'lstm':\n",
    "            model = MultiVariateLSTM(args.n_features, n_layers, hidden_size)\n",
    "        elif model_type.lower() == 'lstm_attention':\n",
    "            model = MultiVariateLSTM_Attention(args.n_features, n_layers, hidden_size)\n",
    "        elif model_type.lower() == 'cnn':\n",
    "            model = MultivariateCNN(num_filters, dropout)\n",
    "        elif model_type.lower() == 'transformers':\n",
    "            model = TransformerModel(d_input=args.look_back, n_head=3, hidden_size=hidden_size, n_layers=n_layers, dropout=dropout)\n",
    "        return model\n",
    "\n",
    "    def step(self, batch):\n",
    "        self.Model.train()\n",
    "        X_batch, y_batch = tuple(t.to(args.device) for t in batch)\n",
    "        self.optimizer.zero_grad()\n",
    "        y_pred, loss = self.Model.forward(X_batch, y_batch)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.mean().item()\n",
    "\n",
    "    def validation(self):\n",
    "        self.Model.eval()\n",
    "        eval_loss = 0.0\n",
    "\n",
    "        result = {}\n",
    "\n",
    "        y_true = np.array([])\n",
    "        y_pred = np.array([])\n",
    "\n",
    "        X_batch, y_batch = tuple(t.to(args.device) for t in self.test_tensor)\n",
    "        with torch.no_grad():\n",
    "            outputs, loss = self.Model.forward(X_batch, y_batch)\n",
    "            eval_loss = loss.mean().item()\n",
    "\n",
    "        return eval_loss\n",
    "\n",
    "    def train(self, epochs=20):\n",
    "        best_lost = float(\"inf\")\n",
    "        best_model = None\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0.0\n",
    "            for batch in self.train_loader:\n",
    "                loss = self.step(batch)\n",
    "                total_loss += loss\n",
    "            train_loss = total_loss/len(self.train_loader)\n",
    "            eval_loss = self.validation()\n",
    "            if eval_loss < best_lost:\n",
    "                best_lost = eval_loss\n",
    "                best_model = copy.deepcopy(self.Model)\n",
    "            if (epoch + 1) == epochs or (epoch + 1) in [c + 1 for c in range(epochs) if c % int(epochs/4) == 0]:\n",
    "                print(f\"Epoch: {epoch:2}/{epochs:2} - train_loss: {train_loss:.4f} - test_loss: {eval_loss:4f}\")\n",
    "        self.Model = best_model\n",
    "        self.Model.eval()\n",
    "        return None\n",
    "\n",
    "    # Lưu model vào trong thư mục models\n",
    "    def save_model_to(self, path = '', city =''):       \n",
    "        torch.save(self.Model, path)\n",
    "\n",
    "    def load_model_to(self, path = ''):       \n",
    "        return torch.load(path)\n",
    "\n",
    "    def evaluate_model(self, np_data=None, plot=True, scaled=True, city=None, k_steps=None, y_scaler =None):\n",
    "        assert scaled, \"data must be scaled\"\n",
    "        self.Model.eval()\n",
    "        tensor_data = torch.from_numpy(np_data)\n",
    "        rmse_list = []\n",
    "        mae_list = [] \n",
    "        mape_list = []\n",
    "\n",
    "        y_predicted_list = []\n",
    "        y_true_list = []\n",
    "\n",
    "        for k_steps in range(1, args.n_predicted_month + 1):\n",
    "            y_predicted = []\n",
    "            y_true = []\n",
    "            for index in range(tensor_data.size(0) - args.look_back):\n",
    "                X = tensor_data[index: index + args.look_back, self.important_features]\n",
    "                # yhat = self.Model.predict(X.unsqueeze(0)).squeeze()\n",
    "\n",
    "                yhat = self.Model.predict(X.unsqueeze(0))\n",
    "                yhat = yhat.squeeze()\n",
    "\n",
    "                y_predicted.append(yhat.detach().cpu().numpy()[k_steps - 1])\n",
    "                y_true.append(tensor_data[index + args.look_back, -1].detach().cpu().numpy())\n",
    "\n",
    "            y_predicted = y_scaler.inverse_transform(np.array(y_predicted).reshape(-1, 1)).reshape(-1, )\n",
    "            y_true = y_scaler.inverse_transform(np.array(y_true).reshape(-1, 1)).reshape(-1, )\n",
    "\n",
    "            rmse = mean_squared_error(y_true, y_predicted, squared=False)\n",
    "            mae = mean_absolute_error(y_true, y_predicted)\n",
    "            mape = mean_absolute_percentage_error(y_true, y_predicted)\n",
    "\n",
    "            rmse_list.append(rmse)\n",
    "            mae_list.append(mae)\n",
    "            mape_list.append(mape)\n",
    "\n",
    "            # print('City: '+self.city+'  _algo:'+self.model_type+'  -RMSE: '+str(rmse))\n",
    "            if plot==True:\n",
    "              plt.grid(True)\n",
    "              plt.plot(y_predicted, label='predicted')\n",
    "              plt.plot(y_true, label='actual')\n",
    "              plt.title(f\"k-steps = {k_steps} - city: \"+self.city+'  _algo:'+self.model_type+'  -RMSE: '+str(rmse))\n",
    "              plt.legend()\n",
    "              plt.show()\n",
    "\n",
    "              plt.show()\n",
    "            y_predicted_list.append(y_predicted)\n",
    "            y_true_list.append(y_true)\n",
    "\n",
    "        return y_true_list, y_predicted_list, rmse_list, mae_list, mape_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6gOIdsGo19Oc"
   },
   "source": [
    "# Objective Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1675101194243,
     "user": {
      "displayName": "Nhat Le",
      "userId": "00702307310725808810"
     },
     "user_tz": -420
    },
    "id": "8px_TDW2mtRt"
   },
   "outputs": [],
   "source": [
    "def objective(trial, city):\n",
    "    # Define search parameters\n",
    "    n_layers = trial.suggest_int('n layers', 3, 6) # a    \n",
    "    hidden_size = trial.suggest_int('Hidden size', 5, 384, log=True) #b\n",
    "    learning_rate = trial.suggest_loguniform('Learning rate', 1e-4, 1e-2)\n",
    "    dropout = trial.suggest_uniform('Dropout rate', 0.01, 0.80)\n",
    "    args.epochs = trial.suggest_int('Epochs', 100, 500, step=10)\n",
    "    lookback_window = 3 # fix cứng optimize sau\n",
    "\n",
    "    # Pre-process data\n",
    "    specific_data = get_city_data(fix_text(city))\n",
    "    specific_data = impute_missing_value(specific_data)\n",
    "    specific_data = convert_to_stationary(specific_data)\n",
    "    specific_data.dropna(inplace=True)\n",
    "\n",
    "    train, test = split_data(specific_data,lookback_window)\n",
    "\n",
    "    # Fit data scaler to training data\n",
    "    full_scaler = MinMaxScaler().fit(train)\n",
    "    y_scaler = MinMaxScaler().fit(train.values[:, -1].reshape(-1, 1))\n",
    "\n",
    "    # Scale train and test data\n",
    "    train = full_scaler.transform(train)\n",
    "    test = full_scaler.transform(test)\n",
    "\n",
    "    # Get data to run model\n",
    "    important_features, train_loader, test_tensor = get_data(train, test, args.batch_size, specific_data)\n",
    "\n",
    "    # Transformer model\n",
    "    trainer = Trainer(model_type='transformer',\n",
    "                  city = city,\n",
    "                  important_features=important_features,\n",
    "                  train_loader=train_loader,\n",
    "                  test_tensor=test_tensor,\n",
    "                  n_layers=n_layers,\n",
    "                  hidden_size=hidden_size,\n",
    "                  learning_rate=learning_rate,\n",
    "                  dropout=dropout)\n",
    "\n",
    "    # Train model\n",
    "    # trainer.train(epochs=args.epochs, trial=trial)\n",
    "    trainer.train(epochs=args.epochs)\n",
    "\n",
    "    # Evaluate model\n",
    "    y_true, y_pred, rmse_list, mae_list, mape_list = trainer.evaluate_model(np_data=test, plot= False, scaled=True, city=city, y_scaler = y_scaler)\n",
    "    # _, _, rmse, mae, = trainer.evaluate_model(np_data=test, plot=False, scaled=True, city=city, y_scaler=y_scaler)\n",
    "\n",
    "    return mae_list[0]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHbv5lQ7dn3S"
   },
   "source": [
    "# Main Cell For Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ggBbhETfXbsk",
    "outputId": "743bc695-4dd4-4b98-e665-df86250c4a04"
   },
   "outputs": [],
   "source": [
    "#########################\n",
    "# Main Cell for optimize\n",
    "#########################\n",
    "dt_started = datetime.now()\n",
    "\n",
    "##################################################\n",
    "# Input param for Optimize Run\n",
    "ntry = 50\n",
    "njob = -1\n",
    "\n",
    "# Lưu thông tin traceback study và error city trong quá trình optimize\n",
    "l_study_city ={}\n",
    "# l_errCity =[]\n",
    "\n",
    "if __name__ == '__main__':  \n",
    "  best_param = pd.DataFrame()\n",
    "  l_studies = {}\n",
    "  l_errCity =[]\n",
    "\n",
    "  for city in cities:\n",
    "    # Use Tree-structured Parzen Estimator sampler to minimise RMSE\n",
    "    sampler = optuna.samplers.TPESampler()\n",
    "    study = optuna.create_study(sampler=sampler, direction='minimize')\n",
    "\n",
    "    # truyền multiple param vào trong biến trial\n",
    "    obj_func = lambda trial: objective(trial, city)\n",
    "\n",
    "    try:\n",
    "      # Optimise over 100 trials\n",
    "      study.optimize(obj_func, n_trials=ntry, n_jobs=njob)\n",
    "\n",
    "      # Print results\n",
    "      print(\"Study statistics for : \")\n",
    "      print(\"  Number of finished trials: \", len(study.trials))\n",
    "      \n",
    "      print(\"Best trial of city: \",city)\n",
    "      best_trial = study.best_trial\n",
    "      print(\"  Value: \", best_trial.value)   \n",
    "\n",
    "      # lưu best param vào trong biến toàn cục\n",
    "      one_city_param = pd.DataFrame({\n",
    "                              'City': city,\n",
    "                              'Alg_name': 'transformer',\n",
    "                              'Best_value': best_trial.value,\n",
    "                              'n_try_opt': ntry,\n",
    "                              'n Feature': 3, # set cứng\n",
    "                              'Batch Size': 16, # set cứng\n",
    "                              'Lookback Window': 3, # set cứng\n",
    "                              'Epochs': best_trial.params['Epochs'],\n",
    "                              'Hidden Size': best_trial.params['Hidden size'],\n",
    "                              'n Layers': best_trial.params['n layers'],\n",
    "                              'Learning rate': best_trial.params['Learning rate'], \n",
    "                              'Num. filters': '', # Transformer không dùng\n",
    "                              'Dropout rate': best_trial.params['Dropout rate']}, index=[0])\n",
    "      one_city_param.to_excel(prj_path_opt+'/tf/diarrhoea_opt_hyperparam_transformer_'+city+'.xlsx')\n",
    "      best_param = best_param.append(one_city_param)\n",
    "    except:# có error thì lưu vào l_errCity để check lại sau \n",
    "      l_errCity.append(city)\n",
    "    # Plot result\n",
    "    l_studies[city] = study # thêm vào danh sách sài sau\n",
    "    print('optimize result of city: '+ city)\n",
    "    # optuna.visualization.plot_optimization_history(study)\n",
    "    # optuna.visualization.plot_param_importances(study)\n",
    "    # optuna.visualization.plot_slice(study)\n",
    "  \n",
    "  best_param.to_excel(prj_path_opt+'/tf/diarrhoea_opt_hyperparam_transformer.xlsx')\n",
    "\n",
    "dt_ended = datetime.now()\n",
    "print('kết thúc study trong:', round((dt_ended - dt_started).total_seconds()/60))\n",
    "# print(l_errCity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RbjNytCjdHLl"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def send_to_telegram(message):\n",
    "\n",
    "    apiToken = '5908735099:AAGVSLrW62aXPBP-GrMvxoVgMsuJxXJpP1Q'\n",
    "    chatID = '@ptn_announcement'\n",
    "    apiURL = f'https://api.telegram.org/bot{apiToken}/sendMessage'\n",
    "\n",
    "    try:\n",
    "        response = requests.post(apiURL, json={'chat_id': chatID, 'text': message})\n",
    "        print(response.text)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "send_to_telegram(\"Server Chạy Xong TF\" )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "24f3299fa11d098f3e36f80146bbe61fdadb7bfe8872ee0c4a379787469f5f10"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
