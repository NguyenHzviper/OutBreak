{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "import optuna\n",
    "import joblib\n",
    "\n",
    "import torch\n",
    "\n",
    "from darts import TimeSeries\n",
    "from darts.models import  RandomForest, LinearRegressionModel, LightGBMModel, \\\n",
    "                        CatBoostModel, XGBModel,  BlockRNNModel, NBEATSModel, NHiTSModel, \\\n",
    "                        TCNModel, TFTModel\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import PoissonRegressor\n",
    "\n",
    "\n",
    "from darts.utils.likelihood_models import GaussianLikelihood\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "prj_path = '../'\n",
    "data_path = prj_path + \"data/new_data/DH/squeezed/\"\n",
    "prj_path_opt= prj_path + \"optimize_hyperparam/opt_results/\"\n",
    "output_process = prj_path + \"data/new_data/DH/processed_data/\"\n",
    "output_featureselection = prj_path + \"data/new_data/DH/feature_selection/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cities = [\n",
    "        'An Giang', 'BR Vũng Tàu', 'Bình Phước', 'Bình Thuận', 'Bình Định',\n",
    "        'Bạc Liêu', 'Bắc Kạn', 'Bắc Giang', 'Cao Bằng', 'Cà Mau',\n",
    "        'Cần Thơ', 'Gia Lai', 'Hà Giang', 'Hà Nội', 'Hà Tĩnh',\n",
    "        'Hòa Bình','Hưng Yên', 'Hải Dương', 'Hải Phòng', 'Khánh Hòa', 'Kiên Giang',\n",
    "        'Kon Tum', 'Lai Châu', 'Long An', 'Lào Cai', 'Lâm Đồng',\n",
    "        'Lạng Sơn','Nam Định', 'Nghệ An', 'Ninh Bình', 'Ninh Thuận',\n",
    "        'Phú Thọ', 'Phú Yên', 'Quảng Bình', 'Quảng Nam', 'Quảng Ngãi',\n",
    "        'Quảng Ninh', 'Quảng Trị', 'Sóc Trăng', 'Sơn La', 'TT Huế',\n",
    "        'Thanh Hóa', 'Thái Bình', 'Thái Nguyên', 'Tiền Giang', 'Trà Vinh',\n",
    "        'Tuyên Quang', 'Tây Ninh', 'Vĩnh Phúc', 'Yên Bái', 'Điện Biên',\n",
    "        'Đà Nẵng', 'Đắk Nông', 'Đắk Lắk', 'Đồng Tháp'\n",
    "]\n",
    "cities = ['An Giang','BR Vũng Tàu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters as args using the Configuration class\n",
    "class Configuration():\n",
    "    def __init__(self):\n",
    "        # lấy bộ test dài 36 tháng = 3 năm\n",
    "        self.test_size = 36\n",
    "        # là nhìn vào dữ liệu trước 3 tháng và dự phóng        \n",
    "        self.look_back = 3\n",
    "        # dự phóng n-step trong 6 tháng\n",
    "        self.n_predicted_period_months = 6\n",
    "        self.n_features = 3\n",
    "        self.seed = 42\n",
    "        # mỗi phần tử x trong tập suppervise có độ lớn là 16 = 16 tháng\n",
    "        self.batch_size = 16\n",
    "        self.device = torch.device(\"cuda\")\n",
    "        self.epochs = 300\n",
    "        #others\n",
    "        self.labels = \"Dengue_fever_rates\"\n",
    "        # Input param for Optimize Run\n",
    "        self.ntry = 1\n",
    "        self.njob = 1\n",
    "\n",
    "args = Configuration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    import random\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    \n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "seed_everything(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supporting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def send_to_telegram(message):\n",
    "\n",
    "    apiToken = '5908735099:AAGVSLrW62aXPBP-GrMvxoVgMsuJxXJpP1Q'\n",
    "    chatID = '@ptn_announcement'\n",
    "    apiURL = f'https://api.telegram.org/bot{apiToken}/sendMessage'\n",
    "\n",
    "    try:\n",
    "        response = requests.post(apiURL, json={'chat_id': chatID, 'text': message})\n",
    "        print(response.text)\n",
    "    except Exception as e:\n",
    "        message_error = \"Bị lỗi rùi: \"+str(e)\n",
    "        response = requests.post(apiURL, json={'chat_id': chatID, 'text': message_error})\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict_all_city_data():\n",
    "  cities_data = {}  \n",
    "  for city in cities:\n",
    "    city_result = pd.read_excel(prj_path+'data/new_data/DH/squeezed/squeezed_'+city+'.xlsx')  \n",
    "    \"\"\"Get all data from all city in 1997 - 2016\"\"\" \n",
    "    city_result = city_result.loc[city_result['year_month'] < '2017-1-1'] \n",
    "    cities_data[city] = city_result\n",
    "  return cities_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data (pre-)processing functions\n",
    "# modification\n",
    "def get_city_data(city_name, dict_full_data):\n",
    "    \"\"\"Returns Diarrhoea rate and climate data\"\"\" \n",
    "    city_data = dict_full_data[city_name].drop(columns=['Diarrhoea_cases','Diarrhoea_rates', 'province',\n",
    "                                                        'Influenza_rates','Influenza_cases',\n",
    "                                                        'Dengue_fever_cases', 'year', 'month'], \n",
    "                                                                  axis=1, \n",
    "                                                                  inplace=False)    \n",
    "    return city_data\n",
    "\n",
    "def convert_to_stationary(city_data):\n",
    "    \"\"\"Subtracts previous value for all cols except disease rates\"\"\"\n",
    "    for col_name in city_data.columns:\n",
    "        if col_name != 'Dengue_fever_rates':\n",
    "            try:\n",
    "                city_data[col_name] = city_data[col_name] - city_data[col_name].shift()\n",
    "            except:\n",
    "                print(col_name)\n",
    "    return city_data\n",
    "\n",
    "def impute_missing_value(city_data):\n",
    "    \"\"\"\n",
    "    Imputes 0 for first 12 months, \n",
    "    last year's value for months 12-24, \n",
    "    and minimum value of last two years for months 25+\n",
    "    \"\"\"\n",
    "    for col in city_data.columns:\n",
    "        for index in range(len(city_data[col])):\n",
    "            if np.isnan(city_data[col].iloc[index]):\n",
    "                if index < 12:\n",
    "                    city_data[col].iloc[index] = 0\n",
    "                elif index >= 12 and index <= 24:\n",
    "                    city_data[col].iloc[index] = city_data[col].iloc[index - 12]\n",
    "                else:\n",
    "                    city_data[col].iloc[index] = min(city_data[col].iloc[index - 12], city_data[col].iloc[index - 24])\n",
    "    return city_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_full_data(dict_full_data):\n",
    "    climate_and_disease_feats = ['Total_Evaporation',\n",
    "       'Total_Rainfall', 'Max_Daily_Rainfall', 'n_raining_days',\n",
    "       'Average_temperature', 'Max_Average_Temperature',\n",
    "       'Min_Average_Temperature', 'Max_Absolute_Temperature',\n",
    "       'Min_Absolute_Temperature', 'Average_Humidity', 'Min_Humidity',\n",
    "       'n_hours_sunshine', 'Dengue_fever_rates']\n",
    "    for city in cities:\n",
    "        city_data = get_city_data(city_name=city,dict_full_data = dict_full_data)\n",
    "        city_data_features = city_data[climate_and_disease_feats]\n",
    "        city_data_features = impute_missing_value(city_data_features)\n",
    "        city_data_features = convert_to_stationary(city_data_features)\n",
    "        city_data_features.dropna(inplace=True)\n",
    "        city_data_features.loc[:, \"year_month\"] = city_data[\"year_month\"]\n",
    "        dict_full_data[city] = city_data_features\n",
    "    return dict_full_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, look_back, n_nextstep = args.n_predicted_period_months):\n",
    "    \"\"\"Splits data into train and test sets based on args (Configuration class)\"\"\"\n",
    "    train = data[: -args.test_size]    \n",
    "    test = data[-args.test_size - look_back-(n_nextstep - 1): ]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_supervised(data,  d_out, d_in, features_list=[]):\n",
    "    \"\"\"\n",
    "    Frames time-series as supervised learning dataset.\n",
    "    \n",
    "    Args:\n",
    "      d_in: lookback window\n",
    "      d_out: number of predicted months\n",
    "      features_list: list of all features **where last col is the disease incidence**\n",
    "\n",
    "    Returns:\n",
    "      Numpy arrays of disease incidence (y) and other predictors (X)\n",
    "    \"\"\"\n",
    "    X, y = list(), list()\n",
    "    for index, _ in enumerate(data):\n",
    "        in_end = index + d_in\n",
    "        out_end = in_end + d_out\n",
    "        if out_end <= len(data):\n",
    "            if len(features_list) == 0 :\n",
    "                X.append(data[index: in_end, :-1])\n",
    "            else:\n",
    "                X.append(data[index: in_end, features_list])\n",
    "            y.append(data[out_end-1: out_end, -1])\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(\n",
    "          random_state = 30,\n",
    "          kernel = 'rbf',\n",
    "          probability=True\n",
    "      )\n",
    "\n",
    "# model = NBEATSModel(\n",
    "#                             input_chunk_length = 3,\n",
    "#                             output_chunk_length = 6)\n",
    "\n",
    "# model = PoissonRegressor(\n",
    "#           max_iter = 20,\n",
    "#       )\n",
    "model_name = type(model).__name__\n",
    "city = \"An Giang\"\n",
    "\n",
    "nstep = 1\n",
    "\n",
    "specific_data = pd.read_csv(output_process+city+'_train_preprocessed.csv', parse_dates=True, index_col= None, encoding = 'unicode_escape')\n",
    "scaler = joblib.load(output_process+city+'_train_scalerMinMaxNorm.save') #ok\n",
    "df_train, df_valid = split_data(specific_data, args.look_back,nstep)\n",
    "\n",
    "# train_X, train_y = to_supervised(df_train, d_out=nstep, d_in=args.look_back )\n",
    "# eval_X, eval_y = to_supervised(df_valid, d_out=nstep, d_in=args.look_back )\n",
    "# model.fit(\n",
    "#         train_X,\n",
    "#         train_y\n",
    "#     )\n",
    "# prediction = model.predict(eval_X)\n",
    "# print(prediction)\n",
    "# if model_name in [\"PoissonRegressor\",\"SVC\"]:\n",
    "#     print(model_name)\n",
    "# else:\n",
    "#     print(\"HOi nè\")\n",
    "#     print(model_name)\n",
    "# PoissonRegressor\n",
    "# SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total_Evaporation</th>\n",
       "      <th>Total_Rainfall</th>\n",
       "      <th>Max_Daily_Rainfall</th>\n",
       "      <th>n_raining_days</th>\n",
       "      <th>Average_temperature</th>\n",
       "      <th>Max_Average_Temperature</th>\n",
       "      <th>Min_Average_Temperature</th>\n",
       "      <th>Max_Absolute_Temperature</th>\n",
       "      <th>Min_Absolute_Temperature</th>\n",
       "      <th>Average_Humidity</th>\n",
       "      <th>Min_Humidity</th>\n",
       "      <th>n_hours_sunshine</th>\n",
       "      <th>Dengue_fever_rates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.367241</td>\n",
       "      <td>0.466281</td>\n",
       "      <td>0.428136</td>\n",
       "      <td>0.658537</td>\n",
       "      <td>0.838710</td>\n",
       "      <td>0.582090</td>\n",
       "      <td>0.780822</td>\n",
       "      <td>0.5500</td>\n",
       "      <td>0.883495</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.2030</td>\n",
       "      <td>0.519037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.569540</td>\n",
       "      <td>0.462807</td>\n",
       "      <td>0.418119</td>\n",
       "      <td>0.439024</td>\n",
       "      <td>0.612903</td>\n",
       "      <td>0.552239</td>\n",
       "      <td>0.479452</td>\n",
       "      <td>0.7125</td>\n",
       "      <td>0.533981</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.9885</td>\n",
       "      <td>0.514028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.520115</td>\n",
       "      <td>0.496458</td>\n",
       "      <td>0.500436</td>\n",
       "      <td>0.707317</td>\n",
       "      <td>0.790323</td>\n",
       "      <td>0.626866</td>\n",
       "      <td>0.698630</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>0.786408</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.236842</td>\n",
       "      <td>0.2800</td>\n",
       "      <td>0.553026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.186207</td>\n",
       "      <td>0.491621</td>\n",
       "      <td>0.499565</td>\n",
       "      <td>0.658537</td>\n",
       "      <td>0.629032</td>\n",
       "      <td>0.268657</td>\n",
       "      <td>0.698630</td>\n",
       "      <td>0.3875</td>\n",
       "      <td>0.679612</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.5210</td>\n",
       "      <td>0.538357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.601149</td>\n",
       "      <td>0.435082</td>\n",
       "      <td>0.320122</td>\n",
       "      <td>0.634146</td>\n",
       "      <td>0.516129</td>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.493151</td>\n",
       "      <td>0.4250</td>\n",
       "      <td>0.611650</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.6420</td>\n",
       "      <td>0.583438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>0.494253</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.401568</td>\n",
       "      <td>0.560976</td>\n",
       "      <td>0.596774</td>\n",
       "      <td>0.447761</td>\n",
       "      <td>0.589041</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.689320</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.6950</td>\n",
       "      <td>0.542559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>0.275862</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.493031</td>\n",
       "      <td>0.560976</td>\n",
       "      <td>0.532258</td>\n",
       "      <td>0.223881</td>\n",
       "      <td>0.493151</td>\n",
       "      <td>0.5500</td>\n",
       "      <td>0.611650</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>0.4400</td>\n",
       "      <td>0.537881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0.465517</td>\n",
       "      <td>0.474796</td>\n",
       "      <td>0.475610</td>\n",
       "      <td>0.560976</td>\n",
       "      <td>0.548387</td>\n",
       "      <td>0.388060</td>\n",
       "      <td>0.479452</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>0.524272</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.394737</td>\n",
       "      <td>0.8300</td>\n",
       "      <td>0.534874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.354905</td>\n",
       "      <td>0.244774</td>\n",
       "      <td>0.560976</td>\n",
       "      <td>0.612903</td>\n",
       "      <td>0.388060</td>\n",
       "      <td>0.534247</td>\n",
       "      <td>0.4125</td>\n",
       "      <td>0.660194</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.394737</td>\n",
       "      <td>0.6500</td>\n",
       "      <td>0.560604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>0.574713</td>\n",
       "      <td>0.453678</td>\n",
       "      <td>0.523519</td>\n",
       "      <td>0.560976</td>\n",
       "      <td>0.161290</td>\n",
       "      <td>0.029851</td>\n",
       "      <td>0.150685</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.165048</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.552632</td>\n",
       "      <td>0.6100</td>\n",
       "      <td>0.531532</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>203 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Total_Evaporation  Total_Rainfall  Max_Daily_Rainfall  n_raining_days  \\\n",
       "0             0.367241        0.466281            0.428136        0.658537   \n",
       "1             0.569540        0.462807            0.418119        0.439024   \n",
       "2             0.520115        0.496458            0.500436        0.707317   \n",
       "3             0.186207        0.491621            0.499565        0.658537   \n",
       "4             0.601149        0.435082            0.320122        0.634146   \n",
       "..                 ...             ...                 ...             ...   \n",
       "198           0.494253        1.000000            0.401568        0.560976   \n",
       "199           0.275862        0.000000            0.493031        0.560976   \n",
       "200           0.465517        0.474796            0.475610        0.560976   \n",
       "201           0.500000        0.354905            0.244774        0.560976   \n",
       "202           0.574713        0.453678            0.523519        0.560976   \n",
       "\n",
       "     Average_temperature  Max_Average_Temperature  Min_Average_Temperature  \\\n",
       "0               0.838710                 0.582090                 0.780822   \n",
       "1               0.612903                 0.552239                 0.479452   \n",
       "2               0.790323                 0.626866                 0.698630   \n",
       "3               0.629032                 0.268657                 0.698630   \n",
       "4               0.516129                 0.313433                 0.493151   \n",
       "..                   ...                      ...                      ...   \n",
       "198             0.596774                 0.447761                 0.589041   \n",
       "199             0.532258                 0.223881                 0.493151   \n",
       "200             0.548387                 0.388060                 0.479452   \n",
       "201             0.612903                 0.388060                 0.534247   \n",
       "202             0.161290                 0.029851                 0.150685   \n",
       "\n",
       "     Max_Absolute_Temperature  Min_Absolute_Temperature  Average_Humidity  \\\n",
       "0                      0.5500                  0.883495              0.45   \n",
       "1                      0.7125                  0.533981              0.25   \n",
       "2                      0.6875                  0.786408              0.45   \n",
       "3                      0.3875                  0.679612              0.50   \n",
       "4                      0.4250                  0.611650              0.45   \n",
       "..                        ...                       ...               ...   \n",
       "198                    0.3750                  0.689320              0.35   \n",
       "199                    0.5500                  0.611650              0.45   \n",
       "200                    0.3125                  0.524272              0.35   \n",
       "201                    0.4125                  0.660194              0.25   \n",
       "202                    0.4000                  0.165048              0.25   \n",
       "\n",
       "     Min_Humidity  n_hours_sunshine  Dengue_fever_rates  \n",
       "0        0.578947            0.2030            0.519037  \n",
       "1        0.368421            0.9885            0.514028  \n",
       "2        0.236842            0.2800            0.553026  \n",
       "3        0.789474            0.5210            0.538357  \n",
       "4        0.526316            0.6420            0.583438  \n",
       "..            ...               ...                 ...  \n",
       "198      0.578947            0.6950            0.542559  \n",
       "199      0.605263            0.4400            0.537881  \n",
       "200      0.394737            0.8300            0.534874  \n",
       "201      0.394737            0.6500            0.560604  \n",
       "202      0.552632            0.6100            0.531532  \n",
       "\n",
       "[203 rows x 13 columns]"
      ]
     },
     "execution_count": 736,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.3672414, 0.4662807, 0.519037 ],\n",
       "        [0.5695402, 0.4628065, 0.514028 ],\n",
       "        [0.5201149, 0.4964578, 0.5530263]],\n",
       "\n",
       "       [[0.5695402, 0.4628065, 0.514028 ],\n",
       "        [0.5201149, 0.4964578, 0.5530263],\n",
       "        [0.1862069, 0.4916213, 0.5383572]],\n",
       "\n",
       "       [[0.5201149, 0.4964578, 0.5530263],\n",
       "        [0.1862069, 0.4916213, 0.5383572],\n",
       "        [0.6011494, 0.4350817, 0.5834378]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.5114943, 0.4659401, 0.545901 ],\n",
       "        [0.4942529, 1.       , 0.5425595],\n",
       "        [0.2758621, 0.       , 0.5378813]],\n",
       "\n",
       "       [[0.4942529, 1.       , 0.5425595],\n",
       "        [0.2758621, 0.       , 0.5378813],\n",
       "        [0.4655172, 0.4747956, 0.5348739]],\n",
       "\n",
       "       [[0.2758621, 0.       , 0.5378813],\n",
       "        [0.4655172, 0.4747956, 0.5348739],\n",
       "        [0.5      , 0.3549046, 0.5606038]]])"
      ]
     },
     "execution_count": 733,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(output_process+city+'_train_preprocessed.csv', parse_dates=True, index_col= None, encoding = 'unicode_escape')\n",
    "features_list = ['Total_Evaporation', 'Total_Rainfall']\n",
    "columns = features_list + [args.labels]\n",
    "train = df.iloc[:,:-1][columns].to_numpy()\n",
    "train_X, train_y = to_supervised(train, d_out=nstep, d_in=args.look_back )\n",
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_supervised(data,  d_out, d_in, features_list=[]):\n",
    "    \"\"\"\n",
    "    Frames time-series as supervised learning dataset.\n",
    "    \n",
    "    Args:\n",
    "      d_in: lookback window\n",
    "      d_out: number of predicted months\n",
    "      features_list: list of all features **where last col is the disease incidence**\n",
    "\n",
    "    Returns:\n",
    "      Numpy arrays of disease incidence (y) and other predictors (X)\n",
    "    \"\"\"\n",
    "    X, y = list(), list()\n",
    "    for index, _ in enumerate(data):\n",
    "        in_end = index + d_in\n",
    "        out_end = in_end + d_out\n",
    "        if out_end <= len(data):\n",
    "            if len(features_list) == 0 :\n",
    "                X.append(data[index: in_end, :])\n",
    "            else:\n",
    "                # display(data)\n",
    "                print(\"index:\",index)\n",
    "                print(\"in_end:\",in_end)\n",
    "                X.append(data[index: in_end, features_list])\n",
    "                \n",
    "            y.append(data[out_end-1: out_end, -1])\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total_Evaporation</th>\n",
       "      <th>Total_Rainfall</th>\n",
       "      <th>Max_Daily_Rainfall</th>\n",
       "      <th>n_raining_days</th>\n",
       "      <th>Average_temperature</th>\n",
       "      <th>Max_Average_Temperature</th>\n",
       "      <th>Min_Average_Temperature</th>\n",
       "      <th>Max_Absolute_Temperature</th>\n",
       "      <th>Min_Absolute_Temperature</th>\n",
       "      <th>Average_Humidity</th>\n",
       "      <th>Min_Humidity</th>\n",
       "      <th>n_hours_sunshine</th>\n",
       "      <th>Dengue_fever_rates</th>\n",
       "      <th>year_month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.367241</td>\n",
       "      <td>0.466281</td>\n",
       "      <td>0.428136</td>\n",
       "      <td>0.658537</td>\n",
       "      <td>0.838710</td>\n",
       "      <td>0.582090</td>\n",
       "      <td>0.780822</td>\n",
       "      <td>0.5500</td>\n",
       "      <td>0.883495</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.2030</td>\n",
       "      <td>0.519037</td>\n",
       "      <td>1997-02-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.569540</td>\n",
       "      <td>0.462807</td>\n",
       "      <td>0.418119</td>\n",
       "      <td>0.439024</td>\n",
       "      <td>0.612903</td>\n",
       "      <td>0.552239</td>\n",
       "      <td>0.479452</td>\n",
       "      <td>0.7125</td>\n",
       "      <td>0.533981</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.9885</td>\n",
       "      <td>0.514028</td>\n",
       "      <td>1997-03-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.520115</td>\n",
       "      <td>0.496458</td>\n",
       "      <td>0.500436</td>\n",
       "      <td>0.707317</td>\n",
       "      <td>0.790323</td>\n",
       "      <td>0.626866</td>\n",
       "      <td>0.698630</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>0.786408</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.236842</td>\n",
       "      <td>0.2800</td>\n",
       "      <td>0.553026</td>\n",
       "      <td>1997-04-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.186207</td>\n",
       "      <td>0.491621</td>\n",
       "      <td>0.499565</td>\n",
       "      <td>0.658537</td>\n",
       "      <td>0.629032</td>\n",
       "      <td>0.268657</td>\n",
       "      <td>0.698630</td>\n",
       "      <td>0.3875</td>\n",
       "      <td>0.679612</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.5210</td>\n",
       "      <td>0.538357</td>\n",
       "      <td>1997-05-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.601149</td>\n",
       "      <td>0.435082</td>\n",
       "      <td>0.320122</td>\n",
       "      <td>0.634146</td>\n",
       "      <td>0.516129</td>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.493151</td>\n",
       "      <td>0.4250</td>\n",
       "      <td>0.611650</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.6420</td>\n",
       "      <td>0.583438</td>\n",
       "      <td>1997-06-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>0.494253</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.401568</td>\n",
       "      <td>0.560976</td>\n",
       "      <td>0.596774</td>\n",
       "      <td>0.447761</td>\n",
       "      <td>0.589041</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.689320</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.6950</td>\n",
       "      <td>0.542559</td>\n",
       "      <td>2013-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>0.275862</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.493031</td>\n",
       "      <td>0.560976</td>\n",
       "      <td>0.532258</td>\n",
       "      <td>0.223881</td>\n",
       "      <td>0.493151</td>\n",
       "      <td>0.5500</td>\n",
       "      <td>0.611650</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>0.4400</td>\n",
       "      <td>0.537881</td>\n",
       "      <td>2013-09-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0.465517</td>\n",
       "      <td>0.474796</td>\n",
       "      <td>0.475610</td>\n",
       "      <td>0.560976</td>\n",
       "      <td>0.548387</td>\n",
       "      <td>0.388060</td>\n",
       "      <td>0.479452</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>0.524272</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.394737</td>\n",
       "      <td>0.8300</td>\n",
       "      <td>0.534874</td>\n",
       "      <td>2013-10-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.354905</td>\n",
       "      <td>0.244774</td>\n",
       "      <td>0.560976</td>\n",
       "      <td>0.612903</td>\n",
       "      <td>0.388060</td>\n",
       "      <td>0.534247</td>\n",
       "      <td>0.4125</td>\n",
       "      <td>0.660194</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.394737</td>\n",
       "      <td>0.6500</td>\n",
       "      <td>0.560604</td>\n",
       "      <td>2013-11-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>0.574713</td>\n",
       "      <td>0.453678</td>\n",
       "      <td>0.523519</td>\n",
       "      <td>0.560976</td>\n",
       "      <td>0.161290</td>\n",
       "      <td>0.029851</td>\n",
       "      <td>0.150685</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.165048</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.552632</td>\n",
       "      <td>0.6100</td>\n",
       "      <td>0.531532</td>\n",
       "      <td>2013-12-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>203 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Total_Evaporation  Total_Rainfall  Max_Daily_Rainfall  n_raining_days  \\\n",
       "0             0.367241        0.466281            0.428136        0.658537   \n",
       "1             0.569540        0.462807            0.418119        0.439024   \n",
       "2             0.520115        0.496458            0.500436        0.707317   \n",
       "3             0.186207        0.491621            0.499565        0.658537   \n",
       "4             0.601149        0.435082            0.320122        0.634146   \n",
       "..                 ...             ...                 ...             ...   \n",
       "198           0.494253        1.000000            0.401568        0.560976   \n",
       "199           0.275862        0.000000            0.493031        0.560976   \n",
       "200           0.465517        0.474796            0.475610        0.560976   \n",
       "201           0.500000        0.354905            0.244774        0.560976   \n",
       "202           0.574713        0.453678            0.523519        0.560976   \n",
       "\n",
       "     Average_temperature  Max_Average_Temperature  Min_Average_Temperature  \\\n",
       "0               0.838710                 0.582090                 0.780822   \n",
       "1               0.612903                 0.552239                 0.479452   \n",
       "2               0.790323                 0.626866                 0.698630   \n",
       "3               0.629032                 0.268657                 0.698630   \n",
       "4               0.516129                 0.313433                 0.493151   \n",
       "..                   ...                      ...                      ...   \n",
       "198             0.596774                 0.447761                 0.589041   \n",
       "199             0.532258                 0.223881                 0.493151   \n",
       "200             0.548387                 0.388060                 0.479452   \n",
       "201             0.612903                 0.388060                 0.534247   \n",
       "202             0.161290                 0.029851                 0.150685   \n",
       "\n",
       "     Max_Absolute_Temperature  Min_Absolute_Temperature  Average_Humidity  \\\n",
       "0                      0.5500                  0.883495              0.45   \n",
       "1                      0.7125                  0.533981              0.25   \n",
       "2                      0.6875                  0.786408              0.45   \n",
       "3                      0.3875                  0.679612              0.50   \n",
       "4                      0.4250                  0.611650              0.45   \n",
       "..                        ...                       ...               ...   \n",
       "198                    0.3750                  0.689320              0.35   \n",
       "199                    0.5500                  0.611650              0.45   \n",
       "200                    0.3125                  0.524272              0.35   \n",
       "201                    0.4125                  0.660194              0.25   \n",
       "202                    0.4000                  0.165048              0.25   \n",
       "\n",
       "     Min_Humidity  n_hours_sunshine  Dengue_fever_rates  year_month  \n",
       "0        0.578947            0.2030            0.519037  1997-02-28  \n",
       "1        0.368421            0.9885            0.514028  1997-03-31  \n",
       "2        0.236842            0.2800            0.553026  1997-04-30  \n",
       "3        0.789474            0.5210            0.538357  1997-05-31  \n",
       "4        0.526316            0.6420            0.583438  1997-06-30  \n",
       "..            ...               ...                 ...         ...  \n",
       "198      0.578947            0.6950            0.542559  2013-08-31  \n",
       "199      0.605263            0.4400            0.537881  2013-09-30  \n",
       "200      0.394737            0.8300            0.534874  2013-10-31  \n",
       "201      0.394737            0.6500            0.560604  2013-11-30  \n",
       "202      0.552632            0.6100            0.531532  2013-12-31  \n",
       "\n",
       "[203 rows x 14 columns]"
      ]
     },
     "execution_count": 725,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.3672414, 0.4662807, 0.4281359, ..., 0.5789474, 0.203    ,\n",
       "        0.519037 ],\n",
       "       [0.5695402, 0.4628065, 0.4181185, ..., 0.3684211, 0.9885   ,\n",
       "        0.514028 ],\n",
       "       [0.5201149, 0.4964578, 0.5004355, ..., 0.2368421, 0.28     ,\n",
       "        0.5530263],\n",
       "       ...,\n",
       "       [0.4655172, 0.4747956, 0.4756098, ..., 0.3947368, 0.83     ,\n",
       "        0.5348739],\n",
       "       [0.5      , 0.3549046, 0.2447735, ..., 0.3947368, 0.65     ,\n",
       "        0.5606038],\n",
       "       [0.5747126, 0.4536785, 0.5235192, ..., 0.5526316, 0.61     ,\n",
       "        0.5315323]])"
      ]
     },
     "execution_count": 723,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(output_process+city+'_train_preprocessed.csv', parse_dates=True, index_col= None, encoding = 'unicode_escape')\n",
    "train = df.iloc[:,:-1].to_numpy()\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidIndexError",
     "evalue": "(slice(0, 3, None), slice(None, None, None))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/indexes/base.py:3790\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3789\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3790\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3791\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:158\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '(slice(0, 3, None), slice(None, None, None))' is an invalid key",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidIndexError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/trinhtruc/Documents/STUDY/NCKH/Source/Source_14012023_v4/optimize_hyperparam/optuna_ml_252023.ipynb Cell 17\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/trinhtruc/Documents/STUDY/NCKH/Source/Source_14012023_v4/optimize_hyperparam/optuna_ml_252023.ipynb#Y342sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m data \u001b[39m=\u001b[39mdf_train\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/trinhtruc/Documents/STUDY/NCKH/Source/Source_14012023_v4/optimize_hyperparam/optuna_ml_252023.ipynb#Y342sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m data[\u001b[39m0\u001b[39;49m:\u001b[39m3\u001b[39;49m,:]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/frame.py:3896\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3894\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3895\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3896\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   3897\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3898\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3797\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3798\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3799\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3800\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3801\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m-> 3802\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_indexing_error(key)\n\u001b[1;32m   3803\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/indexes/base.py:5974\u001b[0m, in \u001b[0;36mIndex._check_indexing_error\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   5970\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_indexing_error\u001b[39m(\u001b[39mself\u001b[39m, key):\n\u001b[1;32m   5971\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_scalar(key):\n\u001b[1;32m   5972\u001b[0m         \u001b[39m# if key is not a scalar, directly raise an error (the code below\u001b[39;00m\n\u001b[1;32m   5973\u001b[0m         \u001b[39m# would convert to numpy arrays and raise later any way) - GH29926\u001b[39;00m\n\u001b[0;32m-> 5974\u001b[0m         \u001b[39mraise\u001b[39;00m InvalidIndexError(key)\n",
      "\u001b[0;31mInvalidIndexError\u001b[0m: (slice(0, 3, None), slice(None, None, None))"
     ]
    }
   ],
   "source": [
    "data =df_train\n",
    "data[0:3,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 0\n",
      "in_end: 3\n"
     ]
    },
    {
     "ename": "InvalidIndexError",
     "evalue": "(slice(0, 3, None), ['Max_Daily_Rainfall', 'Min_Average_Temperature', 'Dengue_fever_rates', 'Dengue_fever_rates'])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/indexes/base.py:3790\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3789\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3790\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3791\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:158\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '(slice(0, 3, None), ['Max_Daily_Rainfall', 'Min_Average_Temperature', 'Dengue_fever_rates', 'Dengue_fever_rates'])' is an invalid key",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidIndexError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/trinhtruc/Documents/STUDY/NCKH/Source/Source_14012023_v4/optimize_hyperparam/optuna_ml_252023.ipynb Cell 18\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/trinhtruc/Documents/STUDY/NCKH/Source/Source_14012023_v4/optimize_hyperparam/optuna_ml_252023.ipynb#Y340sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# df_train, df_valid = split_data(specific_data, args.look_back,nstep)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/trinhtruc/Documents/STUDY/NCKH/Source/Source_14012023_v4/optimize_hyperparam/optuna_ml_252023.ipynb#Y340sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m features_list \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mTotal_Rainfall\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMin_Humidity\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mDengue_fever_rates\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/trinhtruc/Documents/STUDY/NCKH/Source/Source_14012023_v4/optimize_hyperparam/optuna_ml_252023.ipynb#Y340sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m train_X, train_y \u001b[39m=\u001b[39m to_supervised(df_train, d_out\u001b[39m=\u001b[39;49mnstep, d_in\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49mlook_back, features_list\u001b[39m=\u001b[39;49mfeature_list\u001b[39m+\u001b[39;49m[args\u001b[39m.\u001b[39;49mlabels] )\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/trinhtruc/Documents/STUDY/NCKH/Source/Source_14012023_v4/optimize_hyperparam/optuna_ml_252023.ipynb#Y340sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m train_X\n",
      "\u001b[1;32m/Users/trinhtruc/Documents/STUDY/NCKH/Source/Source_14012023_v4/optimize_hyperparam/optuna_ml_252023.ipynb Cell 18\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/trinhtruc/Documents/STUDY/NCKH/Source/Source_14012023_v4/optimize_hyperparam/optuna_ml_252023.ipynb#Y340sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m             \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mindex:\u001b[39m\u001b[39m\"\u001b[39m,index)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/trinhtruc/Documents/STUDY/NCKH/Source/Source_14012023_v4/optimize_hyperparam/optuna_ml_252023.ipynb#Y340sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m             \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39min_end:\u001b[39m\u001b[39m\"\u001b[39m,in_end)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/trinhtruc/Documents/STUDY/NCKH/Source/Source_14012023_v4/optimize_hyperparam/optuna_ml_252023.ipynb#Y340sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m             X\u001b[39m.\u001b[39mappend(data[index: in_end, features_list])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/trinhtruc/Documents/STUDY/NCKH/Source/Source_14012023_v4/optimize_hyperparam/optuna_ml_252023.ipynb#Y340sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m         y\u001b[39m.\u001b[39mappend(data[out_end\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m: out_end, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/trinhtruc/Documents/STUDY/NCKH/Source/Source_14012023_v4/optimize_hyperparam/optuna_ml_252023.ipynb#Y340sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray(X), np\u001b[39m.\u001b[39marray(y)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/frame.py:3896\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3894\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3895\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3896\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   3897\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3898\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3797\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3798\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3799\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3800\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3801\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m-> 3802\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_indexing_error(key)\n\u001b[1;32m   3803\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/indexes/base.py:5974\u001b[0m, in \u001b[0;36mIndex._check_indexing_error\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   5970\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_indexing_error\u001b[39m(\u001b[39mself\u001b[39m, key):\n\u001b[1;32m   5971\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_scalar(key):\n\u001b[1;32m   5972\u001b[0m         \u001b[39m# if key is not a scalar, directly raise an error (the code below\u001b[39;00m\n\u001b[1;32m   5973\u001b[0m         \u001b[39m# would convert to numpy arrays and raise later any way) - GH29926\u001b[39;00m\n\u001b[0;32m-> 5974\u001b[0m         \u001b[39mraise\u001b[39;00m InvalidIndexError(key)\n",
      "\u001b[0;31mInvalidIndexError\u001b[0m: (slice(0, 3, None), ['Max_Daily_Rainfall', 'Min_Average_Temperature', 'Dengue_fever_rates', 'Dengue_fever_rates'])"
     ]
    }
   ],
   "source": [
    "# df_train, df_valid = split_data(specific_data, args.look_back,nstep)\n",
    "features_list = ['Total_Rainfall', 'Min_Humidity', 'Dengue_fever_rates']\n",
    "train_X, train_y = to_supervised(df_train, d_out=nstep, d_in=args.look_back, features_list=feature_list+[args.labels] )\n",
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(df_train, df_eval, model, feature_list , labels, scaler, is_dl_algo,nstep):\n",
    "  \"\"\"\n",
    "  $df: pandas.DataFrame object containing data for training and testing model:\n",
    "  $model: darts model object\n",
    "  $feature_list: Names of the features used as model input\n",
    "  $label: the value the model will be trained to predict\n",
    "  $scaler: scaler object. Note: the scaler will be fitted on training data and applied to test data\n",
    "  $lags: how much to look back into the past to output prediction\n",
    "  $split_index: the point at which to divide train and test_data\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  if is_dl_algo == 1:\n",
    "    print(\"🍋\")\n",
    "  else:\n",
    "    x_train = TimeSeries.from_dataframe(df_train, \"year_month\", feature_list)\n",
    "    y_train = TimeSeries.from_dataframe(df_train, \"year_month\", labels)\n",
    "\n",
    "    x_test = TimeSeries.from_dataframe(df_eval, \"year_month\", feature_list)\n",
    "    y_test = TimeSeries.from_dataframe(df_eval, \"year_month\", labels)\n",
    "\n",
    "    model.fit(y_train, past_covariates = x_train)\n",
    "\n",
    "    prediction = model.predict(len(y_test)-args.look_back, past_covariates = x_test, num_samples=1)\n",
    "\n",
    "    y_true = scaler.inverse_transform(df_eval.iloc[:,:-1])[:,[-1]].reshape(len(df_eval))[args.look_back:]\n",
    "\n",
    "    df_eval[labels][args.look_back:] = np.array(prediction._xa).squeeze()\n",
    "    y_pred = scaler.inverse_transform(df_eval.iloc[args.look_back:,:-1])[:,[-1]].reshape(len(prediction))\n",
    "\n",
    "    # df_compare_test_predict = pd.DataFrame({'y_true':y_true, 'y_pred':y_pred})\n",
    "    # df_compare_test_predict.plot()\n",
    "    # plt.legend()\n",
    "    # plt.show()\n",
    "\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = mse**0.5\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    print(f\"mean_squared_error: {mse:.4f}\")\n",
    "    print(f\"rmse: {rmse}\")\n",
    "    print(f\"mape: {mape}\")\n",
    "    return model, y_true, y_pred, mse, mae, rmse, mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_prediction_for_location(df_train, df_eval, model, location, feature_list, \n",
    "                                                labels, scaler, is_dl_algo,nstep):\n",
    "    \"\"\"train and generate prediction for a province\n",
    "    df: DataFrame object containing features and label(s) for training model\n",
    "    localtion: location_name\n",
    "    feature_list: list of features used as model input,  must be among the column names of df\n",
    "    labels: the values model will be trained to predict\n",
    "    scaler: sklearn scaler object\n",
    "    lags: how long into the past to look back when making prediction\n",
    "    split_index: the point at which to divide data into the train and test subsets.\n",
    "    \"\"\"\n",
    "    model, y_true, prediction_inverse, mse, mae, rmse, mape = train_and_evaluate(df_train, df_eval, model, feature_list, labels, scaler,is_dl_algo,nstep)\n",
    "    df_prediction = pd.DataFrame({\"Date\": df_eval[\"year_month\"][-len(prediction_inverse):],\n",
    "                                  \"Observed\": y_true[-len(prediction_inverse):],\n",
    "                                  \"1-month\": prediction_inverse})\n",
    "    \n",
    "    df_prediction[\"City\"] = location\n",
    "    df_prediction[\"RMSE_1-month\"] = rmse\n",
    "    df_prediction[\"MAE_1-month\"] = mae\n",
    "    df_prediction[\"MAPE_1-month\"] = mape\n",
    "\n",
    "    return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataWithSelectedFeature(city, next_predicted_month):\n",
    "  selected_feature = []\n",
    "  df = pd.read_csv(output_featureselection+str(next_predicted_month)+\"step_feature_selection_3_most.csv\", encoding = 'unicode_escape')\n",
    "  for row in range(len(df)):\n",
    "    if (df[\"City\"][row] == city):\n",
    "      selected_feature.append(df[\"1st_Feature\"][row])\n",
    "      selected_feature.append(df[\"2nd_Feature\"][row])\n",
    "      selected_feature.append(df[\"3rd_Feature\"][row])\n",
    "  return selected_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective and Suggest Hyperparams of Darts Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "city = \"An Giang\"\n",
    "nstep = 6\n",
    "\n",
    "lags_by_nstep = args.look_back + nstep - 1\n",
    "lags_past_covariates_by_nstep = [-lags_by_nstep+2,-lags_by_nstep+1,-lags_by_nstep] #Mảng này chứa ba giá trị tương ứng cho args.lookback 3\n",
    "\n",
    "specific_data = pd.read_csv(output_process+city+'_train_preprocessed.csv', parse_dates=True, index_col= None, encoding = 'unicode_escape')\n",
    "scaler = joblib.load(output_process+city+'_train_scalerMinMaxNorm.save') #ok\n",
    "labels=args.labels\n",
    "\n",
    "df_train, df_eval = split_data(specific_data, args.look_back,nstep)\n",
    "selected_features = getDataWithSelectedFeature(city, nstep)\n",
    "\n",
    "\n",
    "predicted_train_days = len(df_train)-args.look_back-nstep+1\n",
    "predicted_test_days = len(df_eval)-args.look_back-nstep+1\n",
    "x_train = TimeSeries.from_dataframe(df_train, \"year_month\", selected_features)\n",
    "y_train = TimeSeries.from_dataframe(df_train, \"year_month\", labels)\n",
    "\n",
    "x_test = TimeSeries.from_dataframe(df_eval, \"year_month\", selected_features)\n",
    "y_test = TimeSeries.from_dataframe(df_eval, \"year_month\", labels)\n",
    "\n",
    "random_state = 300#trial.suggest_int('random_state', 0, 1000)\n",
    "n_rnn_layers =  2#trial.suggest_int('n_rnn_layers', 1, 3)\n",
    "dropout =  0.2#trial.suggest_uniform('dropout', 0.1, 0.5)\n",
    "hidden_dim =  10#trial.suggest_int('n_rnn_layers', 5, 20)\n",
    "n_epochs =  100#trial.suggest_int('n_epochs', 50, 200)\n",
    "\n",
    "pl_trainer_kwargs = {\n",
    "              \"accelerator\": \"cpu\",\n",
    "            #   \"devices\": -1,\n",
    "            #   \"auto_select_gpus\": True,\n",
    "          }\n",
    "\n",
    "# model = BlockRNNModel(\n",
    "#                     input_chunk_length = 3,\n",
    "#                     output_chunk_length = 6,\n",
    "#                     hidden_dim = hidden_dim,\n",
    "#                     n_rnn_layers = n_rnn_layers,\n",
    "#                     dropout = dropout,\n",
    "#                     n_epochs = n_epochs,\n",
    "#                     pl_trainer_kwargs = pl_trainer_kwargs,\n",
    "#                     random_state=random_state)\n",
    "# model = TFTModel(\n",
    "#                     input_chunk_length = 3,\n",
    "#                     output_chunk_length = 6,\n",
    "#                     add_relative_index = True,\n",
    "#                     dropout = dropout,\n",
    "#                     n_epochs = n_epochs ,\n",
    "#                     pl_trainer_kwargs = pl_trainer_kwargs,\n",
    "#                     random_state=random_state)\n",
    "# model = NHiTSModel(\n",
    "#                           input_chunk_length = 3,\n",
    "#                           output_chunk_length = 6,\n",
    "#                           MaxPool1d = True,\n",
    "#                           dropout = dropout,\n",
    "#                           n_epochs = n_epochs ,\n",
    "#                           pl_trainer_kwargs = pl_trainer_kwargs,\n",
    "#                           random_state=random_state)\n",
    "model = NBEATSModel(\n",
    "                            input_chunk_length = 3,\n",
    "                            output_chunk_length = 6,\n",
    "                            dropout = dropout,\n",
    "                            n_epochs = n_epochs ,\n",
    "                            pl_trainer_kwargs = pl_trainer_kwargs,\n",
    "                            random_state=random_state)\n",
    "# model = TCNModel(\n",
    "#           input_chunk_length = 3,\n",
    "#           output_chunk_length = 6,\n",
    "#           batch_size=16,\n",
    "#           n_epochs=50,\n",
    "#           nr_epochs_val_period=1,\n",
    "#           kernel_size=2,\n",
    "#           num_filters=1,\n",
    "#           weight_norm=True,\n",
    "#           dilation_base=3,\n",
    "#           dropout=0.2,\n",
    "#           optimizer_kwargs={\"lr\": 5e-5},\n",
    "#           add_encoders=None,\n",
    "#           likelihood=GaussianLikelihood(),\n",
    "#           pl_trainer_kwargs=pl_trainer_kwargs,\n",
    "#           model_name=\"tcn_model\",\n",
    "#           force_reset=True,\n",
    "#           save_checkpoints=True,\n",
    "#       )\n",
    "model.fit(y_train, past_covariates = x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 125.50it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><svg style=\"position: absolute; width: 0; height: 0; overflow: hidden\">\n",
       "<defs>\n",
       "<symbol id=\"icon-database\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z\"></path>\n",
       "<path d=\"M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "<path d=\"M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "</symbol>\n",
       "<symbol id=\"icon-file-text2\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z\"></path>\n",
       "<path d=\"M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "</symbol>\n",
       "</defs>\n",
       "</svg>\n",
       "<style>/* CSS stylesheet for displaying xarray objects in jupyterlab.\n",
       " *\n",
       " */\n",
       "\n",
       ":root {\n",
       "  --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1));\n",
       "  --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54));\n",
       "  --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38));\n",
       "  --xr-border-color: var(--jp-border-color2, #e0e0e0);\n",
       "  --xr-disabled-color: var(--jp-layout-color3, #bdbdbd);\n",
       "  --xr-background-color: var(--jp-layout-color0, white);\n",
       "  --xr-background-color-row-even: var(--jp-layout-color1, white);\n",
       "  --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee);\n",
       "}\n",
       "\n",
       "html[theme=dark],\n",
       "body[data-theme=dark],\n",
       "body.vscode-dark {\n",
       "  --xr-font-color0: rgba(255, 255, 255, 1);\n",
       "  --xr-font-color2: rgba(255, 255, 255, 0.54);\n",
       "  --xr-font-color3: rgba(255, 255, 255, 0.38);\n",
       "  --xr-border-color: #1F1F1F;\n",
       "  --xr-disabled-color: #515151;\n",
       "  --xr-background-color: #111111;\n",
       "  --xr-background-color-row-even: #111111;\n",
       "  --xr-background-color-row-odd: #313131;\n",
       "}\n",
       "\n",
       ".xr-wrap {\n",
       "  display: block !important;\n",
       "  min-width: 300px;\n",
       "  max-width: 700px;\n",
       "}\n",
       "\n",
       ".xr-text-repr-fallback {\n",
       "  /* fallback to plain text repr when CSS is not injected (untrusted notebook) */\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-header {\n",
       "  padding-top: 6px;\n",
       "  padding-bottom: 6px;\n",
       "  margin-bottom: 4px;\n",
       "  border-bottom: solid 1px var(--xr-border-color);\n",
       "}\n",
       "\n",
       ".xr-header > div,\n",
       ".xr-header > ul {\n",
       "  display: inline;\n",
       "  margin-top: 0;\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-obj-type,\n",
       ".xr-array-name {\n",
       "  margin-left: 2px;\n",
       "  margin-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-obj-type {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-sections {\n",
       "  padding-left: 0 !important;\n",
       "  display: grid;\n",
       "  grid-template-columns: 150px auto auto 1fr 20px 20px;\n",
       "}\n",
       "\n",
       ".xr-section-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-section-item input {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-item input + label {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label {\n",
       "  cursor: pointer;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label:hover {\n",
       "  color: var(--xr-font-color0);\n",
       "}\n",
       "\n",
       ".xr-section-summary {\n",
       "  grid-column: 1;\n",
       "  color: var(--xr-font-color2);\n",
       "  font-weight: 500;\n",
       "}\n",
       "\n",
       ".xr-section-summary > span {\n",
       "  display: inline-block;\n",
       "  padding-left: 0.5em;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in + label:before {\n",
       "  display: inline-block;\n",
       "  content: '►';\n",
       "  font-size: 11px;\n",
       "  width: 15px;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label:before {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label:before {\n",
       "  content: '▼';\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label > span {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-summary,\n",
       ".xr-section-inline-details {\n",
       "  padding-top: 4px;\n",
       "  padding-bottom: 4px;\n",
       "}\n",
       "\n",
       ".xr-section-inline-details {\n",
       "  grid-column: 2 / -1;\n",
       "}\n",
       "\n",
       ".xr-section-details {\n",
       "  display: none;\n",
       "  grid-column: 1 / -1;\n",
       "  margin-bottom: 5px;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked ~ .xr-section-details {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-array-wrap {\n",
       "  grid-column: 1 / -1;\n",
       "  display: grid;\n",
       "  grid-template-columns: 20px auto;\n",
       "}\n",
       "\n",
       ".xr-array-wrap > label {\n",
       "  grid-column: 1;\n",
       "  vertical-align: top;\n",
       "}\n",
       "\n",
       ".xr-preview {\n",
       "  color: var(--xr-font-color3);\n",
       "}\n",
       "\n",
       ".xr-array-preview,\n",
       ".xr-array-data {\n",
       "  padding: 0 5px !important;\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-array-data,\n",
       ".xr-array-in:checked ~ .xr-array-preview {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-array-in:checked ~ .xr-array-data,\n",
       ".xr-array-preview {\n",
       "  display: inline-block;\n",
       "}\n",
       "\n",
       ".xr-dim-list {\n",
       "  display: inline-block !important;\n",
       "  list-style: none;\n",
       "  padding: 0 !important;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list li {\n",
       "  display: inline-block;\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list:before {\n",
       "  content: '(';\n",
       "}\n",
       "\n",
       ".xr-dim-list:after {\n",
       "  content: ')';\n",
       "}\n",
       "\n",
       ".xr-dim-list li:not(:last-child):after {\n",
       "  content: ',';\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-has-index {\n",
       "  font-weight: bold;\n",
       "}\n",
       "\n",
       ".xr-var-list,\n",
       ".xr-var-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-var-item > div,\n",
       ".xr-var-item label,\n",
       ".xr-var-item > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-even);\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-var-item > .xr-var-name:hover span {\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-var-list > li:nth-child(odd) > div,\n",
       ".xr-var-list > li:nth-child(odd) > label,\n",
       ".xr-var-list > li:nth-child(odd) > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-odd);\n",
       "}\n",
       "\n",
       ".xr-var-name {\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-var-dims {\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-var-dtype {\n",
       "  grid-column: 3;\n",
       "  text-align: right;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-preview {\n",
       "  grid-column: 4;\n",
       "}\n",
       "\n",
       ".xr-index-preview {\n",
       "  grid-column: 2 / 5;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-name,\n",
       ".xr-var-dims,\n",
       ".xr-var-dtype,\n",
       ".xr-preview,\n",
       ".xr-attrs dt {\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-var-name:hover,\n",
       ".xr-var-dims:hover,\n",
       ".xr-var-dtype:hover,\n",
       ".xr-attrs dt:hover {\n",
       "  overflow: visible;\n",
       "  width: auto;\n",
       "  z-index: 1;\n",
       "}\n",
       "\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  display: none;\n",
       "  background-color: var(--xr-background-color) !important;\n",
       "  padding-bottom: 5px !important;\n",
       "}\n",
       "\n",
       ".xr-var-attrs-in:checked ~ .xr-var-attrs,\n",
       ".xr-var-data-in:checked ~ .xr-var-data,\n",
       ".xr-index-data-in:checked ~ .xr-index-data {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       ".xr-var-data > table {\n",
       "  float: right;\n",
       "}\n",
       "\n",
       ".xr-var-name span,\n",
       ".xr-var-data,\n",
       ".xr-index-name div,\n",
       ".xr-index-data,\n",
       ".xr-attrs {\n",
       "  padding-left: 25px !important;\n",
       "}\n",
       "\n",
       ".xr-attrs,\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  grid-column: 1 / -1;\n",
       "}\n",
       "\n",
       "dl.xr-attrs {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  display: grid;\n",
       "  grid-template-columns: 125px auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt,\n",
       ".xr-attrs dd {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  float: left;\n",
       "  padding-right: 10px;\n",
       "  width: auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt {\n",
       "  font-weight: normal;\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-attrs dt:hover span {\n",
       "  display: inline-block;\n",
       "  background: var(--xr-background-color);\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-attrs dd {\n",
       "  grid-column: 2;\n",
       "  white-space: pre-wrap;\n",
       "  word-break: break-all;\n",
       "}\n",
       "\n",
       ".xr-icon-database,\n",
       ".xr-icon-file-text2,\n",
       ".xr-no-icon {\n",
       "  display: inline-block;\n",
       "  vertical-align: middle;\n",
       "  width: 1em;\n",
       "  height: 1.5em !important;\n",
       "  stroke-width: 0;\n",
       "  stroke: currentColor;\n",
       "  fill: currentColor;\n",
       "}\n",
       "</style><pre class='xr-text-repr-fallback'>&lt;TimeSeries (DataArray) (year_month: 6, component: 1, sample: 1)&gt;\n",
       "array([[[0.50291951]],\n",
       "\n",
       "       [[0.52095742]],\n",
       "\n",
       "       [[0.56060101]],\n",
       "\n",
       "       [[0.4539065 ]],\n",
       "\n",
       "       [[0.50775246]],\n",
       "\n",
       "       [[0.5612073 ]]])\n",
       "Coordinates:\n",
       "  * year_month  (year_month) datetime64[ns] 2011-01-31 2011-02-28 ... 2011-06-30\n",
       "  * component   (component) object &#x27;Dengue_fever_rates&#x27;\n",
       "Dimensions without coordinates: sample\n",
       "Attributes:\n",
       "    static_covariates:  None\n",
       "    hierarchy:          None</pre><div class='xr-wrap' style='display:none'><div class='xr-header'><div class='xr-obj-type'>TimeSeries (DataArray)</div><div class='xr-array-name'></div><ul class='xr-dim-list'><li><span class='xr-has-index'>year_month</span>: 6</li><li><span class='xr-has-index'>component</span>: 1</li><li><span>sample</span>: 1</li></ul></div><ul class='xr-sections'><li class='xr-section-item'><div class='xr-array-wrap'><input id='section-fce7476d-6232-4195-93b7-49e6871cc8aa' class='xr-array-in' type='checkbox' checked><label for='section-fce7476d-6232-4195-93b7-49e6871cc8aa' title='Show/hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-array-preview xr-preview'><span>0.5029 0.521 0.5606 0.4539 0.5078 0.5612</span></div><div class='xr-array-data'><pre>array([[[0.50291951]],\n",
       "\n",
       "       [[0.52095742]],\n",
       "\n",
       "       [[0.56060101]],\n",
       "\n",
       "       [[0.4539065 ]],\n",
       "\n",
       "       [[0.50775246]],\n",
       "\n",
       "       [[0.5612073 ]]])</pre></div></div></li><li class='xr-section-item'><input id='section-59d7f076-1e18-4519-a7f7-349923a9d734' class='xr-section-summary-in' type='checkbox'  checked><label for='section-59d7f076-1e18-4519-a7f7-349923a9d734' class='xr-section-summary' >Coordinates: <span>(2)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>year_month</span></div><div class='xr-var-dims'>(year_month)</div><div class='xr-var-dtype'>datetime64[ns]</div><div class='xr-var-preview xr-preview'>2011-01-31 ... 2011-06-30</div><input id='attrs-93e6ac05-fee5-439c-bdba-8379450739be' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-93e6ac05-fee5-439c-bdba-8379450739be' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-38ccabca-5435-46b3-b349-039fd2d78d99' class='xr-var-data-in' type='checkbox'><label for='data-38ccabca-5435-46b3-b349-039fd2d78d99' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([&#x27;2011-01-31T00:00:00.000000000&#x27;, &#x27;2011-02-28T00:00:00.000000000&#x27;,\n",
       "       &#x27;2011-03-31T00:00:00.000000000&#x27;, &#x27;2011-04-30T00:00:00.000000000&#x27;,\n",
       "       &#x27;2011-05-31T00:00:00.000000000&#x27;, &#x27;2011-06-30T00:00:00.000000000&#x27;],\n",
       "      dtype=&#x27;datetime64[ns]&#x27;)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>component</span></div><div class='xr-var-dims'>(component)</div><div class='xr-var-dtype'>object</div><div class='xr-var-preview xr-preview'>&#x27;Dengue_fever_rates&#x27;</div><input id='attrs-16fdd41a-b89f-47bf-a1cb-b0afc26c3537' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-16fdd41a-b89f-47bf-a1cb-b0afc26c3537' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-8098f8dc-d3e1-4a67-82c1-6e202b356d79' class='xr-var-data-in' type='checkbox'><label for='data-8098f8dc-d3e1-4a67-82c1-6e202b356d79' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([&#x27;Dengue_fever_rates&#x27;], dtype=object)</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-920a7c0d-4e05-4e21-b2bf-88ac98ff4b1a' class='xr-section-summary-in' type='checkbox'  ><label for='section-920a7c0d-4e05-4e21-b2bf-88ac98ff4b1a' class='xr-section-summary' >Indexes: <span>(2)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-index-name'><div>year_month</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-ff14125a-92e0-497d-b479-4f0e8747f6f0' class='xr-index-data-in' type='checkbox'/><label for='index-ff14125a-92e0-497d-b479-4f0e8747f6f0' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(DatetimeIndex([&#x27;2011-01-31&#x27;, &#x27;2011-02-28&#x27;, &#x27;2011-03-31&#x27;, &#x27;2011-04-30&#x27;,\n",
       "               &#x27;2011-05-31&#x27;, &#x27;2011-06-30&#x27;],\n",
       "              dtype=&#x27;datetime64[ns]&#x27;, name=&#x27;year_month&#x27;, freq=&#x27;M&#x27;))</pre></div></li><li class='xr-var-item'><div class='xr-index-name'><div>component</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-8a9a0315-948a-4fe4-9470-9c2f8a8051a5' class='xr-index-data-in' type='checkbox'/><label for='index-8a9a0315-948a-4fe4-9470-9c2f8a8051a5' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Index([&#x27;Dengue_fever_rates&#x27;], dtype=&#x27;object&#x27;, name=&#x27;component&#x27;))</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-2c47fd69-2c05-4f50-bf5d-8a07ec2b0d4c' class='xr-section-summary-in' type='checkbox'  checked><label for='section-2c47fd69-2c05-4f50-bf5d-8a07ec2b0d4c' class='xr-section-summary' >Attributes: <span>(2)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><dl class='xr-attrs'><dt><span>static_covariates :</span></dt><dd>None</dd><dt><span>hierarchy :</span></dt><dd>None</dd></dl></div></li></ul></div></div>"
      ],
      "text/plain": [
       "<TimeSeries (DataArray) (year_month: 6, component: 1, sample: 1)>\n",
       "array([[[0.50291951]],\n",
       "\n",
       "       [[0.52095742]],\n",
       "\n",
       "       [[0.56060101]],\n",
       "\n",
       "       [[0.4539065 ]],\n",
       "\n",
       "       [[0.50775246]],\n",
       "\n",
       "       [[0.5612073 ]]])\n",
       "Coordinates:\n",
       "  * year_month  (year_month) datetime64[ns] 2011-01-31 2011-02-28 ... 2011-06-30\n",
       "  * component   (component) object 'Dengue_fever_rates'\n",
       "Dimensions without coordinates: sample\n",
       "Attributes:\n",
       "    static_covariates:  None\n",
       "    hierarchy:          None"
      ]
     },
     "execution_count": 627,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model.predict(6, past_covariates = x_test[0:8], num_samples=1)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(model_name, trial, city, nstep):   \n",
    "    specific_data = pd.read_csv(output_process+city+'_train_preprocessed.csv', parse_dates=True, index_col= None, encoding = 'unicode_escape')\n",
    "    scaler = joblib.load(output_process+city+'_train_scalerMinMaxNorm.save') #ok\n",
    "\n",
    "    df_train, df_valid = split_data(specific_data, args.look_back,nstep)\n",
    "\n",
    "    selected_features = getDataWithSelectedFeature(city, nstep)\n",
    "\n",
    "    lags_by_nstep = args.look_back + nstep - 1\n",
    "    lags_past_covariates_by_nstep = [-lags_by_nstep+2,-lags_by_nstep+1,-lags_by_nstep] #Mảng này chứa ba giá trị tương ứng cho args.lookback 3\n",
    "    is_dl_algo = 0\n",
    "\n",
    "    pl_trainer_kwargs = {\n",
    "              \"accelerator\": \"cpu\",\n",
    "              # \"devices\": -1,\n",
    "              # \"auto_select_gpus\": True,\n",
    "          }\n",
    "\n",
    "    if model_name == \"RandomForest\":\n",
    "      random_state = trial.suggest_int('random_state', 0, 42)\n",
    "      n_estimators = trial.suggest_int('n_estimators', 50, 200)\n",
    "      max_depth = trial.suggest_int('max_depth', 1, 15)\n",
    "      # Create the RandomForest model\n",
    "      model = RandomForest(\n",
    "                    lags = lags_by_nstep,\n",
    "                    lags_past_covariates = lags_past_covariates_by_nstep,\n",
    "                    output_chunk_length = 1,\n",
    "                    n_estimators = n_estimators,\n",
    "                    max_depth = max_depth,\n",
    "                    random_state=random_state)\n",
    "    elif model_name == 'XGBModel':\n",
    "      random_state = trial.suggest_int('random_state', 0, 43)\n",
    "      likelihood = trial.suggest_categorical('likelihood', ['quantile'])\n",
    "      # Create the  model\n",
    "      model = XGBModel(\n",
    "                      lags = lags_by_nstep,\n",
    "                      lags_past_covariates = lags_past_covariates_by_nstep,\n",
    "                      output_chunk_length = 1,\n",
    "                      random_state=random_state,\n",
    "                      likelihood = likelihood\n",
    "                    )\n",
    "    elif model_name == 'LinearRegressionModel':\n",
    "      random_state = trial.suggest_int('random_state', 0, 43)\n",
    "      # Create the  model\n",
    "      model = LinearRegressionModel(\n",
    "                      lags = lags_by_nstep,\n",
    "                      lags_past_covariates = lags_past_covariates_by_nstep,\n",
    "                      output_chunk_length = 1,\n",
    "                      random_state=random_state)\n",
    "    elif model_name == \"CatBoostModel\":\n",
    "      #suggest hyperparams\n",
    "      learning_rate = trial.suggest_float('learning_rate', 0.001, 0.1)\n",
    "      n_estimators = trial.suggest_int('n_estimators', 50, 200)\n",
    "      max_depth = trial.suggest_int('max_depth', 1, 15)\n",
    "      random_state = trial.suggest_int('random_state', 0, 1000)\n",
    "      likelihood = trial.suggest_categorical('likelihood', ['quantile'])\n",
    "      quantiles =  trial.suggest_categorical('quantiles', [None, [0.1, 0.5, 0.9]])\n",
    "      bagging_temperature = trial.suggest_float('bagging_temperature', 0.01, 100.0)\n",
    "      border_count = trial.suggest_int('border_count', 1, 255)\n",
    "      l2_leaf_reg = trial.suggest_float('l2_leaf_reg', 0.1, 10)\n",
    "      random_strength = trial.suggest_float('random_strength', 0.1, 10)\n",
    "      model = CatBoostModel(\n",
    "                            lags=lags_by_nstep,\n",
    "                            lags_past_covariates=lags_past_covariates_by_nstep, \n",
    "                            learning_rate=learning_rate,\n",
    "                            n_estimators=n_estimators,\n",
    "                            max_depth=max_depth, \n",
    "                            output_chunk_length = 1,\n",
    "                            likelihood = likelihood,\n",
    "                            quantiles = quantiles,\n",
    "                            bagging_temperature = bagging_temperature,\n",
    "                            border_count = border_count,\n",
    "                            l2_leaf_reg = l2_leaf_reg,\n",
    "                            random_strength = random_strength,\n",
    "                            random_state=random_state)\n",
    "    elif model_name == \"LightGBMModel\":\n",
    "      params = {\n",
    "        \"lags\": lags_by_nstep,\n",
    "        \"lags_past_covariates\": lags_past_covariates_by_nstep,\n",
    "        \"random_state\": trial.suggest_int(\"random_state\", 0, 999),\n",
    "        \"multi_models\": trial.suggest_categorical(\"multi_models\", [True, False]),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
    "        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.1, 1.0),\n",
    "        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.1, 1.0),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n",
    "        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n",
    "        'verbose': -1,\n",
    "        'likelihood' : trial.suggest_categorical(\"likelihood\", [\"quantile\"])\n",
    "      }\n",
    "\n",
    "      param = params\n",
    "      model = LightGBMModel(\n",
    "          lags = param['lags'],\n",
    "          lags_past_covariates = param['lags_past_covariates'],\n",
    "          output_chunk_length = 1,\n",
    "          random_state = param['random_state'],\n",
    "          multi_models = param['multi_models'],\n",
    "          likelihood = param['likelihood'],\n",
    "          num_leaves = param['num_leaves'],\n",
    "          learning_rate = param['learning_rate'],\n",
    "          feature_fraction = param['feature_fraction'],\n",
    "          bagging_fraction = param['bagging_fraction'],\n",
    "          min_child_samples = param['min_child_samples'],\n",
    "          lambda_l1 = param['lambda_l1'],\n",
    "          verbose = param['verbose']\n",
    "      )\n",
    "    elif model_name == \"SVMRBF\":\n",
    "      random_state = trial.suggest_int('random_state', 0, 42)\n",
    "      model = SVC(\n",
    "          random_state = random_state,\n",
    "          kernel = 'rbf',\n",
    "          probability=True\n",
    "      )\n",
    "    elif model_name == \"PoissonRegressor\":\n",
    "      max_iter = trial.suggest_int('max_iter', 50, 200)\n",
    "      model = PoissonRegressor(\n",
    "          max_iter = max_iter,\n",
    "      )\n",
    "    elif model_name == \"BlockRNNModel\":\n",
    "      #suggest hyperparams\n",
    "      random_state = trial.suggest_int('random_state', 0, 1000)\n",
    "      n_rnn_layers = trial.suggest_int('n_rnn_layers', 1, 3)\n",
    "      dropout = trial.suggest_uniform('dropout', 0.1, 0.5)\n",
    "      hidden_dim = trial.suggest_int('n_rnn_layers', 5, 20)\n",
    "      n_epochs = trial.suggest_int('n_epochs', 50, 200)\n",
    "\n",
    "      model = BlockRNNModel(\n",
    "                          input_chunk_length = args.look_back,\n",
    "                          output_chunk_length = args.n_predicted_period_months,\n",
    "                          hidden_dim = hidden_dim,\n",
    "                          n_rnn_layers = n_rnn_layers,\n",
    "                          dropout = dropout,\n",
    "                          n_epochs = n_epochs,\n",
    "                          pl_trainer_kwargs = pl_trainer_kwargs,\n",
    "                          random_state=random_state)\n",
    "      is_dl_algo = 1\n",
    "    elif model_name == 'TFTModel':\n",
    "      # Define the hyperparameters to optimize\n",
    "      random_state = trial.suggest_int('random_state', 0, 42)\n",
    "      dropout = trial.suggest_uniform('dropout', 0.01, 0.8)\n",
    "      n_epochs = trial.suggest_int('n_epochs', 50, 200)\n",
    "\n",
    "      # Create the TFTModel model\n",
    "      model = TFTModel(\n",
    "                    input_chunk_length = args.look_back,\n",
    "                    output_chunk_length = args.n_predicted_period_months,\n",
    "                    add_relative_index = True,\n",
    "                    dropout = dropout,\n",
    "                    n_epochs = n_epochs ,\n",
    "                    random_state=random_state)\n",
    "      is_dl_algo = 1\n",
    "    elif model_name == 'NHiTSModel':\n",
    "      #suggest hyperparams\n",
    "      random_state = trial.suggest_int('random_state', 0, 42)\n",
    "      dropout = trial.suggest_uniform('dropout', 0.01, 0.80)\n",
    "      n_epochs = trial.suggest_int('n_epochs', 100, 500, step=10)\n",
    "      MaxPool1d = trial.suggest_categorical('MaxPool1d', [True, False])\n",
    "\n",
    "      model = NHiTSModel(\n",
    "                          input_chunk_length = args.look_back,\n",
    "                          output_chunk_length = args.n_predicted_period_months,\n",
    "                          MaxPool1d = MaxPool1d,\n",
    "                          dropout = dropout,\n",
    "                          n_epochs = n_epochs ,\n",
    "                          pl_trainer_kwargs = pl_trainer_kwargs,\n",
    "                          random_state=random_state)\n",
    "      is_dl_algo = 1\n",
    "    elif model_name == \"NBEATSModel\":\n",
    "      random_state = trial.suggest_int('random_state', 0, 42)\n",
    "      dropout = trial.suggest_uniform('dropout', 0.01, 0.80)\n",
    "      n_epochs = trial.suggest_int('n_epochs', 50, 200)\n",
    "      model = NBEATSModel(\n",
    "                            input_chunk_length = args.look_back,\n",
    "                            output_chunk_length = args.n_predicted_period_months,\n",
    "                            dropout = dropout,\n",
    "                            n_epochs = n_epochs ,\n",
    "                            pl_trainer_kwargs = pl_trainer_kwargs,\n",
    "                            random_state=random_state)\n",
    "      is_dl_algo = 1\n",
    "    elif model_name == \"TCNModel\":\n",
    "      params = {\n",
    "        'kernel_size': trial.suggest_int(\"kernel_size\", 2, lags_by_nstep),\n",
    "        'num_filters': trial.suggest_int(\"num_filters\", 1, 5),\n",
    "        'weight_norm': trial.suggest_categorical(\"weight_norm\", [False, True]),\n",
    "        'dilation_base': trial.suggest_int(\"dilation_base\", 2, 4),\n",
    "        'dropout': trial.suggest_float(\"dropout\", 0.0, 0.4),\n",
    "        'learning_rate': trial.suggest_float(\"learning_rate\", 5e-5, 1e-3, log=True),\n",
    "        'include_year': trial.suggest_categorical(\"year\", [False, True]),\n",
    "        'n_epochs': trial.suggest_int(\"n_epochs\", 100, 300),\n",
    "      }\n",
    "      # select input and output chunk lengths\n",
    "      params['input_chunk_length'] = args.look_back\n",
    "      params['output_chunk_length'] = args.n_predicted_period_months  \n",
    "      # optionally also add the (scaled) year value as a past covariate\n",
    "      if params['include_year']:\n",
    "          encoders = {\"datetime_attribute\": {\"past\": [\"year\"]},\n",
    "                      \"transformer\": Scaler()}\n",
    "      else:\n",
    "          encoders = None\n",
    "      params['encoders'] = encoders\n",
    "      param = params\n",
    "      model = TCNModel(\n",
    "          input_chunk_length=param['input_chunk_length'],\n",
    "          output_chunk_length=param['output_chunk_length'],\n",
    "          batch_size=16,\n",
    "          n_epochs=param['n_epochs'],\n",
    "          nr_epochs_val_period=1,\n",
    "          kernel_size=param['kernel_size'],\n",
    "          num_filters=param['num_filters'],\n",
    "          weight_norm=param['weight_norm'],\n",
    "          dilation_base=param['dilation_base'],\n",
    "          dropout=param['dropout'],\n",
    "          optimizer_kwargs={\"lr\": param['learning_rate']},\n",
    "          add_encoders=param['encoders'],\n",
    "          likelihood=GaussianLikelihood(),\n",
    "          pl_trainer_kwargs=pl_trainer_kwargs,\n",
    "          model_name=\"tcn_model\",\n",
    "          force_reset=True,\n",
    "          save_checkpoints=True,\n",
    "      )\n",
    "      is_dl_algo = 1\n",
    "    \n",
    "    mae_error = output_prediction_for_location(df_train, df_valid, model, location=city, feature_list=selected_features,\n",
    "                                                labels=args.labels, scaler=scaler, is_dl_algo = is_dl_algo,nstep)\n",
    "\n",
    "    return mae_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main run optimize and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-27 12:27:26,746] A new study created in memory with name: RandomForest\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⭐️ Nstep:  1\n",
      "⭐️ Model_name:  RandomForest\n",
      "⭐️ City:  An Giang\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-27 12:27:27,158] Trial 0 finished with value: 6.097256629441549 and parameters: {'random_state': 12, 'n_estimators': 175, 'max_depth': 6}. Best is trial 0 with value: 6.097256629441549.\n",
      "[I 2023-10-27 12:27:27,191] A new study created in memory with name: RandomForest\n",
      "[I 2023-10-27 12:27:27,306] Trial 0 finished with value: 13.60339303873743 and parameters: {'random_state': 28, 'n_estimators': 88, 'max_depth': 12}. Best is trial 0 with value: 13.60339303873743.\n",
      "[I 2023-10-27 12:27:27,312] A new study created in memory with name: RandomForest\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_squared_error: 105.0770\n",
      "rmse: 10.250708569440274\n",
      "mape: 86762.76542375951\n",
      "Study statistics for : \n",
      "  Number of finished trials:  1\n",
      "Best trial of city:  An Giang\n",
      "⭐️ City:  BR Vũng Tàu\n",
      "mean_squared_error: 319.2892\n",
      "rmse: 17.868664186081478\n",
      "mape: 5.1471560068223\n",
      "Study statistics for : \n",
      "  Number of finished trials:  1\n",
      "Best trial of city:  BR Vũng Tàu\n",
      "⭐️ Nstep:  2\n",
      "⭐️ Model_name:  RandomForest\n",
      "⭐️ City:  An Giang\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-27 12:27:27,439] Trial 0 finished with value: 5.416176071564901 and parameters: {'random_state': 42, 'n_estimators': 89, 'max_depth': 4}. Best is trial 0 with value: 5.416176071564901.\n",
      "[I 2023-10-27 12:27:27,446] A new study created in memory with name: RandomForest\n",
      "[I 2023-10-27 12:27:27,539] Trial 0 finished with value: 10.398870733354892 and parameters: {'random_state': 36, 'n_estimators': 62, 'max_depth': 13}. Best is trial 0 with value: 10.398870733354892.\n",
      "[I 2023-10-27 12:27:27,549] A new study created in memory with name: RandomForest\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_squared_error: 78.3160\n",
      "rmse: 8.849632941120936\n",
      "mape: 36683.25692995537\n",
      "Study statistics for : \n",
      "  Number of finished trials:  1\n",
      "Best trial of city:  An Giang\n",
      "⭐️ City:  BR Vũng Tàu\n",
      "mean_squared_error: 186.4894\n",
      "rmse: 13.656110371310708\n",
      "mape: 5.926614935339083\n",
      "Study statistics for : \n",
      "  Number of finished trials:  1\n",
      "Best trial of city:  BR Vũng Tàu\n",
      "⭐️ Nstep:  3\n",
      "⭐️ Model_name:  RandomForest\n",
      "⭐️ City:  An Giang\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-27 12:27:27,767] Trial 0 finished with value: 5.883820809194738 and parameters: {'random_state': 4, 'n_estimators': 122, 'max_depth': 13}. Best is trial 0 with value: 5.883820809194738.\n",
      "[I 2023-10-27 12:27:27,773] A new study created in memory with name: RandomForest\n",
      "[I 2023-10-27 12:27:27,891] Trial 0 finished with value: 12.800498357354186 and parameters: {'random_state': 4, 'n_estimators': 82, 'max_depth': 14}. Best is trial 0 with value: 12.800498357354186.\n",
      "[I 2023-10-27 12:27:27,898] A new study created in memory with name: RandomForest\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_squared_error: 92.6774\n",
      "rmse: 9.626912151081413\n",
      "mape: 56348.513387068066\n",
      "Study statistics for : \n",
      "  Number of finished trials:  1\n",
      "Best trial of city:  An Giang\n",
      "⭐️ City:  BR Vũng Tàu\n",
      "mean_squared_error: 262.6209\n",
      "rmse: 16.205581472183017\n",
      "mape: 12.117646372883518\n",
      "Study statistics for : \n",
      "  Number of finished trials:  1\n",
      "Best trial of city:  BR Vũng Tàu\n",
      "⭐️ Nstep:  4\n",
      "⭐️ Model_name:  RandomForest\n",
      "⭐️ City:  An Giang\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-27 12:27:28,094] Trial 0 finished with value: 4.5817607680430985 and parameters: {'random_state': 5, 'n_estimators': 154, 'max_depth': 2}. Best is trial 0 with value: 4.5817607680430985.\n",
      "[I 2023-10-27 12:27:28,101] A new study created in memory with name: RandomForest\n",
      "[I 2023-10-27 12:27:28,186] Trial 0 finished with value: 9.032816742330965 and parameters: {'random_state': 30, 'n_estimators': 55, 'max_depth': 6}. Best is trial 0 with value: 9.032816742330965.\n",
      "[I 2023-10-27 12:27:28,193] A new study created in memory with name: RandomForest\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_squared_error: 32.8157\n",
      "rmse: 5.728502505361685\n",
      "mape: 23855.24432182322\n",
      "Study statistics for : \n",
      "  Number of finished trials:  1\n",
      "Best trial of city:  An Giang\n",
      "⭐️ City:  BR Vũng Tàu\n",
      "mean_squared_error: 154.3058\n",
      "rmse: 12.42198751084482\n",
      "mape: 3.6104415506733516\n",
      "Study statistics for : \n",
      "  Number of finished trials:  1\n",
      "Best trial of city:  BR Vũng Tàu\n",
      "⭐️ Nstep:  5\n",
      "⭐️ Model_name:  RandomForest\n",
      "⭐️ City:  An Giang\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-27 12:27:28,419] Trial 0 finished with value: 4.176993094262514 and parameters: {'random_state': 35, 'n_estimators': 175, 'max_depth': 1}. Best is trial 0 with value: 4.176993094262514.\n",
      "[I 2023-10-27 12:27:28,426] A new study created in memory with name: RandomForest\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_squared_error: 28.7323\n",
      "rmse: 5.360255789860392\n",
      "mape: 27959.55600678128\n",
      "Study statistics for : \n",
      "  Number of finished trials:  1\n",
      "Best trial of city:  An Giang\n",
      "⭐️ City:  BR Vũng Tàu\n",
      "mean_squared_error: 121.1243\n",
      "rmse: 11.00564820384095\n",
      "mape: 2.5147025401791265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-27 12:27:28,624] Trial 0 finished with value: 7.537401765990689 and parameters: {'random_state': 36, 'n_estimators': 131, 'max_depth': 13}. Best is trial 0 with value: 7.537401765990689.\n",
      "[I 2023-10-27 12:27:28,631] A new study created in memory with name: RandomForest\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study statistics for : \n",
      "  Number of finished trials:  1\n",
      "Best trial of city:  BR Vũng Tàu\n",
      "⭐️ Nstep:  6\n",
      "⭐️ Model_name:  RandomForest\n",
      "⭐️ City:  An Giang\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-27 12:27:28,980] Trial 0 finished with value: 5.036410127138646 and parameters: {'random_state': 33, 'n_estimators': 187, 'max_depth': 13}. Best is trial 0 with value: 5.036410127138646.\n",
      "[I 2023-10-27 12:27:28,987] A new study created in memory with name: RandomForest\n",
      "[I 2023-10-27 12:27:29,108] Trial 0 finished with value: 9.733181115039953 and parameters: {'random_state': 10, 'n_estimators': 79, 'max_depth': 7}. Best is trial 0 with value: 9.733181115039953.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_squared_error: 40.9196\n",
      "rmse: 6.39684056164939\n",
      "mape: 60145.4629309903\n",
      "Study statistics for : \n",
      "  Number of finished trials:  1\n",
      "Best trial of city:  An Giang\n",
      "⭐️ City:  BR Vũng Tàu\n",
      "mean_squared_error: 152.8215\n",
      "rmse: 12.36210033301853\n",
      "mape: 3.1381712810440496\n",
      "Study statistics for : \n",
      "  Number of finished trials:  1\n",
      "Best trial of city:  BR Vũng Tàu\n"
     ]
    }
   ],
   "source": [
    "#########################\n",
    "# Main cell for optimize ML algorithm\n",
    "#########################\n",
    "\n",
    "model_name_list = [\n",
    "     \"RandomForest\",\n",
    "    #  \"LinearRegressionModel\",\n",
    "    #  \"LightGBMModel\",\n",
    "    #  \"CatBoostModel\",\n",
    "    #  \"XGBModel\",\n",
    "    # \"PoissonRegressor\",\n",
    "    # \"SVMRBF\"\n",
    "]\n",
    "\n",
    "\n",
    "# Lưu thông tin traceback study và error city trong quá trình optimize\n",
    "l_study_city ={}\n",
    "l_errCity =[]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  for nstep in range(1,args.n_predicted_period_months+1):\n",
    "    print(\"⭐️ Nstep: \",nstep)\n",
    "    lags_by_nstep = args.look_back + nstep - 1\n",
    "    lags_past_covariates_by_nstep = [-lags_by_nstep+2,-lags_by_nstep+1,-lags_by_nstep] #Mảng này chứa ba giá trị tương ứng cho args.lookback 3\n",
    "    lags_past_covariates_in_str = str(-lags_by_nstep+2)+\",\"+str(-lags_by_nstep+1)+\",\"+str(-lags_by_nstep)\n",
    "    for model_name in model_name_list: \n",
    "      print(\"⭐️ Model_name: \",model_name)\n",
    "      best_param = pd.DataFrame()\n",
    "      for city_index in range(len(cities)):\n",
    "        print(\"⭐️ City: \",cities[city_index])\n",
    "        # Use Tree-structured Parzen Estimator sampler to minimise RMSE\n",
    "        sampler = optuna.samplers.TPESampler()\n",
    "        study = optuna.create_study(sampler=sampler, direction='minimize', study_name = model_name)\n",
    "        # truyền multiple param vào trong biến trial\n",
    "        obj_func = lambda trial: objective(model_name, trial, cities[city_index], nstep = nstep)\n",
    "        try:\n",
    "          # Optimise over 100 trials\n",
    "          study.optimize(obj_func, n_trials=args.ntry, n_jobs=args.njob)\n",
    "\n",
    "          # Print results\n",
    "          print(\"Study statistics for : \")\n",
    "          print(\"  Number of finished trials: \", len(study.trials))\n",
    "          print(\"Best trial of city: \",cities[city_index])\n",
    "\n",
    "          best_trial = study.best_trial\n",
    "          # lưu best param vào trong biến toàn cục\n",
    "\n",
    "          if model_name == \"LinearRegressionModel\":\n",
    "            one_city_param = pd.DataFrame({\n",
    "                                'City':  cities[city_index],\n",
    "                                'Alg_name': 'LinearRegressionModel',\n",
    "                                'Best_value': best_trial.value,\n",
    "                                'n_try_opt': args.ntry,\n",
    "                                'lags' : lags_by_nstep,\n",
    "                                'lags_past_covariates': [lags_past_covariates_in_str],\n",
    "                                'output_chunk_length': 1,\n",
    "                                'random_state':best_trial.params['random_state'],\n",
    "                                })\n",
    "          elif model_name == 'XGBModel':\n",
    "            one_city_param = pd.DataFrame({\n",
    "                                'City':  cities[city_index],\n",
    "                                'Alg_name': 'XGBModel',\n",
    "                                'Best_value': best_trial.value,\n",
    "                                'n_try_opt': args.ntry,\n",
    "                                'lags' : lags_by_nstep,\n",
    "                                'lags_past_covariates': [lags_past_covariates_in_str],\n",
    "                                'output_chunk_length': 1,\n",
    "                                'random_state':best_trial.params['random_state'],\n",
    "                                'likelihood': best_trial.params['likelihood'],\n",
    "                                })\n",
    "          elif model_name == \"LightGBMModel\":\n",
    "            one_city_param = pd.DataFrame({\n",
    "                                'City':  cities[city_index],\n",
    "                                'Alg_name': 'LightGBMModel',\n",
    "                                'Best_value': best_trial.value,\n",
    "                                'n_try_opt': args.ntry,\n",
    "                                'lags': lags_by_nstep,\n",
    "                                'lags_past_covariates': [lags_past_covariates_in_str],\n",
    "                                'multi_models': best_trial.params['multi_models'],\n",
    "                                'num_leaves': best_trial.params['num_leaves'], \n",
    "                                'feature_fraction': best_trial.params['feature_fraction'], \n",
    "                                'min_child_samples': best_trial.params['min_child_samples'], \n",
    "                                'lambda_l1': best_trial.params['lambda_l1'], \n",
    "                                'lambda_l2': best_trial.params['lambda_l2'], \n",
    "                                'likelihood': best_trial.params['likelihood'], \n",
    "                                'learning_rate': best_trial.params['learning_rate']\n",
    "                                })\n",
    "          elif model_name == \"CatBoostModel\":\n",
    "            one_city_param = pd.DataFrame({\n",
    "                                'City':  cities[city_index],\n",
    "                                'Alg_name': 'CatBoost',\n",
    "                                'Best_value': best_trial.value,\n",
    "                                'n_try_opt': args.ntry,\n",
    "                                'lags' : lags_by_nstep,\n",
    "                                'lags_past_covariates': [lags_past_covariates_in_str],\n",
    "                                'output_chunk_length': 1,\n",
    "                                'likelihood': best_trial.params['likelihood'],\n",
    "                                'learning_rate': best_trial.params['learning_rate'],\n",
    "                                'n_estimators': best_trial.params['n_estimators'],\n",
    "                                'max_depth': best_trial.params['max_depth'],\n",
    "                                'bagging_temperature': best_trial.params['bagging_temperature'],\n",
    "                                'l2_leaf_reg': best_trial.params['l2_leaf_reg'],\n",
    "                                'random_strength':best_trial.params['random_strength'],\n",
    "                                })\n",
    "          elif model_name == \"RandomForest\":\n",
    "            one_city_param = pd.DataFrame({\n",
    "                                'City':  cities[city_index],\n",
    "                                'Alg_name': 'RandomForest',\n",
    "                                'Best_value': best_trial.value,\n",
    "                                'n_try_opt': args.ntry,\n",
    "                                'lags' : lags_by_nstep,\n",
    "                                'lags_past_covariates': [lags_past_covariates_in_str],\n",
    "                                'output_chunk_length': 1,\n",
    "                                'n_estimators': best_trial.params['n_estimators'],\n",
    "                                'max_depth': best_trial.params['max_depth'],\n",
    "                                'random_state':best_trial.params['random_state'],\n",
    "                                })\n",
    "          elif model_name == \"SVMRBF\":\n",
    "            one_city_param = pd.DataFrame({\n",
    "                                'City':  cities[city_index],\n",
    "                                'Alg_name': 'RandomForest',\n",
    "                                'Best_value': best_trial.value,\n",
    "                                'n_try_opt': args.ntry,\n",
    "                                'random_state':best_trial.params['random_state'],\n",
    "                                'kernel': 'rbf',\n",
    "                                'probability': True,\n",
    "                                })\n",
    "          elif model_name == \"PoissonRegressor\":\n",
    "            one_city_param = pd.DataFrame({\n",
    "                                'City':  cities[city_index],\n",
    "                                'Alg_name': 'RandomForest',\n",
    "                                'Best_value': best_trial.value,\n",
    "                                'n_try_opt': args.ntry,\n",
    "                                'max_iter':best_trial.params['max_iter'],\n",
    "                                })\n",
    "          # file_path = 'opt_results/opt_res_ml_26102023/261023_DF_opt_hyperparam_'+ model_name + '_'+str(nstep)+'-nstep.xlsx'\n",
    "          folder_path = f'opt_results/opt_res_ml_26102023/{model_name}/'\n",
    "          file_path = folder_path+ f'261023_DF_opt_hyperparam_{model_name}_{nstep}-nstep.xlsx'\n",
    "          if(os.path.isfile(file_path)):\n",
    "              with pd.ExcelWriter(file_path,mode=\"a\",engine=\"openpyxl\",if_sheet_exists=\"overlay\") as writer:\n",
    "                  one_city_param.to_excel(writer, header=None, startrow=city_index+1,index=False)\n",
    "          else:\n",
    "              if(not (os.path.isdir(folder_path))):\n",
    "                os.mkdir(folder_path)\n",
    "              with pd.ExcelWriter(file_path,engine=\"openpyxl\") as writer:\n",
    "                  one_city_param.to_excel(writer, startrow=city_index,index=False)\n",
    "        except:# có error thì lưu vào l_errCity để check lại sau \n",
    "          l_errCity.append(cities[city_index])\n",
    "          #send_to_telegram(f'Tỉnh bị lỗi trong quá trình optimize bằng model {model_name}: {cities[city_index]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# Main cell for optimize DL algorithm\n",
    "#########################\n",
    "\n",
    "model_name_list = [\n",
    "    #  \"BlockRNNModel\",\n",
    "    #  \"NBEATSModel\",\n",
    "    #  \"NHiTSModel\",\n",
    "    #  \"TFTModel\",\n",
    "    #  \"TCNModel\",\n",
    "    \n",
    "]\n",
    "\n",
    "# Lưu thông tin traceback study và error city trong quá trình optimize\n",
    "l_study_city ={}\n",
    "l_errCity =[]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for model_name in model_name_list: \n",
    "      print(\"⭐️ Model_name: \",model_name)\n",
    "      best_param = pd.DataFrame()\n",
    "      for city_index in range(len(cities)):\n",
    "        print(\"⭐️ City: \",cities[city_index])\n",
    "        # Use Tree-structured Parzen Estimator sampler to minimise RMSE\n",
    "        sampler = optuna.samplers.TPESampler()\n",
    "        study = optuna.create_study(sampler=sampler, direction='minimize', study_name = model_name)\n",
    "        # truyền multiple param vào trong biến trial\n",
    "        obj_func = lambda trial: objective(model_name, trial, cities[city_index], nstep = args.n_predicted_period_months)\n",
    "        try:\n",
    "          # Optimise over 100 trials\n",
    "          study.optimize(obj_func, n_trials=args.ntry, n_jobs=args.njob)\n",
    "\n",
    "          # Print results\n",
    "          print(\"Study statistics for : \")\n",
    "          print(\"  Number of finished trials: \", len(study.trials))\n",
    "          print(\"Best trial of city: \",cities[city_index])\n",
    "\n",
    "          best_trial = study.best_trial\n",
    "          # lưu best param vào trong biến toàn cục\n",
    "          if model_name == \"NHiTSModel\":\n",
    "            one_city_param = pd.DataFrame({\n",
    "                                'City':  cities[city_index],\n",
    "                                'Alg_name': 'N-HiTS',\n",
    "                                'Best_value': best_trial.value,\n",
    "                                'n_try_opt': args.ntry,\n",
    "                                'input_chunk_length' : lags_by_nstep,\n",
    "                                'output_chunk_length' : 1,\n",
    "                                'MaxPool1d' : best_trial.params['MaxPool1d'],\n",
    "                                'dropout' : best_trial.params['dropout'],\n",
    "                                'n_epochs' : best_trial.params['n_epochs'],\n",
    "                                'random_state' : best_trial.params['random_state'],\n",
    "                                })\n",
    "          elif model_name == \"TCNModel\":\n",
    "            one_city_param = pd.DataFrame({\n",
    "                                'City':  cities[city_index],\n",
    "                                'Alg_name': 'TCNModel',\n",
    "                                'Best_value': best_trial.value,\n",
    "                                'n_try_opt': args.ntry,\n",
    "                                'input_chunk_length': lags_by_nstep,\n",
    "                                'output_chunk_length': 1,\n",
    "                                'n_epochs':best_trial.params['n_epochs'],\n",
    "                                'num_filters':best_trial.params['num_filters'],\n",
    "                                'weight_norm':best_trial.params['weight_norm'],\n",
    "                                'dilation_base':best_trial.params['dilation_base'],\n",
    "                                'dropout':best_trial.params['dropout'],\n",
    "                                'learning_rate':best_trial.params['learning_rate'],\n",
    "                                'year':best_trial.params['year'],\n",
    "                                })\n",
    "          elif model_name == \"NBEATSModel\":\n",
    "            one_city_param = pd.DataFrame({\n",
    "                                'City':  cities[city_index],\n",
    "                                'Alg_name': 'NBeatsModel',\n",
    "                                'Best_value': best_trial.value,\n",
    "                                'n_try_opt': args.ntry,\n",
    "                                'output_chunk_length': 1,\n",
    "                                'input_chunk_length': lags_by_nstep,\n",
    "                                'n_epochs':best_trial.params['n_epochs'],\n",
    "                                'dropout':best_trial.params['dropout'],\n",
    "                                'random_state':best_trial.params['random_state'],\n",
    "                                })  \n",
    "          elif model_name == \"TFTModel\":\n",
    "            one_city_param = pd.DataFrame({\n",
    "                                'City':  cities[city_index],\n",
    "                                'Alg_name': 'TFTModel',\n",
    "                                'Best_value': best_trial.value,\n",
    "                                'n_try_opt': args.ntry,\n",
    "                                'output_chunk_length': 1,\n",
    "                                'input_chunk_length': lags_by_nstep,\n",
    "                                'add_relative_index': True,\n",
    "                                'random_state':best_trial.params['random_state'],\n",
    "                                'n_epochs':best_trial.params['n_epochs'],\n",
    "                                'dropout':best_trial.params['dropout']\n",
    "                                })\n",
    "          elif model_name == \"BlockRNNModel\":\n",
    "            one_city_param = pd.DataFrame({\n",
    "                                'City':  cities[city_index],\n",
    "                                'Alg_name': 'BlockRNNModel',\n",
    "                                'Best_value': best_trial.value,\n",
    "                                'n_try_opt': args.ntry,\n",
    "                                'input_chunk_length': lags_by_nstep,\n",
    "                                'output_chunk_length': 1,\n",
    "                                'random_state':best_trial.params['random_state'],\n",
    "                                'n_epochs':best_trial.params['n_epochs'],\n",
    "                                'n_rnn_layers': best_trial.params['n_rnn_layers'],\n",
    "                                'dropout':best_trial.params['dropout']\n",
    "                                })\n",
    "          # file_path = 'opt_results/opt_res_ml_26102023/261023_DF_opt_hyperparam_'+ model_name + '_'+str(nstep)+'-nstep.xlsx'\n",
    "          folder_path = f'opt_results/opt_res_ml_26102023/{model_name}/'\n",
    "          file_path = folder_path+ f'261023_DF_opt_hyperparam_{model_name}_{nstep}-nstep.xlsx'\n",
    "          if(os.path.isfile(file_path)):\n",
    "              with pd.ExcelWriter(file_path,mode=\"a\",engine=\"openpyxl\",if_sheet_exists=\"overlay\") as writer:\n",
    "                  one_city_param.to_excel(writer, header=None, startrow=city_index+1,index=False)\n",
    "          else:\n",
    "              if(not (os.path.isdir(folder_path))):\n",
    "                os.mkdir(folder_path)\n",
    "              with pd.ExcelWriter(file_path,engine=\"openpyxl\") as writer:\n",
    "                  one_city_param.to_excel(writer, startrow=city_index,index=False)\n",
    "        except:# có error thì lưu vào l_errCity để check lại sau \n",
    "          l_errCity.append(cities[city_index])\n",
    "          #send_to_telegram(f'Tỉnh bị lỗi trong quá trình optimize bằng model {model_name}: {cities[city_index]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send_to_telegram(\"Chạy xong optimize rùiii!!Vô check thuiii!!!\" )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
