{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1509,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "import joblib\n",
    "\n",
    "import torch\n",
    "\n",
    "from darts import TimeSeries\n",
    "from darts.models import  RandomForest, LinearRegressionModel, LightGBMModel, \\\n",
    "                        CatBoostModel, XGBModel,  BlockRNNModel, NBEATSModel, NHiTSModel, \\\n",
    "                        TCNModel, TFTModel\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import PoissonRegressor\n",
    "\n",
    "\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from darts.utils.likelihood_models import GaussianLikelihood\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1510,
   "metadata": {},
   "outputs": [],
   "source": [
    "prj_path = '../'\n",
    "data_path = prj_path + \"data/new_data/DH/squeezed/\"\n",
    "prj_path_opt= prj_path + \"optimize_hyperparam/opt_results/\"\n",
    "output_process = prj_path + \"data/new_data/DH/processed_data/\"\n",
    "output_featureselection = prj_path + \"data/new_data/DH/feature_selection/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1511,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cities = [\n",
    "        'An Giang', 'BR V≈©ng T√†u', 'B√¨nh Ph∆∞·ªõc', 'B√¨nh Thu·∫≠n', 'B√¨nh ƒê·ªãnh',\n",
    "        'B·∫°c Li√™u', 'B·∫Øc K·∫°n', 'B·∫Øc Giang', 'Cao B·∫±ng', 'C√† Mau',\n",
    "        'C·∫ßn Th∆°', 'Gia Lai', 'H√† Giang', 'H√† N·ªôi', 'H√† Tƒ©nh',\n",
    "        'H√≤a B√¨nh','H∆∞ng Y√™n', 'H·∫£i D∆∞∆°ng', 'H·∫£i Ph√≤ng', 'Kh√°nh H√≤a', 'Ki√™n Giang',\n",
    "        'Kon Tum', 'Lai Ch√¢u', 'Long An', 'L√†o Cai', 'L√¢m ƒê·ªìng',\n",
    "        'L·∫°ng S∆°n','Nam ƒê·ªãnh', 'Ngh·ªá An', 'Ninh B√¨nh', 'Ninh Thu·∫≠n',\n",
    "        'Ph√∫ Th·ªç', 'Ph√∫ Y√™n', 'Qu·∫£ng B√¨nh', 'Qu·∫£ng Nam', 'Qu·∫£ng Ng√£i',\n",
    "        'Qu·∫£ng Ninh', 'Qu·∫£ng Tr·ªã', 'S√≥c TrƒÉng', 'S∆°n La', 'TT Hu·∫ø',\n",
    "        'Thanh H√≥a', 'Th√°i B√¨nh', 'Th√°i Nguy√™n', 'Ti·ªÅn Giang', 'Tr√† Vinh',\n",
    "        'Tuy√™n Quang', 'T√¢y Ninh', 'Vƒ©nh Ph√∫c', 'Y√™n B√°i', 'ƒêi·ªán Bi√™n',\n",
    "        'ƒê√† N·∫µng', 'ƒê·∫Øk N√¥ng', 'ƒê·∫Øk L·∫Øk', 'ƒê·ªìng Th√°p'\n",
    "]\n",
    "cities = ['H√† N·ªôi','H·∫£i Ph√≤ng','Qu·∫£ng Ninh','Nam ƒê·ªãnh','Th√°i B√¨nh','Qu·∫£ng Nam','Qu·∫£ng Ng√£i', 'Ph√∫ Y√™n',\n",
    "          'Ninh Thu·∫≠n', 'B√¨nh Thu·∫≠n', 'T√¢y Ninh', 'B√¨nh Ph∆∞·ªõc', 'An Giang', 'Ti·ªÅn Giang','C·∫ßn Th∆°', 'Tr√† Vinh']\n",
    "cities = [ 'B√¨nh Ph∆∞·ªõc', 'An Giang','Qu·∫£ng Ninh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1512,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters as args using the Configuration class\n",
    "class Configuration():\n",
    "    def __init__(self):\n",
    "        # l·∫•y b·ªô test d√†i 36 th√°ng = 3 nƒÉm\n",
    "        self.test_size = 36\n",
    "        # l√† nh√¨n v√†o d·ªØ li·ªáu tr∆∞·ªõc 3 th√°ng v√† d·ª± ph√≥ng        \n",
    "        self.look_back = 3\n",
    "        # d·ª± ph√≥ng n-step trong 6 th√°ng\n",
    "        self.n_predicted_period_months = 6\n",
    "        self.n_features = 3\n",
    "        self.seed = 42\n",
    "        # m·ªói ph·∫ßn t·ª≠ x trong t·∫≠p suppervise c√≥ ƒë·ªô l·ªõn l√† 16 = 16 th√°ng\n",
    "        self.batch_size = 16\n",
    "        self.device = torch.device(\"cuda\")\n",
    "        self.epochs = 300\n",
    "        #others\n",
    "        self.labels = \"Dengue_fever_rates\"\n",
    "        # Input param for Optimize Run\n",
    "        self.ntry = 1\n",
    "        self.njob = 1\n",
    "\n",
    "args = Configuration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1513,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    import random\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    \n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "seed_everything(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supporting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1514,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def send_to_telegram(message):\n",
    "\n",
    "    apiToken = '5908735099:AAGVSLrW62aXPBP-GrMvxoVgMsuJxXJpP1Q'\n",
    "    chatID = '@ptn_announcement'\n",
    "    apiURL = f'https://api.telegram.org/bot{apiToken}/sendMessage'\n",
    "\n",
    "    try:\n",
    "        response = requests.post(apiURL, json={'chat_id': chatID, 'text': message})\n",
    "        print(response.text)\n",
    "    except Exception as e:\n",
    "        message_error = \"B·ªã l·ªói r√πi: \"+str(e)\n",
    "        response = requests.post(apiURL, json={'chat_id': chatID, 'text': message_error})\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1515,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict_all_city_data():\n",
    "  cities_data = {}  \n",
    "  for city in cities:\n",
    "    city_result = pd.read_excel(prj_path+'data/new_data/DH/squeezed/squeezed_'+city+'.xlsx')  \n",
    "    \"\"\"Get all data from all city in 1997 - 2016\"\"\" \n",
    "    city_result = city_result.loc[city_result['year_month'] < '2017-1-1'] \n",
    "    cities_data[city] = city_result\n",
    "  return cities_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1516,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data (pre-)processing functions\n",
    "# modification\n",
    "def get_city_data(city_name, dict_full_data):\n",
    "    \"\"\"Returns Dengue fever rate and climate data\"\"\" \n",
    "    city_data = dict_full_data[city_name].drop(columns=['Diarrhoea_cases','Diarrhoea_rates', 'province',\n",
    "                                                        'Influenza_rates','Influenza_cases',\n",
    "                                                        'Dengue_fever_cases', 'year', 'month'], \n",
    "                                                                  axis=1, \n",
    "                                                                  inplace=False)    \n",
    "    return city_data\n",
    "\n",
    "def convert_to_stationary(city_data):\n",
    "    \"\"\"Subtracts previous value for all cols except disease rates\"\"\"\n",
    "    for col_name in city_data.columns:\n",
    "        if col_name != 'Dengue_fever_rates':\n",
    "            try:\n",
    "                city_data[col_name] = city_data[col_name] - city_data[col_name].shift()\n",
    "            except:\n",
    "                print(col_name)\n",
    "    return city_data\n",
    "\n",
    "def impute_missing_value(city_data):\n",
    "    \"\"\"\n",
    "    Imputes 0 for first 12 months, \n",
    "    last year's value for months 12-24, \n",
    "    and minimum value of last two years for months 25+\n",
    "    \"\"\"\n",
    "    for col in city_data.columns:\n",
    "        for index in range(len(city_data[col])):\n",
    "            if np.isnan(city_data[col].iloc[index]):\n",
    "                if index < 12:\n",
    "                    city_data[col].iloc[index] = 0\n",
    "                elif index >= 12 and index <= 24:\n",
    "                    city_data[col].iloc[index] = city_data[col].iloc[index - 12]\n",
    "                else:\n",
    "                    city_data[col].iloc[index] = min(city_data[col].iloc[index - 12], city_data[col].iloc[index - 24])\n",
    "    return city_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1517,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_full_data(dict_full_data):\n",
    "    climate_and_disease_feats = ['Total_Evaporation',\n",
    "       'Total_Rainfall', 'Max_Daily_Rainfall', 'n_raining_days',\n",
    "       'Average_temperature', 'Max_Average_Temperature',\n",
    "       'Min_Average_Temperature', 'Max_Absolute_Temperature',\n",
    "       'Min_Absolute_Temperature', 'Average_Humidity', 'Min_Humidity',\n",
    "       'n_hours_sunshine', 'Dengue_fever_rates']\n",
    "    for city in cities:\n",
    "        city_data = get_city_data(city_name=city,dict_full_data = dict_full_data)\n",
    "        city_data_features = city_data[climate_and_disease_feats]\n",
    "        city_data_features = impute_missing_value(city_data_features)\n",
    "        city_data_features = convert_to_stationary(city_data_features)\n",
    "        city_data_features.dropna(inplace=True)\n",
    "        city_data_features.loc[:, \"year_month\"] = city_data[\"year_month\"]\n",
    "        dict_full_data[city] = city_data_features\n",
    "    return dict_full_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1518,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, look_back, n_nextstep = args.n_predicted_period_months):\n",
    "    \"\"\"Splits data into train and test sets based on args (Configuration class)\"\"\"\n",
    "    train = data[: -args.test_size]    \n",
    "    test = data[-args.test_size - look_back-(n_nextstep - 1): ]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1519,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_supervised(data,  d_out, d_in, features_list=[]):\n",
    "    \"\"\"\n",
    "    Frames time-series as supervised learning dataset.\n",
    "    \n",
    "    Args:\n",
    "      d_in: lookback window\n",
    "      d_out: number of predicted months\n",
    "      features_list: list of all features **where last col is the disease incidence**\n",
    "\n",
    "    Returns:\n",
    "      Numpy arrays of disease incidence (y) and other predictors (X)\n",
    "    \"\"\"\n",
    "    X, y = list(), list()\n",
    "    for index, _ in enumerate(data):\n",
    "        in_end = index + d_in\n",
    "        out_end = in_end + d_out\n",
    "        if out_end <= len(data):\n",
    "            if len(features_list) == 0 :\n",
    "                X.append(data[index: in_end, :-1])\n",
    "            else:\n",
    "                X.append(data[index: in_end, features_list])\n",
    "            y.append(data[out_end-1: out_end, -1])\n",
    "    return np.array(X), np.array(y).reshape(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1520,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(df_train, df_eval, model, feature_list , labels, scaler, is_dl_algo, is_sklearn_model,nstep):\n",
    "  \"\"\"\n",
    "  $df: pandas.DataFrame object containing data for training and testing model:\n",
    "  $model: darts model object\n",
    "  $feature_list: Names of the features used as model input\n",
    "  $label: the value the model will be trained to predict\n",
    "  $scaler: scaler object. Note: the scaler will be fitted on training data and applied to test data\n",
    "  $lags: how much to look back into the past to output prediction\n",
    "  $split_index: the point at which to divide train and test_data\n",
    "\n",
    "  \"\"\"\n",
    "  \n",
    "\n",
    "  if is_dl_algo == 1:\n",
    "    print(\"üçã\")\n",
    "  else:\n",
    "    if is_sklearn_model == 1:\n",
    "      \n",
    "      train = df_train[feature_list+[args.labels]].iloc[:,:].to_numpy()\n",
    "      test = df_eval[feature_list+[args.labels]].iloc[:,:].to_numpy()\n",
    "      x_train,y_train = to_supervised(train, d_out=nstep, d_in=args.look_back )\n",
    "      x_train = x_train.reshape(len(x_train),x_train.shape[1]*x_train.shape[2])\n",
    "      x_test,y_test = to_supervised(test, d_out=nstep, d_in=args.look_back )\n",
    "      x_test = x_test.reshape(len(x_test),x_test.shape[1]*x_test.shape[2])\n",
    "      \n",
    "      model = model.fit(x_train,y_train)\n",
    "      prediction = model.predict(x_test)\n",
    "      \n",
    "      df_eval_true_inverse = df_eval_pred_inverse = df_eval[-args.test_size:]\n",
    "      y_true = scaler.inverse_transform(df_eval_true_inverse.iloc[:,:-1])[:,[-1]].reshape(len(prediction))\n",
    "\n",
    "      df_eval_pred_inverse[labels]= prediction\n",
    "      y_pred = scaler.inverse_transform(df_eval_pred_inverse.iloc[:,:-1])[:,[-1]].reshape(len(prediction))  \n",
    "\n",
    "    else:\n",
    "      x_train = TimeSeries.from_dataframe(df_train, \"year_month\", feature_list)\n",
    "      y_train = TimeSeries.from_dataframe(df_train, \"year_month\", labels)\n",
    "\n",
    "      x_test = TimeSeries.from_dataframe(df_eval, \"year_month\", feature_list)\n",
    "      y_test = TimeSeries.from_dataframe(df_eval, \"year_month\", labels)\n",
    "\n",
    "      model.fit(y_train, past_covariates = x_train)\n",
    "      prediction = model.predict(args.test_size, past_covariates = x_test, num_samples=1)\n",
    "\n",
    "      df_eval_true_inverse = df_eval_pred_inverse = df_eval[-args.test_size:]\n",
    "      y_true = scaler.inverse_transform(df_eval_true_inverse.iloc[:,:-1])[:,[-1]].reshape(len(prediction))\n",
    "\n",
    "      df_eval_pred_inverse[labels]= np.array(prediction._xa).squeeze()\n",
    "      y_pred = scaler.inverse_transform(df_eval_pred_inverse.iloc[:,:-1])[:,[-1]].reshape(len(prediction))\n",
    "\n",
    "\n",
    "    # df_compare_test_predict = pd.DataFrame({'y_true':y_true, 'y_pred':y_pred})\n",
    "    # df_compare_test_predict.plot()\n",
    "    # plt.legend()\n",
    "    # plt.show()\n",
    "\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = mse**0.5\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    print(f\"mean_squared_error: {mse:.4f}\")\n",
    "    print(f\"rmse: {rmse}\")\n",
    "    print(f\"mape: {mape}\")\n",
    "    return model, y_true, y_pred, mse, mae, rmse, mape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1521,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_prediction_for_location(df_train, df_eval, model, location, feature_list, \n",
    "                                                labels, scaler, is_dl_algo, is_sklearn_model ,nstep):\n",
    "    \"\"\"train and generate prediction for a province\n",
    "    df: DataFrame object containing features and label(s) for training model\n",
    "    localtion: location_name\n",
    "    feature_list: list of features used as model input,  must be among the column names of df\n",
    "    labels: the values model will be trained to predict\n",
    "    scaler: sklearn scaler object\n",
    "    lags: how long into the past to look back when making prediction\n",
    "    split_index: the point at which to divide data into the train and test subsets.\n",
    "    \"\"\"\n",
    "    model, y_true, prediction_inverse, mse, mae, rmse, mape = train_and_evaluate(df_train, df_eval, model, feature_list, labels, scaler,is_dl_algo, is_sklearn_model,nstep)\n",
    "    df_prediction = pd.DataFrame({\"Date\": df_eval[\"year_month\"][-len(prediction_inverse):],\n",
    "                                  \"Observed\": y_true[-len(prediction_inverse):],\n",
    "                                  \"1-month\": prediction_inverse})\n",
    "    \n",
    "    df_prediction[\"City\"] = location\n",
    "    df_prediction[\"RMSE_1-month\"] = rmse\n",
    "    df_prediction[\"MAE_1-month\"] = mae\n",
    "    df_prediction[\"MAPE_1-month\"] = mape\n",
    "\n",
    "    return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1522,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataWithSelectedFeature(city, next_predicted_month):\n",
    "  selected_feature = []\n",
    "  df = pd.read_csv(output_featureselection+str(next_predicted_month)+\"step_feature_selection_3_most.csv\")\n",
    "  for row in range(len(df)):\n",
    "    if (df[\"City\"][row] == city):\n",
    "      selected_feature.append(df[\"1st_Feature\"][row])\n",
    "      selected_feature.append(df[\"2nd_Feature\"][row])\n",
    "      selected_feature.append(df[\"3rd_Feature\"][row])\n",
    "  return selected_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective and Suggest Hyperparams of Darts Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1523,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(model_name, trial, city, nstep):   \n",
    "    specific_data = pd.read_csv(output_process+city+'_train_preprocessed.csv', parse_dates=True, index_col= None, encoding = 'unicode_escape')\n",
    "    scaler = joblib.load(output_process+city+'_train_scalerMinMaxNorm.save') #ok\n",
    "\n",
    "    df_train, df_valid = split_data(specific_data, args.look_back,nstep)\n",
    "\n",
    "    selected_features = getDataWithSelectedFeature(city, nstep)\n",
    "\n",
    "    lags_by_nstep = args.look_back + nstep - 1\n",
    "    lags_past_covariates_by_nstep = [-lags_by_nstep+2,-lags_by_nstep+1,-lags_by_nstep] #M·∫£ng n√†y ch·ª©a ba gi√° tr·ªã t∆∞∆°ng ·ª©ng cho args.lookback 3\n",
    "    is_dl_algo = 0\n",
    "    is_sklearn_model = 0\n",
    "\n",
    "    pl_trainer_kwargs = {\n",
    "              \"accelerator\": \"cpu\",\n",
    "              # \"devices\": -1,\n",
    "              # \"auto_select_gpus\": True,\n",
    "          }\n",
    "\n",
    "    if model_name == \"RandomForest\":\n",
    "      random_state = trial.suggest_int('random_state', 0, 42)\n",
    "      n_estimators = trial.suggest_int('n_estimators', 50, 200)\n",
    "      max_depth = trial.suggest_int('max_depth', 1, 15)\n",
    "      # Create the RandomForest model\n",
    "      model = RandomForest(\n",
    "                    lags = lags_by_nstep,\n",
    "                    lags_past_covariates = lags_past_covariates_by_nstep,\n",
    "                    output_chunk_length = 1,\n",
    "                    n_estimators = n_estimators,\n",
    "                    max_depth = max_depth,\n",
    "                    random_state=random_state)\n",
    "    elif model_name == 'XGBModel':\n",
    "      random_state = trial.suggest_int('random_state', 0, 43)\n",
    "      likelihood = trial.suggest_categorical('likelihood', ['quantile'])\n",
    "      # Create the  model\n",
    "      model = XGBModel(\n",
    "                      lags = lags_by_nstep,\n",
    "                      lags_past_covariates = lags_past_covariates_by_nstep,\n",
    "                      output_chunk_length = 1,\n",
    "                      random_state=random_state,\n",
    "                      likelihood = likelihood\n",
    "                    )\n",
    "    elif model_name == 'LinearRegressionModel':\n",
    "      random_state = trial.suggest_int('random_state', 0, 43)\n",
    "      # Create the  model\n",
    "      model = LinearRegressionModel(\n",
    "                      lags = lags_by_nstep,\n",
    "                      lags_past_covariates = lags_past_covariates_by_nstep,\n",
    "                      output_chunk_length = 1,\n",
    "                      random_state=random_state)\n",
    "    elif model_name == \"CatBoostModel\":\n",
    "      #suggest hyperparams\n",
    "      learning_rate = trial.suggest_float('learning_rate', 0.001, 0.1)\n",
    "      n_estimators = trial.suggest_int('n_estimators', 50, 200)\n",
    "      max_depth = trial.suggest_int('max_depth', 1, 15)\n",
    "      random_state = trial.suggest_int('random_state', 0, 1000)\n",
    "      likelihood = trial.suggest_categorical('likelihood', ['quantile'])\n",
    "      quantiles =  trial.suggest_categorical('quantiles', [None, [0.1, 0.5, 0.9]])\n",
    "      bagging_temperature = trial.suggest_float('bagging_temperature', 0.01, 100.0)\n",
    "      border_count = trial.suggest_int('border_count', 1, 255)\n",
    "      l2_leaf_reg = trial.suggest_float('l2_leaf_reg', 0.1, 10)\n",
    "      random_strength = trial.suggest_float('random_strength', 0.1, 10)\n",
    "      model = CatBoostModel(\n",
    "                            lags=lags_by_nstep,\n",
    "                            lags_past_covariates=lags_past_covariates_by_nstep, \n",
    "                            learning_rate=learning_rate,\n",
    "                            n_estimators=n_estimators,\n",
    "                            max_depth=max_depth, \n",
    "                            output_chunk_length = 1,\n",
    "                            likelihood = likelihood,\n",
    "                            quantiles = quantiles,\n",
    "                            bagging_temperature = bagging_temperature,\n",
    "                            border_count = border_count,\n",
    "                            l2_leaf_reg = l2_leaf_reg,\n",
    "                            random_strength = random_strength,\n",
    "                            random_state=random_state)\n",
    "    elif model_name == \"LightGBMModel\":\n",
    "      params = {\n",
    "        \"lags\": lags_by_nstep,\n",
    "        \"lags_past_covariates\": lags_past_covariates_by_nstep,\n",
    "        \"random_state\": trial.suggest_int(\"random_state\", 0, 999),\n",
    "        \"multi_models\": trial.suggest_categorical(\"multi_models\", [True, False]),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
    "        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.1, 1.0),\n",
    "        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.1, 1.0),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n",
    "        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n",
    "        'verbose': -1,\n",
    "        'likelihood' : trial.suggest_categorical(\"likelihood\", [\"quantile\"])\n",
    "      }\n",
    "\n",
    "      param = params\n",
    "      model = LightGBMModel(\n",
    "          lags = param['lags'],\n",
    "          lags_past_covariates = param['lags_past_covariates'],\n",
    "          output_chunk_length = 1,\n",
    "          random_state = param['random_state'],\n",
    "          multi_models = param['multi_models'],\n",
    "          likelihood = param['likelihood'],\n",
    "          num_leaves = param['num_leaves'],\n",
    "          learning_rate = param['learning_rate'],\n",
    "          feature_fraction = param['feature_fraction'],\n",
    "          bagging_fraction = param['bagging_fraction'],\n",
    "          min_child_samples = param['min_child_samples'],\n",
    "          lambda_l1 = param['lambda_l1'],\n",
    "          verbose = param['verbose']\n",
    "      )\n",
    "    elif model_name == \"SVMRBF\":\n",
    "      max_iter = trial.suggest_int('max_iter', 50, 200)\n",
    "      epsilon = trial.suggest_loguniform('epsilon', 1e-8, 0.1),\n",
    "      model = SVR(\n",
    "          max_iter = max_iter,\n",
    "          kernel = 'rbf',\n",
    "          epsilon = epsilon[0]\n",
    "      )\n",
    "      is_sklearn_model = 1\n",
    "    elif model_name == \"PoissonRegressor\":\n",
    "      max_iter = trial.suggest_int('max_iter', 50, 200)\n",
    "      alpha = trial.suggest_loguniform('alpha', 1e-8, 10.0),\n",
    "      model = PoissonRegressor(\n",
    "          max_iter = max_iter,\n",
    "          fit_intercept = False,\n",
    "          alpha = alpha[0]\n",
    "      )\n",
    "      is_sklearn_model = 1\n",
    "    elif model_name == \"BlockRNNModel\":\n",
    "      #suggest hyperparams\n",
    "      random_state = trial.suggest_int('random_state', 0, 1000)\n",
    "      n_rnn_layers = trial.suggest_int('n_rnn_layers', 1, 3)\n",
    "      dropout = trial.suggest_uniform('dropout', 0.1, 0.5)\n",
    "      hidden_dim = trial.suggest_int('n_rnn_layers', 5, 20)\n",
    "      n_epochs = trial.suggest_int('n_epochs', 50, 200)\n",
    "\n",
    "      model = BlockRNNModel(\n",
    "                          input_chunk_length = args.look_back,\n",
    "                          output_chunk_length = args.n_predicted_period_months,\n",
    "                          hidden_dim = hidden_dim,\n",
    "                          n_rnn_layers = n_rnn_layers,\n",
    "                          dropout = dropout,\n",
    "                          n_epochs = n_epochs,\n",
    "                          pl_trainer_kwargs = pl_trainer_kwargs,\n",
    "                          random_state=random_state)\n",
    "      is_dl_algo = 1\n",
    "    elif model_name == 'TFTModel':\n",
    "      # Define the hyperparameters to optimize\n",
    "      random_state = trial.suggest_int('random_state', 0, 42)\n",
    "      dropout = trial.suggest_uniform('dropout', 0.01, 0.8)\n",
    "      n_epochs = trial.suggest_int('n_epochs', 50, 200)\n",
    "\n",
    "      # Create the TFTModel model\n",
    "      model = TFTModel(\n",
    "                    input_chunk_length = args.look_back,\n",
    "                    output_chunk_length = args.n_predicted_period_months,\n",
    "                    add_relative_index = True,\n",
    "                    dropout = dropout,\n",
    "                    n_epochs = n_epochs ,\n",
    "                    random_state=random_state)\n",
    "      is_dl_algo = 1\n",
    "    elif model_name == 'NHiTSModel':\n",
    "      #suggest hyperparams\n",
    "      random_state = trial.suggest_int('random_state', 0, 42)\n",
    "      dropout = trial.suggest_uniform('dropout', 0.01, 0.80)\n",
    "      n_epochs = trial.suggest_int('n_epochs', 100, 500, step=10)\n",
    "      MaxPool1d = trial.suggest_categorical('MaxPool1d', [True, False])\n",
    "\n",
    "      model = NHiTSModel(\n",
    "                          input_chunk_length = args.look_back,\n",
    "                          output_chunk_length = args.n_predicted_period_months,\n",
    "                          MaxPool1d = MaxPool1d,\n",
    "                          dropout = dropout,\n",
    "                          n_epochs = n_epochs ,\n",
    "                          pl_trainer_kwargs = pl_trainer_kwargs,\n",
    "                          random_state=random_state)\n",
    "      is_dl_algo = 1\n",
    "    elif model_name == \"NBEATSModel\":\n",
    "      random_state = trial.suggest_int('random_state', 0, 42)\n",
    "      dropout = trial.suggest_uniform('dropout', 0.01, 0.80)\n",
    "      n_epochs = trial.suggest_int('n_epochs', 50, 200)\n",
    "      model = NBEATSModel(\n",
    "                            input_chunk_length = args.look_back,\n",
    "                            output_chunk_length = args.n_predicted_period_months,\n",
    "                            dropout = dropout,\n",
    "                            n_epochs = n_epochs ,\n",
    "                            pl_trainer_kwargs = pl_trainer_kwargs,\n",
    "                            random_state=random_state)\n",
    "      is_dl_algo = 1\n",
    "    elif model_name == \"TCNModel\":\n",
    "      params = {\n",
    "        'kernel_size': trial.suggest_int(\"kernel_size\", 2, lags_by_nstep),\n",
    "        'num_filters': trial.suggest_int(\"num_filters\", 1, 5),\n",
    "        'weight_norm': trial.suggest_categorical(\"weight_norm\", [False, True]),\n",
    "        'dilation_base': trial.suggest_int(\"dilation_base\", 2, 4),\n",
    "        'dropout': trial.suggest_float(\"dropout\", 0.0, 0.4),\n",
    "        'learning_rate': trial.suggest_float(\"learning_rate\", 5e-5, 1e-3, log=True),\n",
    "        'include_year': trial.suggest_categorical(\"year\", [False, True]),\n",
    "        'n_epochs': trial.suggest_int(\"n_epochs\", 100, 300),\n",
    "      }\n",
    "      # select input and output chunk lengths\n",
    "      params['input_chunk_length'] = args.look_back\n",
    "      params['output_chunk_length'] = args.n_predicted_period_months  \n",
    "      # optionally also add the (scaled) year value as a past covariate\n",
    "      if params['include_year']:\n",
    "          encoders = {\"datetime_attribute\": {\"past\": [\"year\"]},\n",
    "                      \"transformer\": Scaler()}\n",
    "      else:\n",
    "          encoders = None\n",
    "      params['encoders'] = encoders\n",
    "      param = params\n",
    "      model = TCNModel(\n",
    "          input_chunk_length=param['input_chunk_length'],\n",
    "          output_chunk_length=param['output_chunk_length'],\n",
    "          batch_size=16,\n",
    "          n_epochs=param['n_epochs'],\n",
    "          nr_epochs_val_period=1,\n",
    "          kernel_size=param['kernel_size'],\n",
    "          num_filters=param['num_filters'],\n",
    "          weight_norm=param['weight_norm'],\n",
    "          dilation_base=param['dilation_base'],\n",
    "          dropout=param['dropout'],\n",
    "          optimizer_kwargs={\"lr\": param['learning_rate']},\n",
    "          add_encoders=param['encoders'],\n",
    "          likelihood=GaussianLikelihood(),\n",
    "          pl_trainer_kwargs=pl_trainer_kwargs,\n",
    "          model_name=\"tcn_model\",\n",
    "          force_reset=True,\n",
    "          save_checkpoints=True,\n",
    "      )\n",
    "      is_dl_algo = 1\n",
    "    \n",
    "    mae_error = output_prediction_for_location(df_train, df_valid, model, location=city, feature_list=selected_features,\n",
    "                                                labels=args.labels, scaler=scaler, is_dl_algo = is_dl_algo, is_sklearn_model = is_sklearn_model, nstep = nstep)\n",
    "\n",
    "    return mae_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main run optimize and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1524,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-28 16:19:01,516] A new study created in memory with name: SVMRBF\n",
      "[I 2023-10-28 16:19:01,528] Trial 0 finished with value: 15.696544276983452 and parameters: {'max_iter': 99, 'epsilon': 3.220707204408642e-07}. Best is trial 0 with value: 15.696544276983452.\n",
      "[I 2023-10-28 16:19:01,538] A new study created in memory with name: SVMRBF\n",
      "[I 2023-10-28 16:19:01,544] Trial 0 finished with value: 11.00575874376712 and parameters: {'max_iter': 97, 'epsilon': 0.05220681234300323}. Best is trial 0 with value: 11.00575874376712.\n",
      "[I 2023-10-28 16:19:01,551] A new study created in memory with name: SVMRBF\n",
      "[I 2023-10-28 16:19:01,558] Trial 0 finished with value: 2.2245726032287214 and parameters: {'max_iter': 168, 'epsilon': 0.00033165825982012784}. Best is trial 0 with value: 2.2245726032287214.\n",
      "[I 2023-10-28 16:19:01,564] A new study created in memory with name: PoissonRegressor\n",
      "[I 2023-10-28 16:19:01,572] Trial 0 finished with value: 27.06545700577043 and parameters: {'max_iter': 193, 'alpha': 0.05972853334443229}. Best is trial 0 with value: 27.06545700577043.\n",
      "[I 2023-10-28 16:19:01,578] A new study created in memory with name: PoissonRegressor\n",
      "[I 2023-10-28 16:19:01,585] Trial 0 finished with value: 46.82193333809066 and parameters: {'max_iter': 107, 'alpha': 2.155237350358001}. Best is trial 0 with value: 46.82193333809066.\n",
      "[I 2023-10-28 16:19:01,591] A new study created in memory with name: PoissonRegressor\n",
      "[I 2023-10-28 16:19:01,598] Trial 0 finished with value: 38.878321622364446 and parameters: {'max_iter': 200, 'alpha': 9.107859301974372}. Best is trial 0 with value: 38.878321622364446.\n",
      "[I 2023-10-28 16:19:01,605] A new study created in memory with name: SVMRBF\n",
      "[I 2023-10-28 16:19:01,612] Trial 0 finished with value: 16.90574708181343 and parameters: {'max_iter': 153, 'epsilon': 6.399157105058883e-05}. Best is trial 0 with value: 16.90574708181343.\n",
      "[I 2023-10-28 16:19:01,617] A new study created in memory with name: SVMRBF\n",
      "[I 2023-10-28 16:19:01,624] Trial 0 finished with value: 9.563975531727042 and parameters: {'max_iter': 67, 'epsilon': 1.652031107044852e-05}. Best is trial 0 with value: 9.563975531727042.\n",
      "[I 2023-10-28 16:19:01,630] A new study created in memory with name: SVMRBF\n",
      "[I 2023-10-28 16:19:01,637] Trial 0 finished with value: 1.9138237782740077 and parameters: {'max_iter': 177, 'epsilon': 0.0007071292384633428}. Best is trial 0 with value: 1.9138237782740077.\n",
      "[I 2023-10-28 16:19:01,643] A new study created in memory with name: PoissonRegressor\n",
      "[I 2023-10-28 16:19:01,651] Trial 0 finished with value: 25.745968040576955 and parameters: {'max_iter': 104, 'alpha': 0.004561285555992682}. Best is trial 0 with value: 25.745968040576955.\n",
      "[I 2023-10-28 16:19:01,657] A new study created in memory with name: PoissonRegressor\n",
      "[I 2023-10-28 16:19:01,663] Trial 0 finished with value: 15.114654549659901 and parameters: {'max_iter': 140, 'alpha': 0.09911032299371612}. Best is trial 0 with value: 15.114654549659901.\n",
      "[I 2023-10-28 16:19:01,670] A new study created in memory with name: PoissonRegressor\n",
      "[I 2023-10-28 16:19:01,679] Trial 0 finished with value: 2.1798742887166034 and parameters: {'max_iter': 95, 'alpha': 0.0008960627408349147}. Best is trial 0 with value: 2.1798742887166034.\n",
      "[I 2023-10-28 16:19:01,685] A new study created in memory with name: SVMRBF\n",
      "[I 2023-10-28 16:19:01,692] Trial 0 finished with value: 19.67542642329369 and parameters: {'max_iter': 75, 'epsilon': 1.0584184011465827e-08}. Best is trial 0 with value: 19.67542642329369.\n",
      "[I 2023-10-28 16:19:01,697] A new study created in memory with name: SVMRBF\n",
      "[I 2023-10-28 16:19:01,706] Trial 0 finished with value: 9.149560650762606 and parameters: {'max_iter': 198, 'epsilon': 0.018697434807316445}. Best is trial 0 with value: 9.149560650762606.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è Nstep:  1\n",
      "‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è Model_name:  SVMRBF\n",
      "‚≠êÔ∏è‚≠êÔ∏è City:  B√¨nh Ph∆∞·ªõc\n",
      "mean_squared_error: 646.0508\n",
      "rmse: 25.417529361575472\n",
      "mape: 0.726929707034981\n",
      "Study statistics for : \n",
      "  Number of finished trials:  1\n",
      "Best trial of city:  B√¨nh Ph∆∞·ªõc\n",
      "‚≠êÔ∏è‚≠êÔ∏è City:  An Giang\n",
      "mean_squared_error: 185.2373\n",
      "rmse: 13.61018974313629\n",
      "mape: 1.2264956844980732\n",
      "Study statistics for : \n",
      "  Number of finished trials:  1\n",
      "Best trial of city:  An Giang\n",
      "‚≠êÔ∏è‚≠êÔ∏è City:  Qu·∫£ng Ninh\n",
      "mean_squared_error: 8.4531\n",
      "rmse: 2.907421849857246\n",
      "mape: 1953941763756889.0\n",
      "Study statistics for : \n",
      "  Number of finished trials:  1\n",
      "Best trial of city:  Qu·∫£ng Ninh\n",
      "‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è Model_name:  PoissonRegressor\n",
      "‚≠êÔ∏è‚≠êÔ∏è City:  B√¨nh Ph∆∞·ªõc\n",
      "mean_squared_error: 985.9641\n",
      "rmse: 31.400064510011063\n",
      "mape: 2.1868770407367766\n",
      "Study statistics for : \n",
      "  Number of finished trials:  1\n",
      "Best trial of city:  B√¨nh Ph∆∞·ªõc\n",
      "üê∞B·ª•t ƒê√¢y!!\n",
      "PARAMS:  {'max_iter': 193, 'alpha': 0.05972853334443229}\n",
      "‚≠êÔ∏è‚≠êÔ∏è City:  An Giang\n",
      "mean_squared_error: 2277.9660\n",
      "rmse: 47.728041744055126\n",
      "mape: 5.610080188455412\n",
      "Study statistics for : \n",
      "  Number of finished trials:  1\n",
      "Best trial of city:  An Giang\n",
      "üê∞B·ª•t ƒê√¢y!!\n",
      "PARAMS:  {'max_iter': 107, 'alpha': 2.155237350358001}\n",
      "‚≠êÔ∏è‚≠êÔ∏è City:  Qu·∫£ng Ninh\n",
      "mean_squared_error: 1512.9469\n",
      "rmse: 38.896617719784985\n",
      "mape: 3.993264822221659e+16\n",
      "Study statistics for : \n",
      "  Number of finished trials:  1\n",
      "Best trial of city:  Qu·∫£ng Ninh\n",
      "üê∞B·ª•t ƒê√¢y!!\n",
      "PARAMS:  {'max_iter': 200, 'alpha': 9.107859301974372}\n",
      "‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è Nstep:  2\n",
      "‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è Model_name:  SVMRBF\n",
      "‚≠êÔ∏è‚≠êÔ∏è City:  B√¨nh Ph∆∞·ªõc\n",
      "mean_squared_error: 573.9536\n",
      "rmse: 23.957329447498545\n",
      "mape: 0.8297642935616311\n",
      "Study statistics for : \n",
      "  Number of finished trials:  1\n",
      "Best trial of city:  B√¨nh Ph∆∞·ªõc\n",
      "‚≠êÔ∏è‚≠êÔ∏è City:  An Giang\n",
      "mean_squared_error: 133.3369\n",
      "rmse: 11.547160225494085\n",
      "mape: 0.9325641943716967\n",
      "Study statistics for : \n",
      "  Number of finished trials:  1\n",
      "Best trial of city:  An Giang\n",
      "‚≠êÔ∏è‚≠êÔ∏è City:  Qu·∫£ng Ninh\n",
      "mean_squared_error: 6.7489\n",
      "rmse: 2.597867451860648\n",
      "mape: 1365260060152737.2\n",
      "Study statistics for : \n",
      "  Number of finished trials:  1\n",
      "Best trial of city:  Qu·∫£ng Ninh\n",
      "‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è Model_name:  PoissonRegressor\n",
      "‚≠êÔ∏è‚≠êÔ∏è City:  B√¨nh Ph∆∞·ªõc\n",
      "mean_squared_error: 1083.1550\n",
      "rmse: 32.9113203639496\n",
      "mape: 1.823808817746225\n",
      "Study statistics for : \n",
      "  Number of finished trials:  1\n",
      "Best trial of city:  B√¨nh Ph∆∞·ªõc\n",
      "üê∞B·ª•t ƒê√¢y!!\n",
      "PARAMS:  {'max_iter': 104, 'alpha': 0.004561285555992682}\n",
      "‚≠êÔ∏è‚≠êÔ∏è City:  An Giang\n",
      "mean_squared_error: 297.0867\n",
      "rmse: 17.23620386737385\n",
      "mape: 2.087969267335193\n",
      "Study statistics for : \n",
      "  Number of finished trials:  1\n",
      "Best trial of city:  An Giang\n",
      "üê∞B·ª•t ƒê√¢y!!\n",
      "PARAMS:  {'max_iter': 140, 'alpha': 0.09911032299371612}\n",
      "‚≠êÔ∏è‚≠êÔ∏è City:  Qu·∫£ng Ninh\n",
      "mean_squared_error: 6.2903\n",
      "rmse: 2.5080378952708746\n",
      "mape: 2932224670942091.0\n",
      "Study statistics for : \n",
      "  Number of finished trials:  1\n",
      "Best trial of city:  Qu·∫£ng Ninh\n",
      "üê∞B·ª•t ƒê√¢y!!\n",
      "PARAMS:  {'max_iter': 95, 'alpha': 0.0008960627408349147}\n",
      "‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è Nstep:  3\n",
      "‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è Model_name:  SVMRBF\n",
      "‚≠êÔ∏è‚≠êÔ∏è City:  B√¨nh Ph∆∞·ªõc\n",
      "mean_squared_error: 697.0418\n",
      "rmse: 26.401548639374703\n",
      "mape: 0.9911185334281593\n",
      "Study statistics for : \n",
      "  Number of finished trials:  1\n",
      "Best trial of city:  B√¨nh Ph∆∞·ªõc\n",
      "‚≠êÔ∏è‚≠êÔ∏è City:  An Giang\n",
      "mean_squared_error: 126.2614\n",
      "rmse: 11.236611164430709\n",
      "mape: 1.0485569193494673\n",
      "Study statistics for : \n",
      "  Number of finished trials:  1\n",
      "Best trial of city:  An Giang\n",
      "‚≠êÔ∏è‚≠êÔ∏è City:  Qu·∫£ng Ninh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-28 16:19:01,716] A new study created in memory with name: SVMRBF\n",
      "[I 2023-10-28 16:19:01,735] Trial 0 finished with value: 1.7238284638215728 and parameters: {'max_iter': 71, 'epsilon': 3.278197898894802e-07}. Best is trial 0 with value: 1.7238284638215728.\n",
      "[I 2023-10-28 16:19:01,746] A new study created in memory with name: PoissonRegressor\n",
      "[I 2023-10-28 16:19:01,756] Trial 0 finished with value: 32.01406031437534 and parameters: {'max_iter': 188, 'alpha': 0.14028942220532745}. Best is trial 0 with value: 32.01406031437534.\n",
      "[I 2023-10-28 16:19:01,761] A new study created in memory with name: PoissonRegressor\n",
      "[I 2023-10-28 16:19:01,775] Trial 0 finished with value: 13.218113788095923 and parameters: {'max_iter': 112, 'alpha': 1.0235469086180577e-05}. Best is trial 0 with value: 13.218113788095923.\n",
      "[I 2023-10-28 16:19:01,781] A new study created in memory with name: PoissonRegressor\n",
      "[I 2023-10-28 16:19:01,788] Trial 0 finished with value: 2.7528643479339987 and parameters: {'max_iter': 111, 'alpha': 0.011771449181625041}. Best is trial 0 with value: 2.7528643479339987.\n",
      "[I 2023-10-28 16:19:01,794] A new study created in memory with name: SVMRBF\n",
      "[I 2023-10-28 16:19:01,799] Trial 0 finished with value: 22.320866028218823 and parameters: {'max_iter': 164, 'epsilon': 0.001756183651379053}. Best is trial 0 with value: 22.320866028218823.\n",
      "[I 2023-10-28 16:19:01,804] A new study created in memory with name: SVMRBF\n",
      "[I 2023-10-28 16:19:01,810] Trial 0 finished with value: 10.854758789482336 and parameters: {'max_iter': 100, 'epsilon': 5.015154828432277e-06}. Best is trial 0 with value: 10.854758789482336.\n",
      "[I 2023-10-28 16:19:01,815] A new study created in memory with name: SVMRBF\n",
      "[I 2023-10-28 16:19:01,820] Trial 0 finished with value: 2.1559604899247997 and parameters: {'max_iter': 80, 'epsilon': 6.808050679601272e-06}. Best is trial 0 with value: 2.1559604899247997.\n",
      "[I 2023-10-28 16:19:01,826] A new study created in memory with name: PoissonRegressor\n",
      "[I 2023-10-28 16:19:01,834] Trial 0 finished with value: 19.648559806759 and parameters: {'max_iter': 180, 'alpha': 3.295235999634492e-06}. Best is trial 0 with value: 19.648559806759.\n",
      "[I 2023-10-28 16:19:01,838] A new study created in memory with name: PoissonRegressor\n",
      "[I 2023-10-28 16:19:01,845] Trial 0 finished with value: 12.893200789687201 and parameters: {'max_iter': 74, 'alpha': 0.0037778939975884776}. Best is trial 0 with value: 12.893200789687201.\n",
      "[I 2023-10-28 16:19:01,851] A new study created in memory with name: PoissonRegressor\n",
      "[I 2023-10-28 16:19:01,858] Trial 0 finished with value: 1.9917760236436937 and parameters: {'max_iter': 100, 'alpha': 0.00011487906710869892}. Best is trial 0 with value: 1.9917760236436937.\n",
      "[I 2023-10-28 16:19:01,864] A new study created in memory with name: SVMRBF\n",
      "[I 2023-10-28 16:19:01,869] Trial 0 finished with value: 20.539532327893642 and parameters: {'max_iter': 196, 'epsilon': 0.0005001740598806468}. Best is trial 0 with value: 20.539532327893642.\n",
      "[I 2023-10-28 16:19:01,874] A new study created in memory with name: SVMRBF\n",
      "[I 2023-10-28 16:19:01,879] Trial 0 finished with value: 9.566282148195093 and parameters: {'max_iter': 141, 'epsilon': 0.0005719836659152254}. Best is trial 0 with value: 9.566282148195093.\n",
      "[I 2023-10-28 16:19:01,885] A new study created in memory with name: SVMRBF\n",
      "[I 2023-10-28 16:19:01,890] Trial 0 finished with value: 2.0567020410112513 and parameters: {'max_iter': 107, 'epsilon': 0.0012874890668632657}. Best is trial 0 with value: 2.0567020410112513.\n",
      "[I 2023-10-28 16:19:01,896] A new study created in memory with name: PoissonRegressor\n",
      "[I 2023-10-28 16:19:01,902] Trial 0 finished with value: 19.331398626968838 and parameters: {'max_iter': 162, 'alpha': 0.001238861584297766}. Best is trial 0 with value: 19.331398626968838.\n",
      "[I 2023-10-28 16:19:01,907] A new study created in memory with name: PoissonRegressor\n",
      "[I 2023-10-28 16:19:01,914] Trial 0 finished with value: 10.459190452733136 and parameters: {'max_iter': 186, 'alpha': 0.00018845171403783736}. Best is trial 0 with value: 10.459190452733136.\n",
      "[I 2023-10-28 16:19:01,920] A new study created in memory with name: PoissonRegressor\n",
      "[I 2023-10-28 16:19:01,926] Trial 0 finished with value: 2.127137231244351 and parameters: {'max_iter': 105, 'alpha': 0.0011578179394151256}. Best is trial 0 with value: 2.127137231244351.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_squared_error: 4.6027\n",
      "rmse: 2.145387395418091\n",
      "mape: 1618482214630812.5\n",
      "Study statistics for : \n",
      "  Number of finished trials:  1\n",
      "Best trial of city:  Qu·∫£ng Ninh\n",
      "‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è Model_name:  PoissonRegressor\n",
      "‚≠êÔ∏è‚≠êÔ∏è City:  B√¨nh Ph∆∞·ªõc\n",
      "mean_squared_error: 1331.3026\n",
      "rmse: 36.487019998968215\n",
      "mape: 2.792782632337093\n",
      "Study statistics for : \n",
      "  Number of finished trials:  1\n",
      "Best trial of city:  B√¨nh Ph∆∞·ªõc\n",
      "üê∞B·ª•t ƒê√¢y!!\n",
      "PARAMS:  {'max_iter': 188, 'alpha': 0.14028942220532745}\n",
      "‚≠êÔ∏è‚≠êÔ∏è City:  An Giang\n",
      "mean_squared_error: 210.1463\n",
      "rmse: 14.496423877844451\n",
      "mape: 1.6522617579073038\n",
      "Study statistics for : \n",
      "  Number of finished trials:  1\n",
      "Best trial of city:  An Giang\n",
      "üê∞B·ª•t ƒê√¢y!!\n",
      "PARAMS:  {'max_iter': 112, 'alpha': 1.0235469086180577e-05}\n",
      "‚≠êÔ∏è‚≠êÔ∏è City:  Qu·∫£ng Ninh\n",
      "mean_squared_error: 10.8329\n",
      "rmse: 3.291341679141267\n",
      "mape: 4325951908661849.0\n",
      "Study statistics for : \n",
      "  Number of finished trials:  1\n",
      "Best trial of city:  Qu·∫£ng Ninh\n",
      "üê∞B·ª•t ƒê√¢y!!\n",
      "PARAMS:  {'max_iter': 111, 'alpha': 0.011771449181625041}\n",
      "‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è Nstep:  4\n",
      "‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è Model_name:  SVMRBF\n",
      "‚≠êÔ∏è‚≠êÔ∏è City:  B√¨nh Ph∆∞·ªõc\n",
      "mean_squared_error: 974.1756\n",
      "rmse: 31.211785732492455\n",
      "mape: 1.2430508032184893\n",
      "Study statistics for : \n",
      "  Number of finished trials:  1\n",
      "Best trial of city:  B√¨nh Ph∆∞·ªõc\n",
      "‚≠êÔ∏è‚≠êÔ∏è City:  An Giang\n",
      "mean_squared_error: 172.9075\n",
      "rmse: 13.149430178315917\n",
      "mape: 1.2029750062532676\n",
      "Study statistics for : \n",
      "  Number of finished trials:  1\n",
      "Best trial of city:  An Giang\n",
      "‚≠êÔ∏è‚≠êÔ∏è City:  Qu·∫£ng Ninh\n",
      "mean_squared_error: 7.2598\n",
      "rmse: 2.6944002375236935\n",
      "mape: 2364591628959965.0\n",
      "Study statistics for : \n",
      "  Number of finished trials:  1\n",
      "Best trial of city:  Qu·∫£ng Ninh\n",
      "‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è Model_name:  PoissonRegressor\n",
      "‚≠êÔ∏è‚≠êÔ∏è City:  B√¨nh Ph∆∞·ªõc\n",
      "mean_squared_error: 821.6080\n",
      "rmse: 28.663704566782904\n",
      "mape: 1.1568719490069588\n",
      "Study statistics for : \n",
      "  Number of finished trials:  1\n",
      "Best trial of city:  B√¨nh Ph∆∞·ªõc\n",
      "üê∞B·ª•t ƒê√¢y!!\n",
      "PARAMS:  {'max_iter': 180, 'alpha': 3.295235999634492e-06}\n",
      "‚≠êÔ∏è‚≠êÔ∏è City:  An Giang\n",
      "mean_squared_error: 208.9795\n",
      "rmse: 14.45612308875188\n",
      "mape: 1.7168561570912741\n",
      "Study statistics for : \n",
      "  Number of finished trials:  1\n",
      "Best trial of city:  An Giang\n",
      "üê∞B·ª•t ƒê√¢y!!\n",
      "PARAMS:  {'max_iter': 74, 'alpha': 0.0037778939975884776}\n",
      "‚≠êÔ∏è‚≠êÔ∏è City:  Qu·∫£ng Ninh\n",
      "mean_squared_error: 5.2185\n",
      "rmse: 2.2843941172160194\n",
      "mape: 2496727544510330.0\n",
      "Study statistics for : \n",
      "  Number of finished trials:  1\n",
      "Best trial of city:  Qu·∫£ng Ninh\n",
      "üê∞B·ª•t ƒê√¢y!!\n",
      "PARAMS:  {'max_iter': 100, 'alpha': 0.00011487906710869892}\n",
      "‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è Nstep:  5\n",
      "‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è Model_name:  SVMRBF\n",
      "‚≠êÔ∏è‚≠êÔ∏è City:  B√¨nh Ph∆∞·ªõc\n",
      "mean_squared_error: 865.3168\n",
      "rmse: 29.416267599218227\n",
      "mape: 1.0376584006817366\n",
      "Study statistics for : \n",
      "  Number of finished trials:  1\n",
      "Best trial of city:  B√¨nh Ph∆∞·ªõc\n",
      "‚≠êÔ∏è‚≠êÔ∏è City:  An Giang\n",
      "mean_squared_error: 138.0031\n",
      "rmse: 11.747470763199205\n",
      "mape: 1.1597111193811909\n",
      "Study statistics for : \n",
      "  Number of finished trials:  1\n",
      "Best trial of city:  An Giang\n",
      "‚≠êÔ∏è‚≠êÔ∏è City:  Qu·∫£ng Ninh\n",
      "mean_squared_error: 6.0872\n",
      "rmse: 2.4672265999895115\n",
      "mape: 2414282643449672.0\n",
      "Study statistics for : \n",
      "  Number of finished trials:  1\n",
      "Best trial of city:  Qu·∫£ng Ninh\n",
      "‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è Model_name:  PoissonRegressor\n",
      "‚≠êÔ∏è‚≠êÔ∏è City:  B√¨nh Ph∆∞·ªõc\n",
      "mean_squared_error: 815.1085\n",
      "rmse: 28.550104311414906\n",
      "mape: 1.0723554113206535\n",
      "Study statistics for : \n",
      "  Number of finished trials:  1\n",
      "Best trial of city:  B√¨nh Ph∆∞·ªõc\n",
      "üê∞B·ª•t ƒê√¢y!!\n",
      "PARAMS:  {'max_iter': 162, 'alpha': 0.001238861584297766}\n",
      "‚≠êÔ∏è‚≠êÔ∏è City:  An Giang\n",
      "mean_squared_error: 157.6155\n",
      "rmse: 12.554500801735301\n",
      "mape: 1.437995777134524\n",
      "Study statistics for : \n",
      "  Number of finished trials:  1\n",
      "Best trial of city:  An Giang\n",
      "üê∞B·ª•t ƒê√¢y!!\n",
      "PARAMS:  {'max_iter': 186, 'alpha': 0.00018845171403783736}\n",
      "‚≠êÔ∏è‚≠êÔ∏è City:  Qu·∫£ng Ninh\n",
      "mean_squared_error: 5.9262\n",
      "rmse: 2.434369575157365\n",
      "mape: 2750713958388367.0\n",
      "Study statistics for : \n",
      "  Number of finished trials:  1\n",
      "Best trial of city:  Qu·∫£ng Ninh\n",
      "üê∞B·ª•t ƒê√¢y!!\n",
      "PARAMS:  {'max_iter': 105, 'alpha': 0.0011578179394151256}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-28 16:19:01,932] A new study created in memory with name: SVMRBF\n",
      "[I 2023-10-28 16:19:01,938] Trial 0 finished with value: 21.86613256016615 and parameters: {'max_iter': 139, 'epsilon': 7.256155907125184e-05}. Best is trial 0 with value: 21.86613256016615.\n",
      "[I 2023-10-28 16:19:01,942] A new study created in memory with name: SVMRBF\n",
      "[I 2023-10-28 16:19:01,948] Trial 0 finished with value: 10.627249562344938 and parameters: {'max_iter': 85, 'epsilon': 0.0009305821755605312}. Best is trial 0 with value: 10.627249562344938.\n",
      "[I 2023-10-28 16:19:01,954] A new study created in memory with name: SVMRBF\n",
      "[I 2023-10-28 16:19:01,960] Trial 0 finished with value: 2.004261182642931 and parameters: {'max_iter': 103, 'epsilon': 0.0345530302806269}. Best is trial 0 with value: 2.004261182642931.\n",
      "[I 2023-10-28 16:19:01,965] A new study created in memory with name: PoissonRegressor\n",
      "[I 2023-10-28 16:19:01,974] Trial 0 finished with value: 15.50768279250838 and parameters: {'max_iter': 94, 'alpha': 7.690219472845696e-08}. Best is trial 0 with value: 15.50768279250838.\n",
      "[I 2023-10-28 16:19:01,979] A new study created in memory with name: PoissonRegressor\n",
      "[I 2023-10-28 16:19:01,987] Trial 0 finished with value: 10.120380342263186 and parameters: {'max_iter': 168, 'alpha': 7.835721939693567e-06}. Best is trial 0 with value: 10.120380342263186.\n",
      "[I 2023-10-28 16:19:01,993] A new study created in memory with name: PoissonRegressor\n",
      "[I 2023-10-28 16:19:01,999] Trial 0 finished with value: 2.492377190176281 and parameters: {'max_iter': 126, 'alpha': 0.010963903360710589}. Best is trial 0 with value: 2.492377190176281.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è Nstep:  6\n",
      "‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è Model_name:  SVMRBF\n",
      "‚≠êÔ∏è‚≠êÔ∏è City:  B√¨nh Ph∆∞·ªõc\n",
      "mean_squared_error: 1088.9252\n",
      "rmse: 32.99886627867482\n",
      "mape: 1.0310435047198654\n",
      "Study statistics for : \n",
      "  Number of finished trials:  1\n",
      "Best trial of city:  B√¨nh Ph∆∞·ªõc\n",
      "‚≠êÔ∏è‚≠êÔ∏è City:  An Giang\n",
      "mean_squared_error: 184.0601\n",
      "rmse: 13.5668757429324\n",
      "mape: 1.0785599303832827\n",
      "Study statistics for : \n",
      "  Number of finished trials:  1\n",
      "Best trial of city:  An Giang\n",
      "‚≠êÔ∏è‚≠êÔ∏è City:  Qu·∫£ng Ninh\n",
      "mean_squared_error: 6.2404\n",
      "rmse: 2.4980720833803978\n",
      "mape: 1972606844323483.2\n",
      "Study statistics for : \n",
      "  Number of finished trials:  1\n",
      "Best trial of city:  Qu·∫£ng Ninh\n",
      "‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è Model_name:  PoissonRegressor\n",
      "‚≠êÔ∏è‚≠êÔ∏è City:  B√¨nh Ph∆∞·ªõc\n",
      "mean_squared_error: 663.3397\n",
      "rmse: 25.75538210297813\n",
      "mape: 0.6595740178633841\n",
      "Study statistics for : \n",
      "  Number of finished trials:  1\n",
      "Best trial of city:  B√¨nh Ph∆∞·ªõc\n",
      "üê∞B·ª•t ƒê√¢y!!\n",
      "PARAMS:  {'max_iter': 94, 'alpha': 7.690219472845696e-08}\n",
      "‚≠êÔ∏è‚≠êÔ∏è City:  An Giang\n",
      "mean_squared_error: 179.6154\n",
      "rmse: 13.40206885306683\n",
      "mape: 1.4510886248174575\n",
      "Study statistics for : \n",
      "  Number of finished trials:  1\n",
      "Best trial of city:  An Giang\n",
      "üê∞B·ª•t ƒê√¢y!!\n",
      "PARAMS:  {'max_iter': 168, 'alpha': 7.835721939693567e-06}\n",
      "‚≠êÔ∏è‚≠êÔ∏è City:  Qu·∫£ng Ninh\n",
      "mean_squared_error: 7.6785\n",
      "rmse: 2.7710151971188286\n",
      "mape: 3069568432374005.5\n",
      "Study statistics for : \n",
      "  Number of finished trials:  1\n",
      "Best trial of city:  Qu·∫£ng Ninh\n",
      "üê∞B·ª•t ƒê√¢y!!\n",
      "PARAMS:  {'max_iter': 126, 'alpha': 0.010963903360710589}\n"
     ]
    }
   ],
   "source": [
    "#########################\n",
    "# Main cell for optimize ML algorithm\n",
    "#########################\n",
    "# Hai thu·∫≠t to√°n n√†y ch∆∞a ch·∫°y ƒëc nhe, n√™n ƒë·ª´ng truy·ªÅn v√¥ m·∫£ng ƒë·ªÉ n√≥ ch·∫°y nho√©!\n",
    "# \"PoissonRegressor\"\n",
    "# \"SVMRBF\"\n",
    "\n",
    "model_name_list = [\n",
    "    #  \"RandomForest\",\n",
    "    #  \"LinearRegressionModel\",\n",
    "    #  \"LightGBMModel\",\n",
    "    #  \"CatBoostModel\",\n",
    "    #  \"XGBModel\",\n",
    "    \"SVMRBF\",\n",
    "    \"PoissonRegressor\"\n",
    "    \n",
    "]\n",
    "\n",
    "\n",
    "# L∆∞u th√¥ng tin traceback study v√† error city trong qu√° tr√¨nh optimize\n",
    "l_study_city ={}\n",
    "l_errCity =[]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  for nstep in range(1,args.n_predicted_period_months+1):\n",
    "    print(\"‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è Nstep: \",nstep)\n",
    "    lags_by_nstep = args.look_back + nstep - 1\n",
    "    lags_past_covariates_by_nstep = [-lags_by_nstep+2,-lags_by_nstep+1,-lags_by_nstep] #M·∫£ng n√†y ch·ª©a ba gi√° tr·ªã t∆∞∆°ng ·ª©ng cho args.lookback 3\n",
    "    lags_past_covariates_in_str = f\"{-lags_by_nstep+2},{-lags_by_nstep+1},{-lags_by_nstep}\"\n",
    "    for model_name in model_name_list: \n",
    "      print(\"‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è Model_name: \",model_name)\n",
    "      best_param = pd.DataFrame()\n",
    "      for city_index in range(len(cities)):\n",
    "        print(\"‚≠êÔ∏è‚≠êÔ∏è City: \",cities[city_index])\n",
    "        # Use Tree-structured Parzen Estimator sampler to minimise RMSE\n",
    "        sampler = optuna.samplers.TPESampler()\n",
    "        study = optuna.create_study(sampler=sampler, direction='minimize', study_name = model_name)\n",
    "        # truy·ªÅn multiple param v√†o trong bi·∫øn trial\n",
    "        obj_func = lambda trial: objective(model_name, trial, cities[city_index], nstep = nstep)\n",
    "        try:\n",
    "          # Optimise over 100 trials\n",
    "          study.optimize(obj_func, n_trials=args.ntry, n_jobs=args.njob)\n",
    "\n",
    "          # Print results\n",
    "          print(\"Study statistics for : \")\n",
    "          print(\"  Number of finished trials: \", len(study.trials))\n",
    "          print(\"Best trial of city: \",cities[city_index])\n",
    "\n",
    "          best_trial = study.best_trial\n",
    "          # l∆∞u best param v√†o trong bi·∫øn to√†n c·ª•c\n",
    "\n",
    "          if model_name == \"LinearRegressionModel\":\n",
    "            one_city_param = pd.DataFrame({\n",
    "                                'City':  cities[city_index],\n",
    "                                'Alg_name': 'LinearRegressionModel',\n",
    "                                'Best_value': best_trial.value,\n",
    "                                'n_try_opt': args.ntry,\n",
    "                                'lags' : lags_by_nstep,\n",
    "                                'lags_past_covariates': [lags_past_covariates_in_str],\n",
    "                                'output_chunk_length': 1,\n",
    "                                'random_state':best_trial.params['random_state'],\n",
    "                                })\n",
    "          elif model_name == 'XGBModel':\n",
    "            one_city_param = pd.DataFrame({\n",
    "                                'City':  cities[city_index],\n",
    "                                'Alg_name': 'XGBModel',\n",
    "                                'Best_value': best_trial.value,\n",
    "                                'n_try_opt': args.ntry,\n",
    "                                'lags' : lags_by_nstep,\n",
    "                                'lags_past_covariates': [lags_past_covariates_in_str],\n",
    "                                'output_chunk_length': 1,\n",
    "                                'random_state':best_trial.params['random_state'],\n",
    "                                'likelihood': best_trial.params['likelihood'],\n",
    "                                })\n",
    "          elif model_name == \"LightGBMModel\":\n",
    "            one_city_param = pd.DataFrame({\n",
    "                                'City':  cities[city_index],\n",
    "                                'Alg_name': 'LightGBMModel',\n",
    "                                'Best_value': best_trial.value,\n",
    "                                'n_try_opt': args.ntry,\n",
    "                                'lags': lags_by_nstep,\n",
    "                                'lags_past_covariates': [lags_past_covariates_in_str],\n",
    "                                'multi_models': best_trial.params['multi_models'],\n",
    "                                'num_leaves': best_trial.params['num_leaves'], \n",
    "                                'feature_fraction': best_trial.params['feature_fraction'], \n",
    "                                'min_child_samples': best_trial.params['min_child_samples'], \n",
    "                                'lambda_l1': best_trial.params['lambda_l1'], \n",
    "                                'lambda_l2': best_trial.params['lambda_l2'], \n",
    "                                'likelihood': best_trial.params['likelihood'], \n",
    "                                'learning_rate': best_trial.params['learning_rate']\n",
    "                                })\n",
    "          elif model_name == \"CatBoostModel\":\n",
    "            quantitles = best_trial.params['quantiles']\n",
    "            if quantitles == None:\n",
    "              quantitles = \"IsNone\"\n",
    "            else:\n",
    "              quantitles = f\"{quantitles[0]},{quantitles[1]},{quantitles[2]}\"\n",
    "            one_city_param = pd.DataFrame({\n",
    "                                'City':  cities[city_index],\n",
    "                                'Alg_name': 'CatBoost',\n",
    "                                'Best_value': best_trial.value,\n",
    "                                'n_try_opt': args.ntry,\n",
    "                                'lags' : lags_by_nstep,\n",
    "                                'lags_past_covariates': [lags_past_covariates_in_str],\n",
    "                                'output_chunk_length': 1,\n",
    "                                'learning_rate': best_trial.params['learning_rate'],\n",
    "                                'n_estimators': best_trial.params['n_estimators'],\n",
    "                                'max_depth': best_trial.params['max_depth'],\n",
    "                                'random_state': best_trial.params['random_state'],\n",
    "                                'likelihood': best_trial.params['likelihood'],\n",
    "                                'quantiles': [quantitles],\n",
    "                                'bagging_temperature': best_trial.params['bagging_temperature'],\n",
    "                                'border_count': best_trial.params['border_count'],\n",
    "                                'l2_leaf_reg': best_trial.params['l2_leaf_reg'],\n",
    "                                'random_strength':best_trial.params['random_strength'],\n",
    "                                })\n",
    "            display(one_city_param)\n",
    "          elif model_name == \"RandomForest\":\n",
    "            one_city_param = pd.DataFrame({\n",
    "                                'City':  cities[city_index],\n",
    "                                'Alg_name': 'RandomForest',\n",
    "                                'Best_value': best_trial.value,\n",
    "                                'n_try_opt': args.ntry,\n",
    "                                'lags' : lags_by_nstep,\n",
    "                                'lags_past_covariates': [lags_past_covariates_in_str],\n",
    "                                'output_chunk_length': 1,\n",
    "                                'n_estimators': best_trial.params['n_estimators'],\n",
    "                                'max_depth': best_trial.params['max_depth'],\n",
    "                                'random_state':best_trial.params['random_state'],\n",
    "                                })\n",
    "          elif model_name == \"SVMRBF\":\n",
    "            one_city_param = pd.DataFrame({\n",
    "                                'City':  [cities[city_index]],\n",
    "                                'Alg_name': 'SVMRBF',\n",
    "                                'Best_value': best_trial.value,\n",
    "                                'n_try_opt': args.ntry,\n",
    "                                'max_iter':best_trial.params['max_iter'],\n",
    "                                'kernel': 'rbf',\n",
    "                                'epsilon': best_trial.params['epsilon'],\n",
    "                                })\n",
    "          elif model_name == \"PoissonRegressor\":\n",
    "            print(\"üê∞B·ª•t ƒê√¢y!!\")\n",
    "            print(\"PARAMS: \",best_trial.params)\n",
    "            one_city_param = pd.DataFrame({\n",
    "                                'City':  [cities[city_index]],\n",
    "                                'Alg_name': 'PoissonRegressor',\n",
    "                                'Best_value': best_trial.value,\n",
    "                                'n_try_opt': args.ntry,\n",
    "                                'max_iter':best_trial.params['max_iter'],\n",
    "                                'fit_intercept': \"False\",\n",
    "                                'alpha' : best_trial.params['alpha'],\n",
    "                                })\n",
    "          # file_path = 'opt_results/opt_res_ml_26102023/261023_DF_opt_hyperparam_'+ model_name + '_'+str(nstep)+'-nstep.xlsx'\n",
    "          folder_path = f'opt_results/opt_res_ml_26102023/{model_name}/'\n",
    "          file_path = folder_path+ f'261023_DF_opt_hyperparam_{model_name}_{nstep}-nstep.xlsx'\n",
    "          if(os.path.isfile(file_path)):\n",
    "              with pd.ExcelWriter(file_path,mode=\"a\",engine=\"openpyxl\",if_sheet_exists=\"overlay\") as writer:\n",
    "                  one_city_param.to_excel(writer, header=None, startrow=city_index+1,index=False)\n",
    "          else:\n",
    "              if(not (os.path.isdir(folder_path))):\n",
    "                os.mkdir(folder_path)\n",
    "              with pd.ExcelWriter(file_path,engine=\"openpyxl\") as writer:\n",
    "                  one_city_param.to_excel(writer, startrow=city_index,index=False)\n",
    "        except:# c√≥ error th√¨ l∆∞u v√†o l_errCity ƒë·ªÉ check l·∫°i sau \n",
    "          l_errCity.append(cities[city_index])\n",
    "          #send_to_telegram(f'T·ªânh b·ªã l·ªói trong qu√° tr√¨nh optimize b·∫±ng model {model_name}: {cities[city_index]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1525,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send_to_telegram(\"Ch·∫°y xong optimize r√πiii!!V√¥ check thuiii!!!\" )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
