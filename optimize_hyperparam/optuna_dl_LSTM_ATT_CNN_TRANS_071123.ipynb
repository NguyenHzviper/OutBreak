{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error,\n",
    "    mean_absolute_percentage_error,\n",
    ")\n",
    "import optuna\n",
    "import joblib\n",
    "from statistics import mean\n",
    "import math\n",
    "import torch\n",
    "\n",
    "from darts import TimeSeries\n",
    "from darts.models import BlockRNNModel, NBEATSModel, NHiTSModel, TCNModel, TFTModel\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from darts.utils.likelihood_models import GaussianLikelihood\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prj_path = \"../\"\n",
    "data_path = prj_path + \"data/new_data/DH/squeezed/\"\n",
    "prj_path_opt = prj_path + \"optimize_hyperparam/opt_results/\"\n",
    "output_process = prj_path + \"data/new_data/DH/processed_data/\"\n",
    "output_featureselection = prj_path + \"data/new_data/DH/feature_selection/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cities = [\n",
    "    \"An Giang\",\n",
    "    \"BR Vũng Tàu\",\n",
    "    \"Bình Phước\",\n",
    "    \"Bình Thuận\",\n",
    "    \"Bình Định\",\n",
    "    \"Bạc Liêu\",\n",
    "    \"Bắc Kạn\",\n",
    "    \"Bắc Giang\",\n",
    "    \"Cao Bằng\",\n",
    "    \"Cà Mau\",\n",
    "    \"Cần Thơ\",\n",
    "    \"Gia Lai\",\n",
    "    \"Hà Giang\",\n",
    "    \"Hà Nội\",\n",
    "    \"Hà Tĩnh\",\n",
    "    \"Hòa Bình\",\n",
    "    \"Hưng Yên\",\n",
    "    \"Hải Dương\",\n",
    "    \"Hải Phòng\",\n",
    "    \"Khánh Hòa\",\n",
    "    \"Kiên Giang\",\n",
    "    \"Kon Tum\",\n",
    "    \"Lai Châu\",\n",
    "    \"Long An\",\n",
    "    \"Lào Cai\",\n",
    "    \"Lâm Đồng\",\n",
    "    \"Lạng Sơn\",\n",
    "    \"Nam Định\",\n",
    "    \"Nghệ An\",\n",
    "    \"Ninh Bình\",\n",
    "    \"Ninh Thuận\",\n",
    "    \"Phú Thọ\",\n",
    "    \"Phú Yên\",\n",
    "    \"Quảng Bình\",\n",
    "    \"Quảng Nam\",\n",
    "    \"Quảng Ngãi\",\n",
    "    \"Quảng Ninh\",\n",
    "    \"Quảng Trị\",\n",
    "    \"Sóc Trăng\",\n",
    "    \"Sơn La\",\n",
    "    \"TT Huế\",\n",
    "    \"Thanh Hóa\",\n",
    "    \"Thái Bình\",\n",
    "    \"Thái Nguyên\",\n",
    "    \"Tiền Giang\",\n",
    "    \"Trà Vinh\",\n",
    "    \"Tuyên Quang\",\n",
    "    \"Tây Ninh\",\n",
    "    \"Vĩnh Phúc\",\n",
    "    \"Yên Bái\",\n",
    "    \"Điện Biên\",\n",
    "    \"Đà Nẵng\",\n",
    "    \"Đắk Nông\",\n",
    "    \"Đắk Lắk\",\n",
    "    \"Đồng Tháp\",\n",
    "]\n",
    "cities = [\"An Giang\", \"BR Vũng Tàu\", \"Bình Phước\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters as args using the Configuration class\n",
    "class Configuration:\n",
    "    def __init__(self):\n",
    "        # lấy bộ test dài 36 tháng = 3 năm\n",
    "        self.test_size = 36\n",
    "        # là nhìn vào dữ liệu trước 3 tháng và dự phóng\n",
    "        self.look_back = 3\n",
    "        # dự phóng n-step trong 6 tháng\n",
    "        self.n_predicted_period_months = 6\n",
    "        self.n_features = 3\n",
    "        self.seed = 42\n",
    "        # mỗi phần tử x trong tập suppervise có độ lớn là 16 = 16 tháng\n",
    "        self.batch_size = 16\n",
    "        self.device = torch.device(\"cuda\")\n",
    "        # self.device = torch.device(\"cpu\")\n",
    "        self.epochs = 300\n",
    "        # others\n",
    "        self.labels = \"Dengue_fever_rates\"\n",
    "        # Input param for Optimize Run\n",
    "        self.ntry = 1\n",
    "        self.njob = 1\n",
    "\n",
    "\n",
    "args = Configuration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seeding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    import random\n",
    "    import numpy as np\n",
    "    import torch\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "seed_everything(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supporting functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "def send_to_telegram(message):\n",
    "    apiToken = \"5908735099:AAGVSLrW62aXPBP-GrMvxoVgMsuJxXJpP1Q\"\n",
    "    chatID = \"@ptn_announcement\"\n",
    "    apiURL = f\"https://api.telegram.org/bot{apiToken}/sendMessage\"\n",
    "\n",
    "    try:\n",
    "        response = requests.post(apiURL, json={\"chat_id\": chatID, \"text\": message})\n",
    "        print(response.text)\n",
    "    except Exception as e:\n",
    "        message_error = \"Bị lỗi rùi: \" + str(e)\n",
    "        response = requests.post(\n",
    "            apiURL, json={\"chat_id\": chatID, \"text\": message_error}\n",
    "        )\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict_all_city_data():\n",
    "    cities_data = {}\n",
    "    for city in cities:\n",
    "        city_result = pd.read_excel(\n",
    "            prj_path + \"data/new_data/DH/squeezed/squeezed_\" + city + \".xlsx\"\n",
    "        )\n",
    "        \"\"\"Get all data from all city in 1997 - 2016\"\"\"\n",
    "        city_result = city_result.loc[city_result[\"year_month\"] < \"2017-1-1\"]\n",
    "        cities_data[city] = city_result\n",
    "    return cities_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data (pre-)processing functions\n",
    "# modification\n",
    "def get_city_data(city_name, dict_full_data):\n",
    "    \"\"\"Returns Dengue fever rate and climate data\"\"\"\n",
    "    city_data = dict_full_data[city_name].drop(\n",
    "        columns=[\n",
    "            \"Diarrhoea_cases\",\n",
    "            \"Diarrhoea_rates\",\n",
    "            \"province\",\n",
    "            \"Influenza_rates\",\n",
    "            \"Influenza_cases\",\n",
    "            \"Dengue_fever_cases\",\n",
    "            \"year\",\n",
    "            \"month\",\n",
    "        ],\n",
    "        axis=1,\n",
    "        inplace=False,\n",
    "    )\n",
    "    return city_data\n",
    "\n",
    "\n",
    "def convert_to_stationary(city_data):\n",
    "    \"\"\"Subtracts previous value for all cols except disease rates\"\"\"\n",
    "    for col_name in city_data.columns:\n",
    "        if col_name != \"Dengue_fever_rates\":\n",
    "            try:\n",
    "                city_data[col_name] = city_data[col_name] - city_data[col_name].shift()\n",
    "            except:\n",
    "                print(col_name)\n",
    "    return city_data\n",
    "\n",
    "\n",
    "def impute_missing_value(city_data):\n",
    "    \"\"\"\n",
    "    Imputes 0 for first 12 months,\n",
    "    last year's value for months 12-24,\n",
    "    and minimum value of last two years for months 25+\n",
    "    \"\"\"\n",
    "    for col in city_data.columns:\n",
    "        for index in range(len(city_data[col])):\n",
    "            if np.isnan(city_data[col].iloc[index]):\n",
    "                if index < 12:\n",
    "                    city_data[col].iloc[index] = 0\n",
    "                elif index >= 12 and index <= 24:\n",
    "                    city_data[col].iloc[index] = city_data[col].iloc[index - 12]\n",
    "                else:\n",
    "                    city_data[col].iloc[index] = min(\n",
    "                        city_data[col].iloc[index - 12], city_data[col].iloc[index - 24]\n",
    "                    )\n",
    "    return city_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_full_data(dict_full_data):\n",
    "    climate_and_disease_feats = [\n",
    "        \"Total_Evaporation\",\n",
    "        \"Total_Rainfall\",\n",
    "        \"Max_Daily_Rainfall\",\n",
    "        \"n_raining_days\",\n",
    "        \"Average_temperature\",\n",
    "        \"Max_Average_Temperature\",\n",
    "        \"Min_Average_Temperature\",\n",
    "        \"Max_Absolute_Temperature\",\n",
    "        \"Min_Absolute_Temperature\",\n",
    "        \"Average_Humidity\",\n",
    "        \"Min_Humidity\",\n",
    "        \"n_hours_sunshine\",\n",
    "        \"Dengue_fever_rates\",\n",
    "    ]\n",
    "    for city in cities:\n",
    "        city_data = get_city_data(city_name=city, dict_full_data=dict_full_data)\n",
    "        city_data_features = city_data[climate_and_disease_feats]\n",
    "        city_data_features = impute_missing_value(city_data_features)\n",
    "        city_data_features = convert_to_stationary(city_data_features)\n",
    "        city_data_features.dropna(inplace=True)\n",
    "        city_data_features.loc[:, \"year_month\"] = city_data[\"year_month\"]\n",
    "        dict_full_data[city] = city_data_features\n",
    "    return dict_full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, look_back, n_nextstep=args.n_predicted_period_months):\n",
    "    \"\"\"Splits data into train and test sets based on args (Configuration class)\"\"\"\n",
    "    train = data[: -args.test_size]\n",
    "    test = data[-args.test_size - look_back - (n_nextstep - 1) :]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_supervised(data, d_out, d_in):\n",
    "    \"\"\"\n",
    "    Frames time-series as supervised learning dataset.\n",
    "\n",
    "    Args:\n",
    "      d_in: lookback window\n",
    "      d_out: number of predicted months\n",
    "\n",
    "    Returns:\n",
    "      Numpy arrays of disease incidence (y) and other predictors (X)\n",
    "    \"\"\"\n",
    "    X, y = list(), list()\n",
    "    for index, _ in enumerate(data):\n",
    "        in_end = index + d_in\n",
    "        out_end = in_end + d_out\n",
    "        if out_end <= len(data):\n",
    "            X.append(data[index:in_end, :-1])\n",
    "            y.append(data[in_end:out_end, -1])\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataWithSelectedFeature(city, next_predicted_month):\n",
    "    selected_feature = []\n",
    "    df = pd.read_csv(\n",
    "        output_featureselection\n",
    "        + str(next_predicted_month)\n",
    "        + \"step_feature_selection_3_most.csv\"\n",
    "    )\n",
    "    for row in range(len(df)):\n",
    "        if df[\"City\"][row] == city:\n",
    "            selected_feature.append(df[\"1st_Feature\"][row])\n",
    "            selected_feature.append(df[\"2nd_Feature\"][row])\n",
    "            selected_feature.append(df[\"3rd_Feature\"][row])\n",
    "    return selected_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(df_full, model, location, feature_list, labels, scaler):\n",
    "    \"\"\"\n",
    "    $df: pandas.DataFrame object containing data for training and testing model:\n",
    "    $model: darts model object\n",
    "    $feature_list: Names of the features used as model input\n",
    "    $label: the value the model will be trained to predict\n",
    "    $scaler: scaler object. Note: the scaler will be fitted on training data and applied to test data\n",
    "    $lags: how much to look back into the past to output prediction\n",
    "    $split_index: the point at which to divide train and test_data\n",
    "\n",
    "    \"\"\"\n",
    "    df_train, df_eval = split_data(\n",
    "        df_full, args.look_back, args.n_predicted_period_months\n",
    "    )\n",
    "\n",
    "    print(\"🍋🍋🍋🍋🍋Check var feature selectio n: \",feature_list)\n",
    "    x_train = TimeSeries.from_dataframe(df_train, \"year_month\", feature_list)\n",
    "    y_train = TimeSeries.from_dataframe(df_train, \"year_month\", labels)\n",
    "    x_test = TimeSeries.from_dataframe(df_eval, \"year_month\", feature_list)\n",
    "    y_test = TimeSeries.from_dataframe(df_eval, \"year_month\", labels)\n",
    "\n",
    "    # Cách một khoảng phần này cho x_train thì mới đưa đc chuỗi x_test vừa đủ để dự đoán 36 lần 6step\n",
    "    # ValueError: For the given forecasting horizon `n=xxx`, the provided\n",
    "    # past covariates at dataset index `0` do not extend far enough into\n",
    "    # the future. As `n > output_chunk_length` the past covariates must\n",
    "    # start at time step `xxxxxx`, whereas now they start at\n",
    "    # time step `xxxxx`.\n",
    "\n",
    "    x_train_back = x_train[: -(args.n_predicted_period_months - 1)]\n",
    "    y_train_back = y_train[: -(args.n_predicted_period_months - 1)]\n",
    "\n",
    "    predict_list = []\n",
    "    for i in range(args.test_size + (args.n_predicted_period_months - 1)):\n",
    "        model.fit(y_train_back, past_covariates=x_train_back)\n",
    "        prediction = model.predict(\n",
    "            6,\n",
    "            past_covariates=x_test[0 : i + args.look_back],\n",
    "            num_samples=1,\n",
    "            verbose=False,\n",
    "        )\n",
    "        x_train_back = x_train_back.append(x_test[i + 3])\n",
    "        predict_list.append(np.array(prediction._xa).squeeze())\n",
    "    y_pred_list = []\n",
    "    for step in range(args.n_predicted_period_months):\n",
    "        moving = args.n_predicted_period_months - 1 - step\n",
    "        y_pred_list.append(\n",
    "            [x[step] for x in predict_list][moving : args.test_size + moving]\n",
    "        )\n",
    "\n",
    "    df_eval_true_inverse = df_full[-args.test_size :]\n",
    "    y_true = scaler.inverse_transform(df_eval_true_inverse.iloc[:, :-1])[\n",
    "        :, [-1]\n",
    "    ].reshape(args.test_size)\n",
    "\n",
    "    y_pred_inverse_list = []\n",
    "    for step in range(args.n_predicted_period_months):\n",
    "        df_eval_pred_inverse = df_full[-args.test_size :]\n",
    "        df_eval_pred_inverse[args.labels] = y_pred_list[step]  # step 1\n",
    "        y_pred_inverse = scaler.inverse_transform(df_eval_pred_inverse.iloc[:, :-1])[\n",
    "            :, [-1]\n",
    "        ].reshape(args.test_size)\n",
    "        y_pred_inverse_list.append(y_pred_inverse)\n",
    "\n",
    "    y_pred_inverse_list\n",
    "    df_compare_test_predict = pd.DataFrame(\n",
    "        {\n",
    "            \"y_true\": y_true,\n",
    "            \"y_pred_1step\": y_pred_inverse_list[0],\n",
    "            \"y_pred_2step\": y_pred_inverse_list[1],\n",
    "            \"y_pred_3step\": y_pred_inverse_list[2],\n",
    "            \"y_pred_4step\": y_pred_inverse_list[3],\n",
    "            \"y_pred_5step\": y_pred_inverse_list[4],\n",
    "            \"y_pred_6step\": y_pred_inverse_list[5],\n",
    "        }\n",
    "    )\n",
    "    df_compare_test_predict.plot()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    mse_nstep = []\n",
    "    mae_nstep = []\n",
    "    rmse_nstep = []\n",
    "    mape_nstep = []\n",
    "    for step in range(args.n_predicted_period_months):\n",
    "        mse_nstep.append(mean_squared_error(y_true, y_pred_inverse_list[step]))\n",
    "        mae_nstep.append(mean_absolute_error(y_true, y_pred_inverse_list[step]))\n",
    "        rmse_nstep.append(mse_nstep[step] ** 0.5)\n",
    "        mape_nstep.append(\n",
    "            mean_absolute_percentage_error(y_true, y_pred_inverse_list[step])\n",
    "        )\n",
    "    return (\n",
    "        model,\n",
    "        y_true,\n",
    "        y_pred_inverse_list,\n",
    "        mse_nstep,\n",
    "        mae_nstep,\n",
    "        rmse_nstep,\n",
    "        mape_nstep,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_prediction_for_location(\n",
    "    df_full, model, location, feature_list, labels, scaler\n",
    "):\n",
    "    \"\"\"train and generate prediction for a province\n",
    "    df: DataFrame object containing features and label(s) for training model\n",
    "    localtion: location_name\n",
    "    feature_list: list of features used as model input,  must be among the column names of df\n",
    "    labels: the values model will be trained to predict\n",
    "    scaler: sklearn scaler object\n",
    "    lags: how long into the past to look back when making prediction\n",
    "    split_index: the point at which to divide data into the train and test subsets.\n",
    "    \"\"\"\n",
    "    (\n",
    "        model,\n",
    "        y_true,\n",
    "        y_pred_inverse_list,\n",
    "        mse_nstep,\n",
    "        mae_nstep,\n",
    "        rmse_nstep,\n",
    "        mape_nstep,\n",
    "    ) = train_and_evaluate(df_full, model, location, feature_list, labels, scaler)\n",
    "\n",
    "    df_prediction = pd.DataFrame(\n",
    "        {\n",
    "            \"Date\": df_full[\"year_month\"][-args.test_size :],\n",
    "            \"Observed\": y_true[-args.test_size :],\n",
    "            f\"{1}-month\": y_pred_inverse_list[0],\n",
    "            f\"{2}-month\": y_pred_inverse_list[1],\n",
    "            f\"{3}-month\": y_pred_inverse_list[2],\n",
    "            f\"{4}-month\": y_pred_inverse_list[3],\n",
    "            f\"{5}-month\": y_pred_inverse_list[4],\n",
    "            f\"{6}-month\": y_pred_inverse_list[5],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    df_prediction[\"City\"] = location\n",
    "    for step in range(args.n_predicted_period_months):\n",
    "        df_prediction[f\"RMSE_{step+1}-month\"] = rmse_nstep[step]\n",
    "        df_prediction[f\"MAE_{step+1}-month\"] = mae_nstep[step]\n",
    "        df_prediction[f\"MAPE_{step+1}-month\"] = mape_nstep[step]\n",
    "        df_prediction[f\"MSE_{step+1}-month\"] = mse_nstep[step]\n",
    "    print(\"⭐️⭐️⭐️⭐️⭐️⭐️⭐️\")\n",
    "    display(df_prediction.head(5))\n",
    "    print(mean(mae_nstep))\n",
    "    return mean(mae_nstep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Pytorch LSTM model\n",
    "class MultiVariateLSTM(nn.Module):\n",
    "    def __init__(self, n_feature=3, n_layers=2, hidden_size=50):\n",
    "        super(MultiVariateLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=n_feature,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=n_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.linear = nn.Linear(hidden_size, args.n_predicted_period_months)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.sigma = nn.Parameter(torch.ones(args.n_predicted_period_months))\n",
    "\n",
    "    def forward(self, X_batch, y_batch=None):\n",
    "        output, (last_hidden, _) = self.lstm(X_batch)\n",
    "        last_hidden_vector = output[:, -1, :]\n",
    "        y_predicted = self.linear(last_hidden_vector)\n",
    "        if y_batch != None:\n",
    "            assert y_predicted.size() == y_batch.size()\n",
    "            loss = self.loss_fn(y_predicted, y_batch)\n",
    "            loss = 0.5 * loss / self.sigma**2\n",
    "            loss = loss.sum() + torch.log(1 + self.sigma.prod())\n",
    "            return y_predicted, loss\n",
    "            # return y_predicted, self.loss_fn(y_predicted, y_batch)\n",
    "        else:\n",
    "            return y_predicted\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = torch.tensor(X, device=args.device)\n",
    "        return self.forward(X)\n",
    "\n",
    "\n",
    "# Define Pytorch LSTM-ATT model\n",
    "class MultiVariateLSTM_Attention(nn.Module):\n",
    "    def __init__(self, n_feature=3, n_layers=2, hidden_size=50):\n",
    "        super(MultiVariateLSTM_Attention, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=n_feature,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=n_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.attention_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        # self.linear = nn.Linear(hidden_size*2, args.n_predicted_period_months)\n",
    "        self.linear = nn.Linear(hidden_size, args.n_predicted_period_months)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.sigma = nn.Parameter(torch.ones(args.n_predicted_period_months))\n",
    "\n",
    "    def forward(self, X_batch, y_batch=None):\n",
    "        output, (last_hidden, _) = self.lstm(X_batch)\n",
    "        last_hidden_vector = last_hidden[-1]\n",
    "        remain_hidden_vector = output\n",
    "        e_t = remain_hidden_vector.bmm(\n",
    "            self.attention_linear(last_hidden_vector).unsqueeze(2)\n",
    "        ).squeeze(-1)\n",
    "        alpha_t = F.softmax(e_t, dim=1)\n",
    "        attenion_vector = (\n",
    "            remain_hidden_vector.transpose(2, 1).bmm(alpha_t.unsqueeze(2)).squeeze(-1)\n",
    "        )\n",
    "        # combine_vector = torch.cat((last_hidden_vector, attenion_vector), dim=1)\n",
    "        # combine_vector = last_hidden_vector + attenion_vector\n",
    "        y_predicted = self.linear(attenion_vector)\n",
    "        if y_batch != None:\n",
    "            # assert y_predicted.size() == y_batch.size()\n",
    "            print(\"🍤🍤🍤🍤🍤🍤🍤🍤🍤🍤🍤\")\n",
    "            print(\"SHAPE y_predicted \", y_predicted.shape)\n",
    "            print(\"SHAPE y_batch \", y_batch.shape)\n",
    "\n",
    "            loss = self.loss_fn(y_predicted, y_batch)\n",
    "            loss = 0.5 * loss / self.sigma**2\n",
    "            loss = loss.sum() + torch.log(1 + self.sigma.prod())\n",
    "            return y_predicted, loss\n",
    "            # return y_predicted, self.loss_fn(y_predicted, y_batch)\n",
    "        else:\n",
    "            return y_predicted\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = torch.tensor(X, device=args.device)\n",
    "        return self.forward(X)\n",
    "\n",
    "\n",
    "# Define Pytorch CNN model\n",
    "class MultivariateCNN(nn.Module):\n",
    "    def __init__(self, num_filters=[100, 100, 100], dropout=0.01):\n",
    "        super(MultivariateCNN, self).__init__()\n",
    "        self.loss_fn = loss = nn.MSELoss()\n",
    "        self.filter_sizes = [1, 2, 3]\n",
    "        self.conv1d_list = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv1d(args.n_features, num_filters[i], self.filter_sizes[i])\n",
    "                for i in range(len(self.filter_sizes))\n",
    "            ]\n",
    "        )\n",
    "        self.linear = nn.Linear(np.sum(num_filters), args.n_predicted_period_months)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.sigma = nn.Parameter(torch.ones(args.n_predicted_period_months))\n",
    "\n",
    "    def forward(self, X_batch, y_batch=None):\n",
    "        X_batch = X_batch.permute(0, 2, 1)  # (batch_size, n_features, n_look_back)\n",
    "        X_conv_list = [F.relu(conv1d(X_batch)) for conv1d in self.conv1d_list]\n",
    "        X_pool_list = [\n",
    "            F.max_pool1d(x_conv, kernel_size=x_conv.shape[2]) for x_conv in X_conv_list\n",
    "        ]\n",
    "        X_fc = torch.cat([x_pool.squeeze(dim=2) for x_pool in X_pool_list], dim=1)\n",
    "        y_predicted = self.linear(self.dropout(X_fc))\n",
    "        if y_batch != None:\n",
    "            # assert y_predicted.size() == y_batch.size()\n",
    "            loss = self.loss_fn(y_predicted, y_batch)\n",
    "            loss = 0.5 * loss / self.sigma**2\n",
    "            loss = loss.sum() + torch.log(1 + self.sigma.prod())\n",
    "            return y_predicted, loss\n",
    "            # return y_predicted, self.loss_fn(y_predicted, y_batch)\n",
    "        else:\n",
    "            return y_predicted\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = torch.tensor(X, device=args.device)\n",
    "        return self.forward(X)\n",
    "\n",
    "\n",
    "# Define Pytorch Transformer model\n",
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, d_model=3, n_feature=3, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        pe = torch.zeros(n_feature, d_model)\n",
    "        for pos in range(n_feature):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** (2 * i / d_model)))\n",
    "                if i + 1 < d_model:\n",
    "                    pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * i + 1) / d_model)))\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x * math.sqrt(self.d_model)\n",
    "        length = x.size(1)\n",
    "        pe = Variable(self.pe[:, :length], requires_grad=False)\n",
    "        if x.is_cuda:\n",
    "            pe.cuda()\n",
    "        x = x + pe\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, d_input=3, n_head=3, hidden_size=256, n_layers=3, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.pe = PositionalEncoder(dropout=dropout)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(\n",
    "            d_model=d_input,\n",
    "            nhead=n_head,\n",
    "            dim_feedforward=hidden_size,\n",
    "            dropout=dropout,\n",
    "            activation=\"gelu\",\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, n_layers)\n",
    "        self.decoder = nn.Linear(d_input * n_head, args.n_predicted_period_months)\n",
    "        self.sigma = nn.Parameter(torch.ones(args.n_predicted_period_months))\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "    def forward(self, X_batch, y_batch=None):\n",
    "        X_batch = self.pe(X_batch)\n",
    "        X_batch = self.transformer_encoder(X_batch)\n",
    "        X_batch = X_batch.view(X_batch.size(0), -1)\n",
    "\n",
    "        y_predicted = self.decoder(X_batch)\n",
    "        if y_batch != None:\n",
    "            assert y_predicted.size() == y_batch.size()\n",
    "            loss = self.loss_fn(y_predicted, y_batch)\n",
    "            loss = 0.5 * loss / self.sigma**2\n",
    "            loss = loss.sum() + torch.log(1 + self.sigma.prod())\n",
    "            return y_predicted, loss\n",
    "        else:\n",
    "            return y_predicted\n",
    "        return X_batch\n",
    "\n",
    "    def predict(self, X):\n",
    "        with torch.no_grad():\n",
    "            X = torch.tensor(X, device=args.device)\n",
    "        return self.forward(X).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create class to train and evaluate models\n",
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_type,\n",
    "        learning_rate,\n",
    "        important_features,\n",
    "        train_loader,\n",
    "        test_tensor,\n",
    "        n_layers=2,\n",
    "        hidden_size=128,\n",
    "        num_filters=[100, 100, 100],\n",
    "        dropout=0.01,\n",
    "        look_back=\"\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialise trainer, allowing input of LSTM, LSTM-ATT, or CNN\n",
    "        hyperparameters. Adam optimiser used for all models.\n",
    "        \"\"\"\n",
    "        self.model_type = model_type\n",
    "        self.look_back = look_back\n",
    "        self.model = self.init_model(\n",
    "            model_type, n_layers, hidden_size, num_filters, dropout\n",
    "        )\n",
    "        self.model.double().to(args.device)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        self.important_features, self.train_loader, self.test_tensor = (\n",
    "            important_features,\n",
    "            train_loader,\n",
    "            test_tensor,\n",
    "        )\n",
    "\n",
    "    def init_model(self, model_type, n_layers, hidden_size, num_filters, dropout):\n",
    "        \"\"\"Initialise a model based on whether LSTM, LSTM-ATT, or CNN is chosen.\"\"\"\n",
    "        print(\"🌷🌷🌷🌷🌷🌷🌷🌷🌷\")\n",
    "        print(model_type)\n",
    "        if model_type == \"LSTM\":\n",
    "            model = MultiVariateLSTM(args.n_features, n_layers, hidden_size)\n",
    "        elif model_type == \"LSTM_ATT\":\n",
    "            model = MultiVariateLSTM_Attention(args.n_features, n_layers, hidden_size)\n",
    "        elif model_type == \"CNN\":\n",
    "            model = MultivariateCNN(num_filters, dropout)\n",
    "        elif model_type == \"TRANSFORMERS\":\n",
    "            model = TransformerModel(\n",
    "                d_input=args.look_back,\n",
    "                n_head=3,\n",
    "                hidden_size=hidden_size,\n",
    "                n_layers=n_layers,\n",
    "                dropout=dropout,\n",
    "            )\n",
    "        return model\n",
    "\n",
    "    def step(self, batch):\n",
    "        self.model.train()\n",
    "        X_batch, y_batch = tuple(t.to(args.device) for t in batch)\n",
    "        self.optimizer.zero_grad()\n",
    "        y_pred, loss = self.model.forward(X_batch, y_batch)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.mean().item()\n",
    "\n",
    "    def validation(self):\n",
    "        self.model.eval()\n",
    "        eval_loss = 0.0\n",
    "\n",
    "        result = {}\n",
    "\n",
    "        y_true = np.array([])\n",
    "        y_pred = np.array([])\n",
    "\n",
    "        X_batch, y_batch = tuple(t.to(args.device) for t in self.test_tensor)\n",
    "        with torch.no_grad():\n",
    "            outputs, loss = self.model.forward(X_batch, y_batch)\n",
    "            eval_loss = loss.mean().item()\n",
    "\n",
    "        return eval_loss\n",
    "\n",
    "    def train(self, epochs=20):\n",
    "        print(\"🍭🍭🍭🍭🍭\")\n",
    "        best_lost = float(\"inf\")\n",
    "        best_model = None\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0.0\n",
    "            for batch in self.train_loader:\n",
    "                loss = self.step(batch)\n",
    "                total_loss += loss\n",
    "            train_loss = total_loss / len(self.train_loader)\n",
    "            eval_loss = self.validation()\n",
    "            if eval_loss < best_lost:\n",
    "                best_lost = eval_loss\n",
    "                best_model = copy.deepcopy(self.model)\n",
    "            if (epoch + 1) == epochs or (epoch + 1) in [\n",
    "                c + 1 for c in range(epochs) if c % int(epochs / 4) == 0\n",
    "            ]:\n",
    "                print(\n",
    "                    f\"Epoch: {epoch:2}/{epochs:2} - train_loss: {train_loss:.4f} - test_loss: {eval_loss:4f}\"\n",
    "                )\n",
    "        self.model = best_model\n",
    "        self.model.eval()\n",
    "        return None\n",
    "\n",
    "    def evaluate_model(\n",
    "        self,\n",
    "        np_data=None,\n",
    "        df_eval=None,\n",
    "        plot=True,\n",
    "        scaled=True,\n",
    "        city=None,\n",
    "        k_steps=None,\n",
    "        scaler=None,\n",
    "    ):\n",
    "        assert scaled, \"data must be scaled\"\n",
    "        self.model.eval()\n",
    "        tensor_data = torch.from_numpy(np_data)  # np_data = bộ dữ liệu đưa vào\n",
    "        rmse_list = []\n",
    "        mae_list = []\n",
    "        mape_list = []\n",
    "\n",
    "        y_predicted_list = []\n",
    "        y_true_list = []\n",
    "\n",
    "        df_eval_true_inverse = df_eval[-args.test_size :]\n",
    "        for k_steps in range(1, args.n_predicted_period_months + 1):\n",
    "            y_predicted = []\n",
    "            for index in range(tensor_data.size(0) - args.look_back):\n",
    "                X = tensor_data[index : index + args.look_back, [0, 1, 2]]\n",
    "\n",
    "                yhat = self.model.predict(X.unsqueeze(0))\n",
    "                yhat = yhat.squeeze()\n",
    "\n",
    "                y_predicted.append(yhat.detach().cpu().numpy()[k_steps - 1])\n",
    "\n",
    "            moving = args.n_predicted_period_months - k_steps\n",
    "            y_predicted = y_predicted[moving : args.test_size + moving]\n",
    "\n",
    "            df_eval_pred_inverse = df_eval[-args.test_size :]\n",
    "            df_eval_pred_inverse[args.labels] = y_predicted  # step 1\n",
    "            y_predicted = scaler.inverse_transform(df_eval_pred_inverse.iloc[:, :-1])[\n",
    "                :, [-1]\n",
    "            ].reshape(args.test_size)\n",
    "            y_true = scaler.inverse_transform(df_eval_true_inverse.iloc[:, :-1])[\n",
    "                :, [-1]\n",
    "            ].reshape(args.test_size)\n",
    "\n",
    "            if plot == True:\n",
    "                plt.plot(y_predicted, label=\"predicted\")\n",
    "                plt.plot(y_true, label=\"actual\")\n",
    "                plt.title(f\"k-steps = {k_steps}\")\n",
    "                plt.legend()\n",
    "                plt.show()\n",
    "\n",
    "            rmse = mean_squared_error(y_true, y_predicted, squared=False)\n",
    "            mae = mean_absolute_error(y_true, y_predicted)\n",
    "            mape = mean_absolute_percentage_error(y_true, y_predicted)\n",
    "\n",
    "            rmse_list.append(rmse)\n",
    "            mae_list.append(mae)\n",
    "            mape_list.append(mape)\n",
    "\n",
    "            y_predicted_list.append(y_predicted)\n",
    "            y_true_list.append(y_true)\n",
    "        print(\"🍔🍔🍔🍔y_true_list🍔🍔🍔\")\n",
    "        print(y_true_list)\n",
    "        return y_true_list, y_predicted_list, rmse_list, mae_list, mape_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective and Suggest Hyperparams of Darts Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(model_name, trial, city):\n",
    "    specific_data = pd.read_csv(\n",
    "        output_process + city + \"_train_preprocessed.csv\",\n",
    "        parse_dates=True,\n",
    "        index_col=None,\n",
    "        encoding=\"unicode_escape\",\n",
    "    )\n",
    "    scaler = joblib.load(output_process + city + \"_train_scalerMinMaxNorm.save\")  # ok\n",
    "    selected_features = getDataWithSelectedFeature(city, 6)\n",
    "    df_train, df_eval = split_data(\n",
    "        specific_data, args.look_back, args.n_predicted_period_months\n",
    "    )\n",
    "    train = df_train[selected_features + [args.labels]].iloc[:, :].to_numpy()\n",
    "    test = df_eval[selected_features + [args.labels]].iloc[:, :].to_numpy()\n",
    "\n",
    "    train_X, train_y = to_supervised(\n",
    "        train, d_out=args.n_predicted_period_months, d_in=args.look_back\n",
    "    )\n",
    "    test_X, test_y = to_supervised(\n",
    "        test, d_out=args.n_predicted_period_months, d_in=args.look_back\n",
    "    )\n",
    "\n",
    "    print(\"🍤🍤🍤🍤🍤🍤🍤🍤🍤🍤🍤\")\n",
    "    print(\"SHAPE train_y \", train_y.shape)\n",
    "    print(\"SHAPE train_X \", train_X.shape)\n",
    "    print(\"SHAPE test_y \", test_y.shape)\n",
    "    print(\"SHAPE test_X \", test_X.shape)\n",
    "\n",
    "    train_tensor = TensorDataset(torch.from_numpy(train_X), torch.from_numpy(train_y))\n",
    "    test_tensor = (torch.from_numpy(test_X), torch.from_numpy(test_y))\n",
    "\n",
    "    train_loader = DataLoader(train_tensor, batch_size=args.batch_size, shuffle=True)\n",
    "\n",
    "    print(\"🍉🍉🍉🍉🍉🍉🍉🍉🍉🍉🍉🍉🍉\", model_name)\n",
    "    if model_name == \"CNN\":\n",
    "        # Define search parameters\n",
    "        args.epochs = trial.suggest_int(\"Epochs\", 100, 500, step=10)\n",
    "        learning_rate = trial.suggest_loguniform(\"Learning rate\", 1e-4, 1e-2)\n",
    "        num_filters = trial.suggest_categorical(\n",
    "            \"Num. filters\",\n",
    "            [\n",
    "                [64, 64, 64],\n",
    "                [100, 100, 100],\n",
    "                [128, 128, 128],\n",
    "                [16, 32, 64],\n",
    "                [32, 64, 128],\n",
    "            ],\n",
    "        )\n",
    "        dropout = trial.suggest_uniform(\"Dropout rate\", 0.01, 0.80)\n",
    "        # CNN model\n",
    "        trainer = Trainer(\n",
    "            model_type=\"CNN\",\n",
    "            learning_rate=learning_rate,\n",
    "            important_features=selected_features,\n",
    "            train_loader=train_loader,\n",
    "            test_tensor=test_tensor,\n",
    "            num_filters=num_filters,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "    elif model_name == \"LSTM\":\n",
    "        args.epochs = trial.suggest_int(\"Epochs\", 100, 500, step=10)\n",
    "        hidden_size = trial.suggest_int(\"Hidden size\", 128, 512, log=True)\n",
    "        n_layers = trial.suggest_int(\"n layers\", 2, 5)\n",
    "        learning_rate = trial.suggest_loguniform(\"Learning rate\", 1e-4, 1e-2)\n",
    "        trainer = Trainer(\n",
    "            model_type=\"LSTM\",\n",
    "            learning_rate=learning_rate,\n",
    "            important_features=selected_features,\n",
    "            train_loader=train_loader,\n",
    "            test_tensor=test_tensor,\n",
    "            n_layers=n_layers,\n",
    "            hidden_size=hidden_size,\n",
    "            look_back=args.look_back,\n",
    "        )\n",
    "    elif model_name == \"LSTM_ATT\":\n",
    "        args.epochs = trial.suggest_int(\"Epochs\", 100, 500, step=10)\n",
    "        hidden_size = trial.suggest_int(\"Hidden size\", 128, 512, log=True)\n",
    "        n_layers = trial.suggest_int(\"n layers\", 2, 10)\n",
    "        learning_rate = trial.suggest_loguniform(\"Learning rate\", 1e-4, 1e-2)\n",
    "        trainer = Trainer(\n",
    "            model_type=\"LSTM_ATT\",\n",
    "            learning_rate=learning_rate,\n",
    "            important_features=selected_features,\n",
    "            train_loader=train_loader,\n",
    "            test_tensor=test_tensor,\n",
    "            n_layers=n_layers,\n",
    "            hidden_size=hidden_size,\n",
    "            look_back=args.look_back,\n",
    "        )\n",
    "\n",
    "    elif model_name == \"TRANSFORMERS\":\n",
    "        n_layers = trial.suggest_int(\"n layers\", 2, 10)  # a\n",
    "        hidden_size = trial.suggest_int(\"Hidden size\", 128, 512, log=True)\n",
    "        learning_rate = trial.suggest_loguniform(\"Learning rate\", 1e-4, 1e-2)\n",
    "        dropout = trial.suggest_uniform(\"Dropout rate\", 0.01, 0.80)\n",
    "        args.epochs = trial.suggest_int(\"Epochs\", 100, 500, step=10)\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model_type=\"TRANSFORMERS\",\n",
    "            important_features=selected_features,\n",
    "            train_loader=train_loader,\n",
    "            test_tensor=test_tensor,\n",
    "            n_layers=n_layers,\n",
    "            hidden_size=hidden_size,\n",
    "            learning_rate=learning_rate,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "    # Train model\n",
    "    trainer.train(epochs=args.epochs)\n",
    "\n",
    "    # Evaluate model\n",
    "    y_true, y_pred, rmse_list, mae_list, mape_list = trainer.evaluate_model(\n",
    "        np_data=test, df_eval=df_eval, plot=True, scaled=True, city=city, scaler=scaler\n",
    "    )\n",
    "    # _, _, rmse, mae, = trainer.evaluate_model(np_data=test, plot=False, scaled=True, city=city, y_scaler=y_scaler)\n",
    "\n",
    "    return mean(mae_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concate_2_filter_str(listfilter=\"\"):\n",
    "    string_filter = \",\".join(str(e) for e in listfilter)\n",
    "    return string_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main run optimize and save\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# Main Cell for optimize\n",
    "#########################\n",
    "model_name_list = [\n",
    "    # \"CNN\",\n",
    "    \"LSTM\",\n",
    "    # \"LSTM_ATT\",\n",
    "    # \"TRANSFORMERS\",\n",
    "]\n",
    "\n",
    "dt_started = datetime.now()\n",
    "\n",
    "# Input param for Optimize Run\n",
    "ntry = args.ntry\n",
    "njob = args.njob\n",
    "\n",
    "\n",
    "# Lưu thông tin traceback study và error city trong quá trình optimize\n",
    "l_study_city = {}\n",
    "l_errCity = []\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for model_name in model_name_list:\n",
    "        print(f\"🔥🔥🔥🔥🔥🔥🔥🔥🔥{model_name}\")\n",
    "        for city_index in range(len(cities)):\n",
    "            print(f\"🔥🔥🔥🔥🔥🔥🔥🔥{cities[city_index]}\")\n",
    "            # Use Tree-structured Parzen Estimator sampler to minimise RMSE\n",
    "            sampler = optuna.samplers.TPESampler()\n",
    "            study = optuna.create_study(sampler=sampler, direction=\"minimize\")\n",
    "\n",
    "            # truyền multiple param vào trong biến trial\n",
    "            obj_func = lambda trial: objective(model_name, trial, cities[city_index])\n",
    "\n",
    "            try:\n",
    "                # Optimise over 100 trials\n",
    "                study.optimize(obj_func, n_trials=ntry, n_jobs=njob)\n",
    "\n",
    "                # Print results\n",
    "                print(\"Study statistics for : \")\n",
    "                print(\"  Number of finished trials: \", len(study.trials))\n",
    "\n",
    "                print(\"Best trial of city: \", cities[city_index])\n",
    "                best_trial = study.best_trial\n",
    "                print(\"  Value: \", best_trial.value)\n",
    "                if model_name == \"CNN\":\n",
    "                    # lưu best param vào trong biến toàn cục\n",
    "                    one_city_param = pd.DataFrame(\n",
    "                        {\n",
    "                            \"City\": [cities[city_index]],\n",
    "                            \"Alg_name\": \"CNN\",\n",
    "                            \"Best_value\": best_trial.value,\n",
    "                            \"n_try_opt\": ntry,\n",
    "                            \"n Feature\": args.n_features,  # dùng chung cho tất cả các model và algorithm\n",
    "                            \"Batch Size\": args.batch_size,  # dùng chung cho tất cả các model và algorithm\n",
    "                            \"Lookback Window\": args.look_back,  # dùng chung cho tất cả các tỉnh của CNN\n",
    "                            \"Epochs\": best_trial.params[\"Epochs\"],\n",
    "                            \"Hiden Size\": \"\",  # CNN không quan tâm\n",
    "                            \"n Layers\": \"\",  # CNN không quan tâm\n",
    "                            \"Learning rate\": best_trial.params[\"Learning rate\"],\n",
    "                            \"Num. filters\": concate_2_filter_str(\n",
    "                                best_trial.params[\"Num. filters\"]\n",
    "                            ),  # đoạn này ép kiểu không sẽ thành x 3 do cấu trúc []\n",
    "                            \"Dropout rate\": best_trial.params[\"Dropout rate\"],\n",
    "                        },\n",
    "                        index=[0],\n",
    "                    )\n",
    "                elif model_name == \"LSTM\":\n",
    "                    # lưu best param vào trong biến toàn cục\n",
    "                    one_city_param = pd.DataFrame(\n",
    "                        {\n",
    "                            \"City\": [cities[city_index]],\n",
    "                            \"Alg_name\": \"LSTM\",\n",
    "                            \"Best_value\": best_trial.value,\n",
    "                            \"n_try_opt\": ntry,\n",
    "                            \"n Feature\": args.n_features,  # dùng chung cho tất cả các model và algorithm\n",
    "                            \"Batch Size\": args.batch_size,  # dùng chung cho tất cả các model và algorithm\n",
    "                            \"Lookback Window\": args.look_back,\n",
    "                            \"Epochs\": best_trial.params[\"Epochs\"],\n",
    "                            \"Hiden Size\": best_trial.params[\"Hidden size\"],\n",
    "                            \"n Layers\": best_trial.params[\"n layers\"],\n",
    "                            \"Learning rate\": best_trial.params[\"Learning rate\"],\n",
    "                            \"Num. filters\": \"\",  # đoạn này ép kiểu không sẽ thành x 3 do cấu trúc []\n",
    "                            \"Dropout rate\": \"\",\n",
    "                        },\n",
    "                        index=[0],\n",
    "                    )\n",
    "                elif model_name == \"LSTM_ATT\":\n",
    "                    one_city_param = pd.DataFrame(\n",
    "                        {\n",
    "                            \"City\": [cities[city_index]],\n",
    "                            \"Alg_name\": \"LSTM_ATT\",\n",
    "                            \"Best_value\": best_trial.value,\n",
    "                            \"n_try_opt\": ntry,\n",
    "                            \"n Feature\": args.n_features,  # dùng chung cho tất cả các model và algorithm\n",
    "                            \"Batch Size\": args.batch_size,  # dùng chung cho tất cả các model và algorithm\n",
    "                            \"Lookback Window\": args.look_back,\n",
    "                            \"Epochs\": best_trial.params[\"Epochs\"],\n",
    "                            \"Hiden Size\": best_trial.params[\"Hidden size\"],\n",
    "                            \"n Layers\": best_trial.params[\"n layers\"],\n",
    "                            \"Learning rate\": best_trial.params[\"Learning rate\"],\n",
    "                            \"Num. filters\": \"\",\n",
    "                            \"Dropout rate\": \"\",\n",
    "                        },\n",
    "                        index=[0],\n",
    "                    )\n",
    "                elif model_name == \"TRANSFORMERS\":\n",
    "                    one_city_param = pd.DataFrame(\n",
    "                        {\n",
    "                            \"City\": [cities[city_index]],\n",
    "                            \"Alg_name\": \"transformer\",\n",
    "                            \"Best_value\": best_trial.value,\n",
    "                            \"n_try_opt\": ntry,\n",
    "                            \"n Feature\": args.n_features,  # dùng chung cho tất cả các model và algorithm\n",
    "                            \"Batch Size\": args.batch_size,  # dùng chung cho tất cả các model và algorithm\n",
    "                            \"Lookback Window\": args.look_back,\n",
    "                            \"Epochs\": best_trial.params[\"Epochs\"],\n",
    "                            \"Hidden Size\": best_trial.params[\"Hidden size\"],\n",
    "                            \"n Layers\": best_trial.params[\"n layers\"],\n",
    "                            \"Learning rate\": best_trial.params[\"Learning rate\"],\n",
    "                            \"Num. filters\": \"\",  # Transformer không dùng\n",
    "                            \"Dropout rate\": best_trial.params[\"Dropout rate\"],\n",
    "                        },\n",
    "                        index=[0],\n",
    "                    )\n",
    "                folder_path = f\"opt_results/opt_res_ml_26102023/{model_name}/\"\n",
    "                file_path = (\n",
    "                    folder_path\n",
    "                    + f\"261023_DF_opt_hyperparam_{model_name}_multi-nstep.xlsx\"\n",
    "                )\n",
    "                if os.path.isfile(file_path):\n",
    "                    with pd.ExcelWriter(\n",
    "                        file_path,\n",
    "                        mode=\"a\",\n",
    "                        engine=\"openpyxl\",\n",
    "                        if_sheet_exists=\"overlay\",\n",
    "                    ) as writer:\n",
    "                        one_city_param.to_excel(\n",
    "                            writer, header=None, startrow=city_index + 1, index=False\n",
    "                        )\n",
    "                else:\n",
    "                    if not (os.path.isdir(folder_path)):\n",
    "                        os.mkdir(folder_path)\n",
    "                    with pd.ExcelWriter(file_path, engine=\"openpyxl\") as writer:\n",
    "                        one_city_param.to_excel(\n",
    "                            writer, startrow=city_index, index=False\n",
    "                        )\n",
    "            except:  # có error thì lưu vào l_errCity để check lại sau\n",
    "                l_errCity.append(cities[city_index])\n",
    "    # lưu kết quả vào file CNN\n",
    "#   best_param.to_excel(prj_path_opt+'cnn/diarrhoea_opt_hyperparam_cnn.xlsx')\n",
    "\n",
    "dt_ended = datetime.now()\n",
    "print(\"kết thúc study trong:\", round((dt_ended - dt_started).total_seconds() / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send_to_telegram(\"Chạy xong optimize rùiii!!Vô check thuiii!!!\" )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
