{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error,\n",
    "    mean_absolute_percentage_error,\n",
    ")\n",
    "import optuna\n",
    "import joblib\n",
    "from statistics import mean\n",
    "import math\n",
    "import torch\n",
    "\n",
    "from darts import TimeSeries\n",
    "from darts.models import BlockRNNModel, NBEATSModel, NHiTSModel, TCNModel, TFTModel\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from darts.utils.likelihood_models import GaussianLikelihood\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prj_path = \"../\"\n",
    "data_path = prj_path + \"data/new_data/DH/squeezed/\"\n",
    "prj_path_opt = prj_path + \"optimize_hyperparam/opt_results/\"\n",
    "output_process = prj_path + \"data/new_data/DH/processed_data/\"\n",
    "output_featureselection = prj_path + \"data/new_data/DH/feature_selection/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cities = [\n",
    "    \"An Giang\",\n",
    "    \"BR V≈©ng T√†u\",\n",
    "    \"B√¨nh Ph∆∞·ªõc\",\n",
    "    \"B√¨nh Thu·∫≠n\",\n",
    "    \"B√¨nh ƒê·ªãnh\",\n",
    "    \"B·∫°c Li√™u\",\n",
    "    \"B·∫Øc K·∫°n\",\n",
    "    \"B·∫Øc Giang\",\n",
    "    \"Cao B·∫±ng\",\n",
    "    \"C√† Mau\",\n",
    "    \"C·∫ßn Th∆°\",\n",
    "    \"Gia Lai\",\n",
    "    \"H√† Giang\",\n",
    "    \"H√† N·ªôi\",\n",
    "    \"H√† Tƒ©nh\",\n",
    "    \"H√≤a B√¨nh\",\n",
    "    \"H∆∞ng Y√™n\",\n",
    "    \"H·∫£i D∆∞∆°ng\",\n",
    "    \"H·∫£i Ph√≤ng\",\n",
    "    \"Kh√°nh H√≤a\",\n",
    "    \"Ki√™n Giang\",\n",
    "    \"Kon Tum\",\n",
    "    \"Lai Ch√¢u\",\n",
    "    \"Long An\",\n",
    "    \"L√†o Cai\",\n",
    "    \"L√¢m ƒê·ªìng\",\n",
    "    \"L·∫°ng S∆°n\",\n",
    "    \"Nam ƒê·ªãnh\",\n",
    "    \"Ngh·ªá An\",\n",
    "    \"Ninh B√¨nh\",\n",
    "    \"Ninh Thu·∫≠n\",\n",
    "    \"Ph√∫ Th·ªç\",\n",
    "    \"Ph√∫ Y√™n\",\n",
    "    \"Qu·∫£ng B√¨nh\",\n",
    "    \"Qu·∫£ng Nam\",\n",
    "    \"Qu·∫£ng Ng√£i\",\n",
    "    \"Qu·∫£ng Ninh\",\n",
    "    \"Qu·∫£ng Tr·ªã\",\n",
    "    \"S√≥c TrƒÉng\",\n",
    "    \"S∆°n La\",\n",
    "    \"TT Hu·∫ø\",\n",
    "    \"Thanh H√≥a\",\n",
    "    \"Th√°i B√¨nh\",\n",
    "    \"Th√°i Nguy√™n\",\n",
    "    \"Ti·ªÅn Giang\",\n",
    "    \"Tr√† Vinh\",\n",
    "    \"Tuy√™n Quang\",\n",
    "    \"T√¢y Ninh\",\n",
    "    \"Vƒ©nh Ph√∫c\",\n",
    "    \"Y√™n B√°i\",\n",
    "    \"ƒêi·ªán Bi√™n\",\n",
    "    \"ƒê√† N·∫µng\",\n",
    "    \"ƒê·∫Øk N√¥ng\",\n",
    "    \"ƒê·∫Øk L·∫Øk\",\n",
    "    \"ƒê·ªìng Th√°p\",\n",
    "]\n",
    "cities = [\"An Giang\", \"BR V≈©ng T√†u\", \"B√¨nh Ph∆∞·ªõc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters as args using the Configuration class\n",
    "class Configuration:\n",
    "    def __init__(self):\n",
    "        # l·∫•y b·ªô test d√†i 36 th√°ng = 3 nƒÉm\n",
    "        self.test_size = 36\n",
    "        # l√† nh√¨n v√†o d·ªØ li·ªáu tr∆∞·ªõc 3 th√°ng v√† d·ª± ph√≥ng\n",
    "        self.look_back = 3\n",
    "        # d·ª± ph√≥ng n-step trong 6 th√°ng\n",
    "        self.n_predicted_period_months = 6\n",
    "        self.n_features = 3\n",
    "        self.seed = 42\n",
    "        # m·ªói ph·∫ßn t·ª≠ x trong t·∫≠p suppervise c√≥ ƒë·ªô l·ªõn l√† 16 = 16 th√°ng\n",
    "        self.batch_size = 16\n",
    "        self.device = torch.device(\"cuda\")\n",
    "        # self.device = torch.device(\"cpu\")\n",
    "        self.epochs = 300\n",
    "        # others\n",
    "        self.labels = \"Dengue_fever_rates\"\n",
    "        # Input param for Optimize Run\n",
    "        self.ntry = 1\n",
    "        self.njob = 1\n",
    "\n",
    "\n",
    "args = Configuration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seeding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    import random\n",
    "    import numpy as np\n",
    "    import torch\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "seed_everything(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supporting functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "def send_to_telegram(message):\n",
    "    apiToken = \"5908735099:AAGVSLrW62aXPBP-GrMvxoVgMsuJxXJpP1Q\"\n",
    "    chatID = \"@ptn_announcement\"\n",
    "    apiURL = f\"https://api.telegram.org/bot{apiToken}/sendMessage\"\n",
    "\n",
    "    try:\n",
    "        response = requests.post(apiURL, json={\"chat_id\": chatID, \"text\": message})\n",
    "        print(response.text)\n",
    "    except Exception as e:\n",
    "        message_error = \"B·ªã l·ªói r√πi: \" + str(e)\n",
    "        response = requests.post(\n",
    "            apiURL, json={\"chat_id\": chatID, \"text\": message_error}\n",
    "        )\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict_all_city_data():\n",
    "    cities_data = {}\n",
    "    for city in cities:\n",
    "        city_result = pd.read_excel(\n",
    "            prj_path + \"data/new_data/DH/squeezed/squeezed_\" + city + \".xlsx\"\n",
    "        )\n",
    "        \"\"\"Get all data from all city in 1997 - 2016\"\"\"\n",
    "        city_result = city_result.loc[city_result[\"year_month\"] < \"2017-1-1\"]\n",
    "        cities_data[city] = city_result\n",
    "    return cities_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data (pre-)processing functions\n",
    "# modification\n",
    "def get_city_data(city_name, dict_full_data):\n",
    "    \"\"\"Returns Dengue fever rate and climate data\"\"\"\n",
    "    city_data = dict_full_data[city_name].drop(\n",
    "        columns=[\n",
    "            \"Diarrhoea_cases\",\n",
    "            \"Diarrhoea_rates\",\n",
    "            \"province\",\n",
    "            \"Influenza_rates\",\n",
    "            \"Influenza_cases\",\n",
    "            \"Dengue_fever_cases\",\n",
    "            \"year\",\n",
    "            \"month\",\n",
    "        ],\n",
    "        axis=1,\n",
    "        inplace=False,\n",
    "    )\n",
    "    return city_data\n",
    "\n",
    "\n",
    "def convert_to_stationary(city_data):\n",
    "    \"\"\"Subtracts previous value for all cols except disease rates\"\"\"\n",
    "    for col_name in city_data.columns:\n",
    "        if col_name != \"Dengue_fever_rates\":\n",
    "            try:\n",
    "                city_data[col_name] = city_data[col_name] - city_data[col_name].shift()\n",
    "            except:\n",
    "                print(col_name)\n",
    "    return city_data\n",
    "\n",
    "\n",
    "def impute_missing_value(city_data):\n",
    "    \"\"\"\n",
    "    Imputes 0 for first 12 months,\n",
    "    last year's value for months 12-24,\n",
    "    and minimum value of last two years for months 25+\n",
    "    \"\"\"\n",
    "    for col in city_data.columns:\n",
    "        for index in range(len(city_data[col])):\n",
    "            if np.isnan(city_data[col].iloc[index]):\n",
    "                if index < 12:\n",
    "                    city_data[col].iloc[index] = 0\n",
    "                elif index >= 12 and index <= 24:\n",
    "                    city_data[col].iloc[index] = city_data[col].iloc[index - 12]\n",
    "                else:\n",
    "                    city_data[col].iloc[index] = min(\n",
    "                        city_data[col].iloc[index - 12], city_data[col].iloc[index - 24]\n",
    "                    )\n",
    "    return city_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_full_data(dict_full_data):\n",
    "    climate_and_disease_feats = [\n",
    "        \"Total_Evaporation\",\n",
    "        \"Total_Rainfall\",\n",
    "        \"Max_Daily_Rainfall\",\n",
    "        \"n_raining_days\",\n",
    "        \"Average_temperature\",\n",
    "        \"Max_Average_Temperature\",\n",
    "        \"Min_Average_Temperature\",\n",
    "        \"Max_Absolute_Temperature\",\n",
    "        \"Min_Absolute_Temperature\",\n",
    "        \"Average_Humidity\",\n",
    "        \"Min_Humidity\",\n",
    "        \"n_hours_sunshine\",\n",
    "        \"Dengue_fever_rates\",\n",
    "    ]\n",
    "    for city in cities:\n",
    "        city_data = get_city_data(city_name=city, dict_full_data=dict_full_data)\n",
    "        city_data_features = city_data[climate_and_disease_feats]\n",
    "        city_data_features = impute_missing_value(city_data_features)\n",
    "        city_data_features = convert_to_stationary(city_data_features)\n",
    "        city_data_features.dropna(inplace=True)\n",
    "        city_data_features.loc[:, \"year_month\"] = city_data[\"year_month\"]\n",
    "        dict_full_data[city] = city_data_features\n",
    "    return dict_full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, look_back, n_nextstep=args.n_predicted_period_months):\n",
    "    \"\"\"Splits data into train and test sets based on args (Configuration class)\"\"\"\n",
    "    train = data[: -args.test_size]\n",
    "    test = data[-args.test_size - look_back - (n_nextstep - 1) :]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_supervised(data, d_out, d_in):\n",
    "    \"\"\"\n",
    "    Frames time-series as supervised learning dataset.\n",
    "\n",
    "    Args:\n",
    "      d_in: lookback window\n",
    "      d_out: number of predicted months\n",
    "\n",
    "    Returns:\n",
    "      Numpy arrays of disease incidence (y) and other predictors (X)\n",
    "    \"\"\"\n",
    "    X, y = list(), list()\n",
    "    for index, _ in enumerate(data):\n",
    "        in_end = index + d_in\n",
    "        out_end = in_end + d_out\n",
    "        if out_end <= len(data):\n",
    "            X.append(data[index:in_end, :-1])\n",
    "            y.append(data[in_end:out_end, -1])\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataWithSelectedFeature(city, next_predicted_month):\n",
    "    selected_feature = []\n",
    "    df = pd.read_csv(\n",
    "        output_featureselection\n",
    "        + str(next_predicted_month)\n",
    "        + \"step_feature_selection_3_most.csv\"\n",
    "    )\n",
    "    for row in range(len(df)):\n",
    "        if df[\"City\"][row] == city:\n",
    "            selected_feature.append(df[\"1st_Feature\"][row])\n",
    "            selected_feature.append(df[\"2nd_Feature\"][row])\n",
    "            selected_feature.append(df[\"3rd_Feature\"][row])\n",
    "    return selected_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(df_full, model, location, feature_list, labels, scaler):\n",
    "    \"\"\"\n",
    "    $df: pandas.DataFrame object containing data for training and testing model:\n",
    "    $model: darts model object\n",
    "    $feature_list: Names of the features used as model input\n",
    "    $label: the value the model will be trained to predict\n",
    "    $scaler: scaler object. Note: the scaler will be fitted on training data and applied to test data\n",
    "    $lags: how much to look back into the past to output prediction\n",
    "    $split_index: the point at which to divide train and test_data\n",
    "\n",
    "    \"\"\"\n",
    "    df_train, df_eval = split_data(\n",
    "        df_full, args.look_back, args.n_predicted_period_months\n",
    "    )\n",
    "\n",
    "    print(\"üçãüçãüçãüçãüçãCheck var feature selectio n: \",feature_list)\n",
    "    x_train = TimeSeries.from_dataframe(df_train, \"year_month\", feature_list)\n",
    "    y_train = TimeSeries.from_dataframe(df_train, \"year_month\", labels)\n",
    "    x_test = TimeSeries.from_dataframe(df_eval, \"year_month\", feature_list)\n",
    "    y_test = TimeSeries.from_dataframe(df_eval, \"year_month\", labels)\n",
    "\n",
    "    # C√°ch m·ªôt kho·∫£ng ph·∫ßn n√†y cho x_train th√¨ m·ªõi ƒë∆∞a ƒëc chu·ªói x_test v·ª´a ƒë·ªß ƒë·ªÉ d·ª± ƒëo√°n 36 l·∫ßn 6step\n",
    "    # ValueError: For the given forecasting horizon `n=xxx`, the provided\n",
    "    # past covariates at dataset index `0` do not extend far enough into\n",
    "    # the future. As `n > output_chunk_length` the past covariates must\n",
    "    # start at time step `xxxxxx`, whereas now they start at\n",
    "    # time step `xxxxx`.\n",
    "\n",
    "    x_train_back = x_train[: -(args.n_predicted_period_months - 1)]\n",
    "    y_train_back = y_train[: -(args.n_predicted_period_months - 1)]\n",
    "\n",
    "    predict_list = []\n",
    "    for i in range(args.test_size + (args.n_predicted_period_months - 1)):\n",
    "        model.fit(y_train_back, past_covariates=x_train_back)\n",
    "        prediction = model.predict(\n",
    "            6,\n",
    "            past_covariates=x_test[0 : i + args.look_back],\n",
    "            num_samples=1,\n",
    "            verbose=False,\n",
    "        )\n",
    "        x_train_back = x_train_back.append(x_test[i + 3])\n",
    "        predict_list.append(np.array(prediction._xa).squeeze())\n",
    "    y_pred_list = []\n",
    "    for step in range(args.n_predicted_period_months):\n",
    "        moving = args.n_predicted_period_months - 1 - step\n",
    "        y_pred_list.append(\n",
    "            [x[step] for x in predict_list][moving : args.test_size + moving]\n",
    "        )\n",
    "\n",
    "    df_eval_true_inverse = df_full[-args.test_size :]\n",
    "    y_true = scaler.inverse_transform(df_eval_true_inverse.iloc[:, :-1])[\n",
    "        :, [-1]\n",
    "    ].reshape(args.test_size)\n",
    "\n",
    "    y_pred_inverse_list = []\n",
    "    for step in range(args.n_predicted_period_months):\n",
    "        df_eval_pred_inverse = df_full[-args.test_size :]\n",
    "        df_eval_pred_inverse[args.labels] = y_pred_list[step]  # step 1\n",
    "        y_pred_inverse = scaler.inverse_transform(df_eval_pred_inverse.iloc[:, :-1])[\n",
    "            :, [-1]\n",
    "        ].reshape(args.test_size)\n",
    "        y_pred_inverse_list.append(y_pred_inverse)\n",
    "\n",
    "    y_pred_inverse_list\n",
    "    df_compare_test_predict = pd.DataFrame(\n",
    "        {\n",
    "            \"y_true\": y_true,\n",
    "            \"y_pred_1step\": y_pred_inverse_list[0],\n",
    "            \"y_pred_2step\": y_pred_inverse_list[1],\n",
    "            \"y_pred_3step\": y_pred_inverse_list[2],\n",
    "            \"y_pred_4step\": y_pred_inverse_list[3],\n",
    "            \"y_pred_5step\": y_pred_inverse_list[4],\n",
    "            \"y_pred_6step\": y_pred_inverse_list[5],\n",
    "        }\n",
    "    )\n",
    "    df_compare_test_predict.plot()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    mse_nstep = []\n",
    "    mae_nstep = []\n",
    "    rmse_nstep = []\n",
    "    mape_nstep = []\n",
    "    for step in range(args.n_predicted_period_months):\n",
    "        mse_nstep.append(mean_squared_error(y_true, y_pred_inverse_list[step]))\n",
    "        mae_nstep.append(mean_absolute_error(y_true, y_pred_inverse_list[step]))\n",
    "        rmse_nstep.append(mse_nstep[step] ** 0.5)\n",
    "        mape_nstep.append(\n",
    "            mean_absolute_percentage_error(y_true, y_pred_inverse_list[step])\n",
    "        )\n",
    "    return (\n",
    "        model,\n",
    "        y_true,\n",
    "        y_pred_inverse_list,\n",
    "        mse_nstep,\n",
    "        mae_nstep,\n",
    "        rmse_nstep,\n",
    "        mape_nstep,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_prediction_for_location(\n",
    "    df_full, model, location, feature_list, labels, scaler\n",
    "):\n",
    "    \"\"\"train and generate prediction for a province\n",
    "    df: DataFrame object containing features and label(s) for training model\n",
    "    localtion: location_name\n",
    "    feature_list: list of features used as model input,  must be among the column names of df\n",
    "    labels: the values model will be trained to predict\n",
    "    scaler: sklearn scaler object\n",
    "    lags: how long into the past to look back when making prediction\n",
    "    split_index: the point at which to divide data into the train and test subsets.\n",
    "    \"\"\"\n",
    "    (\n",
    "        model,\n",
    "        y_true,\n",
    "        y_pred_inverse_list,\n",
    "        mse_nstep,\n",
    "        mae_nstep,\n",
    "        rmse_nstep,\n",
    "        mape_nstep,\n",
    "    ) = train_and_evaluate(df_full, model, location, feature_list, labels, scaler)\n",
    "\n",
    "    df_prediction = pd.DataFrame(\n",
    "        {\n",
    "            \"Date\": df_full[\"year_month\"][-args.test_size :],\n",
    "            \"Observed\": y_true[-args.test_size :],\n",
    "            f\"{1}-month\": y_pred_inverse_list[0],\n",
    "            f\"{2}-month\": y_pred_inverse_list[1],\n",
    "            f\"{3}-month\": y_pred_inverse_list[2],\n",
    "            f\"{4}-month\": y_pred_inverse_list[3],\n",
    "            f\"{5}-month\": y_pred_inverse_list[4],\n",
    "            f\"{6}-month\": y_pred_inverse_list[5],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    df_prediction[\"City\"] = location\n",
    "    for step in range(args.n_predicted_period_months):\n",
    "        df_prediction[f\"RMSE_{step+1}-month\"] = rmse_nstep[step]\n",
    "        df_prediction[f\"MAE_{step+1}-month\"] = mae_nstep[step]\n",
    "        df_prediction[f\"MAPE_{step+1}-month\"] = mape_nstep[step]\n",
    "        df_prediction[f\"MSE_{step+1}-month\"] = mse_nstep[step]\n",
    "    print(\"‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è\")\n",
    "    display(df_prediction.head(5))\n",
    "    print(mean(mae_nstep))\n",
    "    return mean(mae_nstep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Pytorch LSTM model\n",
    "class MultiVariateLSTM(nn.Module):\n",
    "    def __init__(self, n_feature=3, n_layers=2, hidden_size=50):\n",
    "        super(MultiVariateLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=n_feature,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=n_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.linear = nn.Linear(hidden_size, args.n_predicted_period_months)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.sigma = nn.Parameter(torch.ones(args.n_predicted_period_months))\n",
    "\n",
    "    def forward(self, X_batch, y_batch=None):\n",
    "        output, (last_hidden, _) = self.lstm(X_batch)\n",
    "        last_hidden_vector = output[:, -1, :]\n",
    "        y_predicted = self.linear(last_hidden_vector)\n",
    "        if y_batch != None:\n",
    "            assert y_predicted.size() == y_batch.size()\n",
    "            loss = self.loss_fn(y_predicted, y_batch)\n",
    "            loss = 0.5 * loss / self.sigma**2\n",
    "            loss = loss.sum() + torch.log(1 + self.sigma.prod())\n",
    "            return y_predicted, loss\n",
    "            # return y_predicted, self.loss_fn(y_predicted, y_batch)\n",
    "        else:\n",
    "            return y_predicted\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = torch.tensor(X, device=args.device)\n",
    "        return self.forward(X)\n",
    "\n",
    "\n",
    "# Define Pytorch LSTM-ATT model\n",
    "class MultiVariateLSTM_Attention(nn.Module):\n",
    "    def __init__(self, n_feature=3, n_layers=2, hidden_size=50):\n",
    "        super(MultiVariateLSTM_Attention, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=n_feature,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=n_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.attention_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        # self.linear = nn.Linear(hidden_size*2, args.n_predicted_period_months)\n",
    "        self.linear = nn.Linear(hidden_size, args.n_predicted_period_months)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.sigma = nn.Parameter(torch.ones(args.n_predicted_period_months))\n",
    "\n",
    "    def forward(self, X_batch, y_batch=None):\n",
    "        output, (last_hidden, _) = self.lstm(X_batch)\n",
    "        last_hidden_vector = last_hidden[-1]\n",
    "        remain_hidden_vector = output\n",
    "        e_t = remain_hidden_vector.bmm(\n",
    "            self.attention_linear(last_hidden_vector).unsqueeze(2)\n",
    "        ).squeeze(-1)\n",
    "        alpha_t = F.softmax(e_t, dim=1)\n",
    "        attenion_vector = (\n",
    "            remain_hidden_vector.transpose(2, 1).bmm(alpha_t.unsqueeze(2)).squeeze(-1)\n",
    "        )\n",
    "        # combine_vector = torch.cat((last_hidden_vector, attenion_vector), dim=1)\n",
    "        # combine_vector = last_hidden_vector + attenion_vector\n",
    "        y_predicted = self.linear(attenion_vector)\n",
    "        if y_batch != None:\n",
    "            # assert y_predicted.size() == y_batch.size()\n",
    "            print(\"üç§üç§üç§üç§üç§üç§üç§üç§üç§üç§üç§\")\n",
    "            print(\"SHAPE y_predicted \", y_predicted.shape)\n",
    "            print(\"SHAPE y_batch \", y_batch.shape)\n",
    "\n",
    "            loss = self.loss_fn(y_predicted, y_batch)\n",
    "            loss = 0.5 * loss / self.sigma**2\n",
    "            loss = loss.sum() + torch.log(1 + self.sigma.prod())\n",
    "            return y_predicted, loss\n",
    "            # return y_predicted, self.loss_fn(y_predicted, y_batch)\n",
    "        else:\n",
    "            return y_predicted\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = torch.tensor(X, device=args.device)\n",
    "        return self.forward(X)\n",
    "\n",
    "\n",
    "# Define Pytorch CNN model\n",
    "class MultivariateCNN(nn.Module):\n",
    "    def __init__(self, num_filters=[100, 100, 100], dropout=0.01):\n",
    "        super(MultivariateCNN, self).__init__()\n",
    "        self.loss_fn = loss = nn.MSELoss()\n",
    "        self.filter_sizes = [1, 2, 3]\n",
    "        self.conv1d_list = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv1d(args.n_features, num_filters[i], self.filter_sizes[i])\n",
    "                for i in range(len(self.filter_sizes))\n",
    "            ]\n",
    "        )\n",
    "        self.linear = nn.Linear(np.sum(num_filters), args.n_predicted_period_months)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.sigma = nn.Parameter(torch.ones(args.n_predicted_period_months))\n",
    "\n",
    "    def forward(self, X_batch, y_batch=None):\n",
    "        X_batch = X_batch.permute(0, 2, 1)  # (batch_size, n_features, n_look_back)\n",
    "        X_conv_list = [F.relu(conv1d(X_batch)) for conv1d in self.conv1d_list]\n",
    "        X_pool_list = [\n",
    "            F.max_pool1d(x_conv, kernel_size=x_conv.shape[2]) for x_conv in X_conv_list\n",
    "        ]\n",
    "        X_fc = torch.cat([x_pool.squeeze(dim=2) for x_pool in X_pool_list], dim=1)\n",
    "        y_predicted = self.linear(self.dropout(X_fc))\n",
    "        if y_batch != None:\n",
    "            # assert y_predicted.size() == y_batch.size()\n",
    "            loss = self.loss_fn(y_predicted, y_batch)\n",
    "            loss = 0.5 * loss / self.sigma**2\n",
    "            loss = loss.sum() + torch.log(1 + self.sigma.prod())\n",
    "            return y_predicted, loss\n",
    "            # return y_predicted, self.loss_fn(y_predicted, y_batch)\n",
    "        else:\n",
    "            return y_predicted\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = torch.tensor(X, device=args.device)\n",
    "        return self.forward(X)\n",
    "\n",
    "\n",
    "# Define Pytorch Transformer model\n",
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, d_model=3, n_feature=3, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        pe = torch.zeros(n_feature, d_model)\n",
    "        for pos in range(n_feature):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** (2 * i / d_model)))\n",
    "                if i + 1 < d_model:\n",
    "                    pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * i + 1) / d_model)))\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x * math.sqrt(self.d_model)\n",
    "        length = x.size(1)\n",
    "        pe = Variable(self.pe[:, :length], requires_grad=False)\n",
    "        if x.is_cuda:\n",
    "            pe.cuda()\n",
    "        x = x + pe\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, d_input=3, n_head=3, hidden_size=256, n_layers=3, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.pe = PositionalEncoder(dropout=dropout)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(\n",
    "            d_model=d_input,\n",
    "            nhead=n_head,\n",
    "            dim_feedforward=hidden_size,\n",
    "            dropout=dropout,\n",
    "            activation=\"gelu\",\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, n_layers)\n",
    "        self.decoder = nn.Linear(d_input * n_head, args.n_predicted_period_months)\n",
    "        self.sigma = nn.Parameter(torch.ones(args.n_predicted_period_months))\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "    def forward(self, X_batch, y_batch=None):\n",
    "        X_batch = self.pe(X_batch)\n",
    "        X_batch = self.transformer_encoder(X_batch)\n",
    "        X_batch = X_batch.view(X_batch.size(0), -1)\n",
    "\n",
    "        y_predicted = self.decoder(X_batch)\n",
    "        if y_batch != None:\n",
    "            assert y_predicted.size() == y_batch.size()\n",
    "            loss = self.loss_fn(y_predicted, y_batch)\n",
    "            loss = 0.5 * loss / self.sigma**2\n",
    "            loss = loss.sum() + torch.log(1 + self.sigma.prod())\n",
    "            return y_predicted, loss\n",
    "        else:\n",
    "            return y_predicted\n",
    "        return X_batch\n",
    "\n",
    "    def predict(self, X):\n",
    "        with torch.no_grad():\n",
    "            X = torch.tensor(X, device=args.device)\n",
    "        return self.forward(X).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create class to train and evaluate models\n",
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_type,\n",
    "        learning_rate,\n",
    "        important_features,\n",
    "        train_loader,\n",
    "        test_tensor,\n",
    "        n_layers=2,\n",
    "        hidden_size=128,\n",
    "        num_filters=[100, 100, 100],\n",
    "        dropout=0.01,\n",
    "        look_back=\"\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialise trainer, allowing input of LSTM, LSTM-ATT, or CNN\n",
    "        hyperparameters. Adam optimiser used for all models.\n",
    "        \"\"\"\n",
    "        self.model_type = model_type\n",
    "        self.look_back = look_back\n",
    "        self.model = self.init_model(\n",
    "            model_type, n_layers, hidden_size, num_filters, dropout\n",
    "        )\n",
    "        self.model.double().to(args.device)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        self.important_features, self.train_loader, self.test_tensor = (\n",
    "            important_features,\n",
    "            train_loader,\n",
    "            test_tensor,\n",
    "        )\n",
    "\n",
    "    def init_model(self, model_type, n_layers, hidden_size, num_filters, dropout):\n",
    "        \"\"\"Initialise a model based on whether LSTM, LSTM-ATT, or CNN is chosen.\"\"\"\n",
    "        print(\"üå∑üå∑üå∑üå∑üå∑üå∑üå∑üå∑üå∑\")\n",
    "        print(model_type)\n",
    "        if model_type == \"LSTM\":\n",
    "            model = MultiVariateLSTM(args.n_features, n_layers, hidden_size)\n",
    "        elif model_type == \"LSTM_ATT\":\n",
    "            model = MultiVariateLSTM_Attention(args.n_features, n_layers, hidden_size)\n",
    "        elif model_type == \"CNN\":\n",
    "            model = MultivariateCNN(num_filters, dropout)\n",
    "        elif model_type == \"TRANSFORMERS\":\n",
    "            model = TransformerModel(\n",
    "                d_input=args.look_back,\n",
    "                n_head=3,\n",
    "                hidden_size=hidden_size,\n",
    "                n_layers=n_layers,\n",
    "                dropout=dropout,\n",
    "            )\n",
    "        return model\n",
    "\n",
    "    def step(self, batch):\n",
    "        self.model.train()\n",
    "        X_batch, y_batch = tuple(t.to(args.device) for t in batch)\n",
    "        self.optimizer.zero_grad()\n",
    "        y_pred, loss = self.model.forward(X_batch, y_batch)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.mean().item()\n",
    "\n",
    "    def validation(self):\n",
    "        self.model.eval()\n",
    "        eval_loss = 0.0\n",
    "\n",
    "        result = {}\n",
    "\n",
    "        y_true = np.array([])\n",
    "        y_pred = np.array([])\n",
    "\n",
    "        X_batch, y_batch = tuple(t.to(args.device) for t in self.test_tensor)\n",
    "        with torch.no_grad():\n",
    "            outputs, loss = self.model.forward(X_batch, y_batch)\n",
    "            eval_loss = loss.mean().item()\n",
    "\n",
    "        return eval_loss\n",
    "\n",
    "    def train(self, epochs=20):\n",
    "        print(\"üç≠üç≠üç≠üç≠üç≠\")\n",
    "        best_lost = float(\"inf\")\n",
    "        best_model = None\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0.0\n",
    "            for batch in self.train_loader:\n",
    "                loss = self.step(batch)\n",
    "                total_loss += loss\n",
    "            train_loss = total_loss / len(self.train_loader)\n",
    "            eval_loss = self.validation()\n",
    "            if eval_loss < best_lost:\n",
    "                best_lost = eval_loss\n",
    "                best_model = copy.deepcopy(self.model)\n",
    "            if (epoch + 1) == epochs or (epoch + 1) in [\n",
    "                c + 1 for c in range(epochs) if c % int(epochs / 4) == 0\n",
    "            ]:\n",
    "                print(\n",
    "                    f\"Epoch: {epoch:2}/{epochs:2} - train_loss: {train_loss:.4f} - test_loss: {eval_loss:4f}\"\n",
    "                )\n",
    "        self.model = best_model\n",
    "        self.model.eval()\n",
    "        return None\n",
    "\n",
    "    def evaluate_model(\n",
    "        self,\n",
    "        np_data=None,\n",
    "        df_eval=None,\n",
    "        plot=True,\n",
    "        scaled=True,\n",
    "        city=None,\n",
    "        k_steps=None,\n",
    "        scaler=None,\n",
    "    ):\n",
    "        assert scaled, \"data must be scaled\"\n",
    "        self.model.eval()\n",
    "        tensor_data = torch.from_numpy(np_data)  # np_data = b·ªô d·ªØ li·ªáu ƒë∆∞a v√†o\n",
    "        rmse_list = []\n",
    "        mae_list = []\n",
    "        mape_list = []\n",
    "\n",
    "        y_predicted_list = []\n",
    "        y_true_list = []\n",
    "\n",
    "        df_eval_true_inverse = df_eval[-args.test_size :]\n",
    "        for k_steps in range(1, args.n_predicted_period_months + 1):\n",
    "            y_predicted = []\n",
    "            for index in range(tensor_data.size(0) - args.look_back):\n",
    "                X = tensor_data[index : index + args.look_back, [0, 1, 2]]\n",
    "\n",
    "                yhat = self.model.predict(X.unsqueeze(0))\n",
    "                yhat = yhat.squeeze()\n",
    "\n",
    "                y_predicted.append(yhat.detach().cpu().numpy()[k_steps - 1])\n",
    "\n",
    "            moving = args.n_predicted_period_months - k_steps\n",
    "            y_predicted = y_predicted[moving : args.test_size + moving]\n",
    "\n",
    "            df_eval_pred_inverse = df_eval[-args.test_size :]\n",
    "            df_eval_pred_inverse[args.labels] = y_predicted  # step 1\n",
    "            y_predicted = scaler.inverse_transform(df_eval_pred_inverse.iloc[:, :-1])[\n",
    "                :, [-1]\n",
    "            ].reshape(args.test_size)\n",
    "            y_true = scaler.inverse_transform(df_eval_true_inverse.iloc[:, :-1])[\n",
    "                :, [-1]\n",
    "            ].reshape(args.test_size)\n",
    "\n",
    "            if plot == True:\n",
    "                plt.plot(y_predicted, label=\"predicted\")\n",
    "                plt.plot(y_true, label=\"actual\")\n",
    "                plt.title(f\"k-steps = {k_steps}\")\n",
    "                plt.legend()\n",
    "                plt.show()\n",
    "\n",
    "            rmse = mean_squared_error(y_true, y_predicted, squared=False)\n",
    "            mae = mean_absolute_error(y_true, y_predicted)\n",
    "            mape = mean_absolute_percentage_error(y_true, y_predicted)\n",
    "\n",
    "            rmse_list.append(rmse)\n",
    "            mae_list.append(mae)\n",
    "            mape_list.append(mape)\n",
    "\n",
    "            y_predicted_list.append(y_predicted)\n",
    "            y_true_list.append(y_true)\n",
    "        print(\"üçîüçîüçîüçîy_true_listüçîüçîüçî\")\n",
    "        print(y_true_list)\n",
    "        return y_true_list, y_predicted_list, rmse_list, mae_list, mape_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective and Suggest Hyperparams of Darts Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(model_name, trial, city):\n",
    "    specific_data = pd.read_csv(\n",
    "        output_process + city + \"_train_preprocessed.csv\",\n",
    "        parse_dates=True,\n",
    "        index_col=None,\n",
    "        encoding=\"unicode_escape\",\n",
    "    )\n",
    "    scaler = joblib.load(output_process + city + \"_train_scalerMinMaxNorm.save\")  # ok\n",
    "    selected_features = getDataWithSelectedFeature(city, 6)\n",
    "    df_train, df_eval = split_data(\n",
    "        specific_data, args.look_back, args.n_predicted_period_months\n",
    "    )\n",
    "    train = df_train[selected_features + [args.labels]].iloc[:, :].to_numpy()\n",
    "    test = df_eval[selected_features + [args.labels]].iloc[:, :].to_numpy()\n",
    "\n",
    "    train_X, train_y = to_supervised(\n",
    "        train, d_out=args.n_predicted_period_months, d_in=args.look_back\n",
    "    )\n",
    "    test_X, test_y = to_supervised(\n",
    "        test, d_out=args.n_predicted_period_months, d_in=args.look_back\n",
    "    )\n",
    "\n",
    "    print(\"üç§üç§üç§üç§üç§üç§üç§üç§üç§üç§üç§\")\n",
    "    print(\"SHAPE train_y \", train_y.shape)\n",
    "    print(\"SHAPE train_X \", train_X.shape)\n",
    "    print(\"SHAPE test_y \", test_y.shape)\n",
    "    print(\"SHAPE test_X \", test_X.shape)\n",
    "\n",
    "    train_tensor = TensorDataset(torch.from_numpy(train_X), torch.from_numpy(train_y))\n",
    "    test_tensor = (torch.from_numpy(test_X), torch.from_numpy(test_y))\n",
    "\n",
    "    train_loader = DataLoader(train_tensor, batch_size=args.batch_size, shuffle=True)\n",
    "\n",
    "    print(\"üçâüçâüçâüçâüçâüçâüçâüçâüçâüçâüçâüçâüçâ\", model_name)\n",
    "    if model_name == \"CNN\":\n",
    "        # Define search parameters\n",
    "        args.epochs = trial.suggest_int(\"Epochs\", 100, 500, step=10)\n",
    "        learning_rate = trial.suggest_loguniform(\"Learning rate\", 1e-4, 1e-2)\n",
    "        num_filters = trial.suggest_categorical(\n",
    "            \"Num. filters\",\n",
    "            [\n",
    "                [64, 64, 64],\n",
    "                [100, 100, 100],\n",
    "                [128, 128, 128],\n",
    "                [16, 32, 64],\n",
    "                [32, 64, 128],\n",
    "            ],\n",
    "        )\n",
    "        dropout = trial.suggest_uniform(\"Dropout rate\", 0.01, 0.80)\n",
    "        # CNN model\n",
    "        trainer = Trainer(\n",
    "            model_type=\"CNN\",\n",
    "            learning_rate=learning_rate,\n",
    "            important_features=selected_features,\n",
    "            train_loader=train_loader,\n",
    "            test_tensor=test_tensor,\n",
    "            num_filters=num_filters,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "    elif model_name == \"LSTM\":\n",
    "        args.epochs = trial.suggest_int(\"Epochs\", 100, 500, step=10)\n",
    "        hidden_size = trial.suggest_int(\"Hidden size\", 128, 512, log=True)\n",
    "        n_layers = trial.suggest_int(\"n layers\", 2, 5)\n",
    "        learning_rate = trial.suggest_loguniform(\"Learning rate\", 1e-4, 1e-2)\n",
    "        trainer = Trainer(\n",
    "            model_type=\"LSTM\",\n",
    "            learning_rate=learning_rate,\n",
    "            important_features=selected_features,\n",
    "            train_loader=train_loader,\n",
    "            test_tensor=test_tensor,\n",
    "            n_layers=n_layers,\n",
    "            hidden_size=hidden_size,\n",
    "            look_back=args.look_back,\n",
    "        )\n",
    "    elif model_name == \"LSTM_ATT\":\n",
    "        args.epochs = trial.suggest_int(\"Epochs\", 100, 500, step=10)\n",
    "        hidden_size = trial.suggest_int(\"Hidden size\", 128, 512, log=True)\n",
    "        n_layers = trial.suggest_int(\"n layers\", 2, 10)\n",
    "        learning_rate = trial.suggest_loguniform(\"Learning rate\", 1e-4, 1e-2)\n",
    "        trainer = Trainer(\n",
    "            model_type=\"LSTM_ATT\",\n",
    "            learning_rate=learning_rate,\n",
    "            important_features=selected_features,\n",
    "            train_loader=train_loader,\n",
    "            test_tensor=test_tensor,\n",
    "            n_layers=n_layers,\n",
    "            hidden_size=hidden_size,\n",
    "            look_back=args.look_back,\n",
    "        )\n",
    "\n",
    "    elif model_name == \"TRANSFORMERS\":\n",
    "        n_layers = trial.suggest_int(\"n layers\", 2, 10)  # a\n",
    "        hidden_size = trial.suggest_int(\"Hidden size\", 128, 512, log=True)\n",
    "        learning_rate = trial.suggest_loguniform(\"Learning rate\", 1e-4, 1e-2)\n",
    "        dropout = trial.suggest_uniform(\"Dropout rate\", 0.01, 0.80)\n",
    "        args.epochs = trial.suggest_int(\"Epochs\", 100, 500, step=10)\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model_type=\"TRANSFORMERS\",\n",
    "            important_features=selected_features,\n",
    "            train_loader=train_loader,\n",
    "            test_tensor=test_tensor,\n",
    "            n_layers=n_layers,\n",
    "            hidden_size=hidden_size,\n",
    "            learning_rate=learning_rate,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "    # Train model\n",
    "    trainer.train(epochs=args.epochs)\n",
    "\n",
    "    # Evaluate model\n",
    "    y_true, y_pred, rmse_list, mae_list, mape_list = trainer.evaluate_model(\n",
    "        np_data=test, df_eval=df_eval, plot=True, scaled=True, city=city, scaler=scaler\n",
    "    )\n",
    "    # _, _, rmse, mae, = trainer.evaluate_model(np_data=test, plot=False, scaled=True, city=city, y_scaler=y_scaler)\n",
    "\n",
    "    return mean(mae_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concate_2_filter_str(listfilter=\"\"):\n",
    "    string_filter = \",\".join(str(e) for e in listfilter)\n",
    "    return string_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main run optimize and save\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# Main Cell for optimize\n",
    "#########################\n",
    "model_name_list = [\n",
    "    # \"CNN\",\n",
    "    \"LSTM\",\n",
    "    # \"LSTM_ATT\",\n",
    "    # \"TRANSFORMERS\",\n",
    "]\n",
    "\n",
    "dt_started = datetime.now()\n",
    "\n",
    "# Input param for Optimize Run\n",
    "ntry = args.ntry\n",
    "njob = args.njob\n",
    "\n",
    "\n",
    "# L∆∞u th√¥ng tin traceback study v√† error city trong qu√° tr√¨nh optimize\n",
    "l_study_city = {}\n",
    "l_errCity = []\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for model_name in model_name_list:\n",
    "        print(f\"üî•üî•üî•üî•üî•üî•üî•üî•üî•{model_name}\")\n",
    "        for city_index in range(len(cities)):\n",
    "            print(f\"üî•üî•üî•üî•üî•üî•üî•üî•{cities[city_index]}\")\n",
    "            # Use Tree-structured Parzen Estimator sampler to minimise RMSE\n",
    "            sampler = optuna.samplers.TPESampler()\n",
    "            study = optuna.create_study(sampler=sampler, direction=\"minimize\")\n",
    "\n",
    "            # truy·ªÅn multiple param v√†o trong bi·∫øn trial\n",
    "            obj_func = lambda trial: objective(model_name, trial, cities[city_index])\n",
    "\n",
    "            try:\n",
    "                # Optimise over 100 trials\n",
    "                study.optimize(obj_func, n_trials=ntry, n_jobs=njob)\n",
    "\n",
    "                # Print results\n",
    "                print(\"Study statistics for : \")\n",
    "                print(\"  Number of finished trials: \", len(study.trials))\n",
    "\n",
    "                print(\"Best trial of city: \", cities[city_index])\n",
    "                best_trial = study.best_trial\n",
    "                print(\"  Value: \", best_trial.value)\n",
    "                if model_name == \"CNN\":\n",
    "                    # l∆∞u best param v√†o trong bi·∫øn to√†n c·ª•c\n",
    "                    one_city_param = pd.DataFrame(\n",
    "                        {\n",
    "                            \"City\": [cities[city_index]],\n",
    "                            \"Alg_name\": \"CNN\",\n",
    "                            \"Best_value\": best_trial.value,\n",
    "                            \"n_try_opt\": ntry,\n",
    "                            \"n Feature\": args.n_features,  # d√πng chung cho t·∫•t c·∫£ c√°c model v√† algorithm\n",
    "                            \"Batch Size\": args.batch_size,  # d√πng chung cho t·∫•t c·∫£ c√°c model v√† algorithm\n",
    "                            \"Lookback Window\": args.look_back,  # d√πng chung cho t·∫•t c·∫£ c√°c t·ªânh c·ªßa CNN\n",
    "                            \"Epochs\": best_trial.params[\"Epochs\"],\n",
    "                            \"Hiden Size\": \"\",  # CNN kh√¥ng quan t√¢m\n",
    "                            \"n Layers\": \"\",  # CNN kh√¥ng quan t√¢m\n",
    "                            \"Learning rate\": best_trial.params[\"Learning rate\"],\n",
    "                            \"Num. filters\": concate_2_filter_str(\n",
    "                                best_trial.params[\"Num. filters\"]\n",
    "                            ),  # ƒëo·∫°n n√†y √©p ki·ªÉu kh√¥ng s·∫Ω th√†nh x 3 do c·∫•u tr√∫c []\n",
    "                            \"Dropout rate\": best_trial.params[\"Dropout rate\"],\n",
    "                        },\n",
    "                        index=[0],\n",
    "                    )\n",
    "                elif model_name == \"LSTM\":\n",
    "                    # l∆∞u best param v√†o trong bi·∫øn to√†n c·ª•c\n",
    "                    one_city_param = pd.DataFrame(\n",
    "                        {\n",
    "                            \"City\": [cities[city_index]],\n",
    "                            \"Alg_name\": \"LSTM\",\n",
    "                            \"Best_value\": best_trial.value,\n",
    "                            \"n_try_opt\": ntry,\n",
    "                            \"n Feature\": args.n_features,  # d√πng chung cho t·∫•t c·∫£ c√°c model v√† algorithm\n",
    "                            \"Batch Size\": args.batch_size,  # d√πng chung cho t·∫•t c·∫£ c√°c model v√† algorithm\n",
    "                            \"Lookback Window\": args.look_back,\n",
    "                            \"Epochs\": best_trial.params[\"Epochs\"],\n",
    "                            \"Hiden Size\": best_trial.params[\"Hidden size\"],\n",
    "                            \"n Layers\": best_trial.params[\"n layers\"],\n",
    "                            \"Learning rate\": best_trial.params[\"Learning rate\"],\n",
    "                            \"Num. filters\": \"\",  # ƒëo·∫°n n√†y √©p ki·ªÉu kh√¥ng s·∫Ω th√†nh x 3 do c·∫•u tr√∫c []\n",
    "                            \"Dropout rate\": \"\",\n",
    "                        },\n",
    "                        index=[0],\n",
    "                    )\n",
    "                elif model_name == \"LSTM_ATT\":\n",
    "                    one_city_param = pd.DataFrame(\n",
    "                        {\n",
    "                            \"City\": [cities[city_index]],\n",
    "                            \"Alg_name\": \"LSTM_ATT\",\n",
    "                            \"Best_value\": best_trial.value,\n",
    "                            \"n_try_opt\": ntry,\n",
    "                            \"n Feature\": args.n_features,  # d√πng chung cho t·∫•t c·∫£ c√°c model v√† algorithm\n",
    "                            \"Batch Size\": args.batch_size,  # d√πng chung cho t·∫•t c·∫£ c√°c model v√† algorithm\n",
    "                            \"Lookback Window\": args.look_back,\n",
    "                            \"Epochs\": best_trial.params[\"Epochs\"],\n",
    "                            \"Hiden Size\": best_trial.params[\"Hidden size\"],\n",
    "                            \"n Layers\": best_trial.params[\"n layers\"],\n",
    "                            \"Learning rate\": best_trial.params[\"Learning rate\"],\n",
    "                            \"Num. filters\": \"\",\n",
    "                            \"Dropout rate\": \"\",\n",
    "                        },\n",
    "                        index=[0],\n",
    "                    )\n",
    "                elif model_name == \"TRANSFORMERS\":\n",
    "                    one_city_param = pd.DataFrame(\n",
    "                        {\n",
    "                            \"City\": [cities[city_index]],\n",
    "                            \"Alg_name\": \"transformer\",\n",
    "                            \"Best_value\": best_trial.value,\n",
    "                            \"n_try_opt\": ntry,\n",
    "                            \"n Feature\": args.n_features,  # d√πng chung cho t·∫•t c·∫£ c√°c model v√† algorithm\n",
    "                            \"Batch Size\": args.batch_size,  # d√πng chung cho t·∫•t c·∫£ c√°c model v√† algorithm\n",
    "                            \"Lookback Window\": args.look_back,\n",
    "                            \"Epochs\": best_trial.params[\"Epochs\"],\n",
    "                            \"Hidden Size\": best_trial.params[\"Hidden size\"],\n",
    "                            \"n Layers\": best_trial.params[\"n layers\"],\n",
    "                            \"Learning rate\": best_trial.params[\"Learning rate\"],\n",
    "                            \"Num. filters\": \"\",  # Transformer kh√¥ng d√πng\n",
    "                            \"Dropout rate\": best_trial.params[\"Dropout rate\"],\n",
    "                        },\n",
    "                        index=[0],\n",
    "                    )\n",
    "                folder_path = f\"opt_results/opt_res_ml_26102023/{model_name}/\"\n",
    "                file_path = (\n",
    "                    folder_path\n",
    "                    + f\"261023_DF_opt_hyperparam_{model_name}_multi-nstep.xlsx\"\n",
    "                )\n",
    "                if os.path.isfile(file_path):\n",
    "                    with pd.ExcelWriter(\n",
    "                        file_path,\n",
    "                        mode=\"a\",\n",
    "                        engine=\"openpyxl\",\n",
    "                        if_sheet_exists=\"overlay\",\n",
    "                    ) as writer:\n",
    "                        one_city_param.to_excel(\n",
    "                            writer, header=None, startrow=city_index + 1, index=False\n",
    "                        )\n",
    "                else:\n",
    "                    if not (os.path.isdir(folder_path)):\n",
    "                        os.mkdir(folder_path)\n",
    "                    with pd.ExcelWriter(file_path, engine=\"openpyxl\") as writer:\n",
    "                        one_city_param.to_excel(\n",
    "                            writer, startrow=city_index, index=False\n",
    "                        )\n",
    "            except:  # c√≥ error th√¨ l∆∞u v√†o l_errCity ƒë·ªÉ check l·∫°i sau\n",
    "                l_errCity.append(cities[city_index])\n",
    "    # l∆∞u k·∫øt qu·∫£ v√†o file CNN\n",
    "#   best_param.to_excel(prj_path_opt+'cnn/diarrhoea_opt_hyperparam_cnn.xlsx')\n",
    "\n",
    "dt_ended = datetime.now()\n",
    "print(\"k·∫øt th√∫c study trong:\", round((dt_ended - dt_started).total_seconds() / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send_to_telegram(\"Ch·∫°y xong optimize r√πiii!!V√¥ check thuiii!!!\" )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
