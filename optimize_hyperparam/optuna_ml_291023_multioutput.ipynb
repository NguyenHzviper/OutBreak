{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/trinhtruc/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "import joblib\n",
    "from statistics import mean \n",
    "\n",
    "import torch\n",
    "\n",
    "from darts import TimeSeries\n",
    "from darts.models import  RandomForest, LinearRegressionModel, LightGBMModel, \\\n",
    "                        CatBoostModel, XGBModel,  BlockRNNModel, NBEATSModel, NHiTSModel, \\\n",
    "                        TCNModel, TFTModel\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import PoissonRegressor\n",
    "\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from darts.utils.likelihood_models import GaussianLikelihood\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prj_path = '../'\n",
    "data_path = prj_path + \"data/new_data/DH/squeezed/\"\n",
    "prj_path_opt= prj_path + \"optimize_hyperparam/opt_results/\"\n",
    "output_process = prj_path + \"data/new_data/DH/processed_data/\"\n",
    "output_featureselection = prj_path + \"data/new_data/DH/feature_selection/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cities = [\n",
    "        'An Giang', 'BR V≈©ng T√†u', 'B√¨nh Ph∆∞·ªõc', 'B√¨nh Thu·∫≠n', 'B√¨nh ƒê·ªãnh',\n",
    "        'B·∫°c Li√™u', 'B·∫Øc K·∫°n', 'B·∫Øc Giang', 'Cao B·∫±ng', 'C√† Mau',\n",
    "        'C·∫ßn Th∆°', 'Gia Lai', 'H√† Giang', 'H√† N·ªôi', 'H√† Tƒ©nh',\n",
    "        'H√≤a B√¨nh','H∆∞ng Y√™n', 'H·∫£i D∆∞∆°ng', 'H·∫£i Ph√≤ng', 'Kh√°nh H√≤a', 'Ki√™n Giang',\n",
    "        'Kon Tum', 'Lai Ch√¢u', 'Long An', 'L√†o Cai', 'L√¢m ƒê·ªìng',\n",
    "        'L·∫°ng S∆°n','Nam ƒê·ªãnh', 'Ngh·ªá An', 'Ninh B√¨nh', 'Ninh Thu·∫≠n',\n",
    "        'Ph√∫ Th·ªç', 'Ph√∫ Y√™n', 'Qu·∫£ng B√¨nh', 'Qu·∫£ng Nam', 'Qu·∫£ng Ng√£i',\n",
    "        'Qu·∫£ng Ninh', 'Qu·∫£ng Tr·ªã', 'S√≥c TrƒÉng', 'S∆°n La', 'TT Hu·∫ø',\n",
    "        'Thanh H√≥a', 'Th√°i B√¨nh', 'Th√°i Nguy√™n', 'Ti·ªÅn Giang', 'Tr√† Vinh',\n",
    "        'Tuy√™n Quang', 'T√¢y Ninh', 'Vƒ©nh Ph√∫c', 'Y√™n B√°i', 'ƒêi·ªán Bi√™n',\n",
    "        'ƒê√† N·∫µng', 'ƒê·∫Øk N√¥ng', 'ƒê·∫Øk L·∫Øk', 'ƒê·ªìng Th√°p'\n",
    "]\n",
    "cities = ['H√† N·ªôi','H·∫£i Ph√≤ng','Qu·∫£ng Ninh','Nam ƒê·ªãnh','Th√°i B√¨nh','Qu·∫£ng Nam','Qu·∫£ng Ng√£i', 'Ph√∫ Y√™n',\n",
    "          'Ninh Thu·∫≠n', 'B√¨nh Thu·∫≠n', 'T√¢y Ninh', 'B√¨nh Ph∆∞·ªõc', 'An Giang', 'Ti·ªÅn Giang','C·∫ßn Th∆°', 'Tr√† Vinh']\n",
    "cities = [ 'B√¨nh Ph∆∞·ªõc', 'An Giang','Qu·∫£ng Ninh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters as args using the Configuration class\n",
    "class Configuration():\n",
    "    def __init__(self):\n",
    "        # l·∫•y b·ªô test d√†i 36 th√°ng = 3 nƒÉm\n",
    "        self.test_size = 36\n",
    "        # l√† nh√¨n v√†o d·ªØ li·ªáu tr∆∞·ªõc 3 th√°ng v√† d·ª± ph√≥ng        \n",
    "        self.look_back = 3\n",
    "        # d·ª± ph√≥ng n-step trong 6 th√°ng\n",
    "        self.n_predicted_period_months = 6\n",
    "        self.n_features = 3\n",
    "        self.seed = 42\n",
    "        # m·ªói ph·∫ßn t·ª≠ x trong t·∫≠p suppervise c√≥ ƒë·ªô l·ªõn l√† 16 = 16 th√°ng\n",
    "        self.batch_size = 16\n",
    "        self.device = torch.device(\"cuda\")\n",
    "        self.epochs = 300\n",
    "        #others\n",
    "        self.labels = \"Dengue_fever_rates\"\n",
    "        # Input param for Optimize Run\n",
    "        self.ntry = 1\n",
    "        self.njob = 1\n",
    "\n",
    "args = Configuration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    import random\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    \n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "seed_everything(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supporting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def send_to_telegram(message):\n",
    "\n",
    "    apiToken = '5908735099:AAGVSLrW62aXPBP-GrMvxoVgMsuJxXJpP1Q'\n",
    "    chatID = '@ptn_announcement'\n",
    "    apiURL = f'https://api.telegram.org/bot{apiToken}/sendMessage'\n",
    "\n",
    "    try:\n",
    "        response = requests.post(apiURL, json={'chat_id': chatID, 'text': message})\n",
    "        print(response.text)\n",
    "    except Exception as e:\n",
    "        message_error = \"B·ªã l·ªói r√πi: \"+str(e)\n",
    "        response = requests.post(apiURL, json={'chat_id': chatID, 'text': message_error})\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict_all_city_data():\n",
    "  cities_data = {}  \n",
    "  for city in cities:\n",
    "    city_result = pd.read_excel(prj_path+'data/new_data/DH/squeezed/squeezed_'+city+'.xlsx')  \n",
    "    \"\"\"Get all data from all city in 1997 - 2016\"\"\" \n",
    "    city_result = city_result.loc[city_result['year_month'] < '2017-1-1'] \n",
    "    cities_data[city] = city_result\n",
    "  return cities_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data (pre-)processing functions\n",
    "# modification\n",
    "def get_city_data(city_name, dict_full_data):\n",
    "    \"\"\"Returns Dengue fever rate and climate data\"\"\" \n",
    "    city_data = dict_full_data[city_name].drop(columns=['Diarrhoea_cases','Diarrhoea_rates', 'province',\n",
    "                                                        'Influenza_rates','Influenza_cases',\n",
    "                                                        'Dengue_fever_cases', 'year', 'month'], \n",
    "                                                                  axis=1, \n",
    "                                                                  inplace=False)    \n",
    "    return city_data\n",
    "\n",
    "def convert_to_stationary(city_data):\n",
    "    \"\"\"Subtracts previous value for all cols except disease rates\"\"\"\n",
    "    for col_name in city_data.columns:\n",
    "        if col_name != 'Dengue_fever_rates':\n",
    "            try:\n",
    "                city_data[col_name] = city_data[col_name] - city_data[col_name].shift()\n",
    "            except:\n",
    "                print(col_name)\n",
    "    return city_data\n",
    "\n",
    "def impute_missing_value(city_data):\n",
    "    \"\"\"\n",
    "    Imputes 0 for first 12 months, \n",
    "    last year's value for months 12-24, \n",
    "    and minimum value of last two years for months 25+\n",
    "    \"\"\"\n",
    "    for col in city_data.columns:\n",
    "        for index in range(len(city_data[col])):\n",
    "            if np.isnan(city_data[col].iloc[index]):\n",
    "                if index < 12:\n",
    "                    city_data[col].iloc[index] = 0\n",
    "                elif index >= 12 and index <= 24:\n",
    "                    city_data[col].iloc[index] = city_data[col].iloc[index - 12]\n",
    "                else:\n",
    "                    city_data[col].iloc[index] = min(city_data[col].iloc[index - 12], city_data[col].iloc[index - 24])\n",
    "    return city_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_full_data(dict_full_data):\n",
    "    climate_and_disease_feats = ['Total_Evaporation',\n",
    "       'Total_Rainfall', 'Max_Daily_Rainfall', 'n_raining_days',\n",
    "       'Average_temperature', 'Max_Average_Temperature',\n",
    "       'Min_Average_Temperature', 'Max_Absolute_Temperature',\n",
    "       'Min_Absolute_Temperature', 'Average_Humidity', 'Min_Humidity',\n",
    "       'n_hours_sunshine', 'Dengue_fever_rates']\n",
    "    for city in cities:\n",
    "        city_data = get_city_data(city_name=city,dict_full_data = dict_full_data)\n",
    "        city_data_features = city_data[climate_and_disease_feats]\n",
    "        city_data_features = impute_missing_value(city_data_features)\n",
    "        city_data_features = convert_to_stationary(city_data_features)\n",
    "        city_data_features.dropna(inplace=True)\n",
    "        city_data_features.loc[:, \"year_month\"] = city_data[\"year_month\"]\n",
    "        dict_full_data[city] = city_data_features\n",
    "    return dict_full_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, look_back, n_nextstep = args.n_predicted_period_months):\n",
    "    \"\"\"Splits data into train and test sets based on args (Configuration class)\"\"\"\n",
    "    train = data[: -args.test_size]    \n",
    "    test = data[-args.test_size - look_back-(n_nextstep - 1): ]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_supervised(data,  d_out, d_in, features_list=[]):\n",
    "    \"\"\"\n",
    "    Frames time-series as supervised learning dataset.\n",
    "    \n",
    "    Args:\n",
    "      d_in: lookback window\n",
    "      d_out: number of predicted months\n",
    "      features_list: list of all features **where last col is the disease incidence**\n",
    "\n",
    "    Returns:\n",
    "      Numpy arrays of disease incidence (y) and other predictors (X)\n",
    "    \"\"\"\n",
    "    X, y = list(), list()\n",
    "    for index, _ in enumerate(data):\n",
    "        in_end = index + d_in\n",
    "        out_end = in_end + d_out\n",
    "        if out_end <= len(data):\n",
    "            if len(features_list) == 0 :\n",
    "                X.append(data[index: in_end, :-1])\n",
    "            else:\n",
    "                X.append(data[index: in_end, features_list])\n",
    "            y.append(data[out_end-1: out_end, -1])\n",
    "    return np.array(X), np.array(y).reshape(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(df_full, model_name, location, feature_list, labels, scaler, cfg):\n",
    "    \"\"\"\n",
    "    $df: pandas.DataFrame object containing data for training and testing model:\n",
    "    $model: darts model object\n",
    "    $feature_list: Names of the features used as model input\n",
    "    $label: the value the model will be trained to predict\n",
    "    $scaler: scaler object. Note: the scaler will be fitted on training data and applied to test data\n",
    "    $lags: how much to look back into the past to output prediction\n",
    "    $split_index: the point at which to divide train and test_data\n",
    "\n",
    "    \"\"\"\n",
    "    order, sorder, trend = cfg\n",
    "    trainlist = [x for x in df_full.Dengue_fever_rates]\n",
    "    nstep = args.n_predicted_period_months\n",
    "\n",
    "    predict_list = []\n",
    "    for i in range (args.test_size+(nstep-1)):\n",
    "        history = trainlist[:-args.test_size-(nstep-1-i)]\n",
    "        model = SARIMAX(history, \n",
    "                        order=order, \n",
    "                        seasonal_order=sorder, \n",
    "                        trend = trend,\n",
    "                        enforce_stationarity=False,\n",
    "                        enforce_invertibility=False)\n",
    "        model_fit = model.fit(disp=False)\n",
    "        yhat = model_fit.predict(len(trainlist), len(trainlist) + nstep - 1)\n",
    "        predict_list.append(yhat)\n",
    "        \n",
    "    y_pred_list = []\n",
    "    for step in range(nstep):\n",
    "        moving = nstep-1-step\n",
    "        y_pred_list.append([x[step] for x in predict_list][moving:args.test_size+moving])\n",
    "\n",
    "    df_eval_true_inverse = df_full[-args.test_size:]\n",
    "    y_true = scaler.inverse_transform(df_eval_true_inverse.iloc[:,:-1])[:,[-1]].reshape(args.test_size)\n",
    "\n",
    "    y_pred_inverse_list = []\n",
    "    for step in range(nstep):\n",
    "        df_eval_pred_inverse = df_full[-args.test_size:]\n",
    "        df_eval_pred_inverse[args.labels]= y_pred_list[step] #step 1\n",
    "        y_pred_inverse = scaler.inverse_transform(df_eval_pred_inverse.iloc[:,:-1])[:,[-1]].reshape(args.test_size)\n",
    "        y_pred_inverse_list.append(y_pred_inverse)\n",
    "\n",
    "    y_pred_inverse_list\n",
    "    df_compare_test_predict = pd.DataFrame({\n",
    "        'y_true':y_true,\n",
    "        'y_pred_1step':y_pred_inverse_list[0],\n",
    "        'y_pred_2step':y_pred_inverse_list[1],\n",
    "        'y_pred_3step':y_pred_inverse_list[2],\n",
    "        'y_pred_4step':y_pred_inverse_list[3],\n",
    "        'y_pred_5step':y_pred_inverse_list[4],\n",
    "        'y_pred_6step':y_pred_inverse_list[5],\n",
    "        })\n",
    "    df_compare_test_predict.plot()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    mse_nstep = []\n",
    "    mae_nstep = []\n",
    "    rmse_nstep = []\n",
    "    mape_nstep = []\n",
    "    for step in range(nstep):\n",
    "        mse_nstep.append(mean_squared_error(y_true, y_pred_inverse_list[step]))\n",
    "        mae_nstep.append(mean_absolute_error(y_true, y_pred_inverse_list[step]))\n",
    "        rmse_nstep.append(mse_nstep[step]**0.5)\n",
    "        mape_nstep.append(mean_absolute_percentage_error(y_true, y_pred_inverse_list[step]))\n",
    "    return model, y_true, y_pred_inverse_list, mse_nstep, mae_nstep, rmse_nstep, mape_nstep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_prediction_for_location(df_full, model_name, location, feature_list, \n",
    "                                                labels, scaler, cfg):\n",
    "    \"\"\"train and generate prediction for a province\n",
    "    df: DataFrame object containing features and label(s) for training model\n",
    "    localtion: location_name\n",
    "    feature_list: list of features used as model input,  must be among the column names of df\n",
    "    labels: the values model will be trained to predict\n",
    "    scaler: sklearn scaler object\n",
    "    lags: how long into the past to look back when making prediction\n",
    "    split_index: the point at which to divide data into the train and test subsets.\n",
    "    \"\"\"\n",
    "    model, y_true, y_pred_inverse_list, mse_nstep, mae_nstep, rmse_nstep, mape_nstep = train_and_evaluate(df_full, model_name, location, feature_list, labels, scaler, cfg)\n",
    "    \n",
    "    df_prediction = pd.DataFrame({\"Date\": df_full[\"year_month\"][-args.test_size:],\n",
    "                                  \"Observed\": y_true[-args.test_size:],\n",
    "                                  f\"{1}-month\": y_pred_inverse_list[0],\n",
    "                                  f\"{2}-month\": y_pred_inverse_list[1],\n",
    "                                  f\"{3}-month\": y_pred_inverse_list[2],\n",
    "                                  f\"{4}-month\": y_pred_inverse_list[3],\n",
    "                                  f\"{5}-month\": y_pred_inverse_list[4],\n",
    "                                  f\"{6}-month\": y_pred_inverse_list[5]})\n",
    "    \n",
    "    df_prediction[\"City\"] = location\n",
    "    for step in range(args.n_predicted_period_months):\n",
    "        df_prediction[f\"RMSE_{step+1}-month\"] = rmse_nstep[step]\n",
    "        df_prediction[f\"MAE_{step+1}-month\"] = mae_nstep[step]\n",
    "        df_prediction[f\"MAPE_{step+1}-month\"] = mape_nstep[step]\n",
    "        df_prediction[f\"MSE_{step+1}-month\"] = mse_nstep[step]\n",
    "    print(\"‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è\")\n",
    "    display(df_prediction.head(5))\n",
    "    print(mean(mae_nstep))\n",
    "    return mean(mae_nstep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataWithSelectedFeature(city, next_predicted_month):\n",
    "  selected_feature = []\n",
    "  df = pd.read_csv(output_featureselection+str(next_predicted_month)+\"step_feature_selection_3_most.csv\")\n",
    "  for row in range(len(df)):\n",
    "    if (df[\"City\"][row] == city):\n",
    "      selected_feature.append(df[\"1st_Feature\"][row])\n",
    "      selected_feature.append(df[\"2nd_Feature\"][row])\n",
    "      selected_feature.append(df[\"3rd_Feature\"][row])\n",
    "  return selected_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective and Suggest Hyperparams of Darts Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(model_name, trial, city):   \n",
    "    specific_data = pd.read_csv(output_process+city+'_train_preprocessed.csv', parse_dates=True, index_col= None, encoding = 'unicode_escape')\n",
    "    scaler = joblib.load(output_process+city+'_train_scalerMinMaxNorm.save') #ok\n",
    "    # df_train, df_valid = split_data(specific_data, args.look_back,nstep)\n",
    "\n",
    "    selected_features = getDataWithSelectedFeature(city, 6)\n",
    "\n",
    "    # lags_by_nstep = args.look_back + nstep - 1\n",
    "    # lags_past_covariates_by_nstep = [-lags_by_nstep+2,-lags_by_nstep+1,-lags_by_nstep] #M·∫£ng n√†y ch·ª©a ba gi√° tr·ªã t∆∞∆°ng ·ª©ng cho args.lookback 3\n",
    "    # is_dl_algo = 0\n",
    "    # is_sklearn_model = 0\n",
    "\n",
    "    if model_name == \"SARIMA\":\n",
    "      p = trial.suggest_int('p', 0, 5)\n",
    "      d = trial.suggest_int('d', 0, 1)\n",
    "      q = trial.suggest_int('q', 0, 5)\n",
    "      t = trial.suggest_categorical('t', ['n', 'c', 't', 'ct'])\n",
    "      P = trial.suggest_int('P', 0, 6)\n",
    "      D = trial.suggest_int('D', 0, 1)\n",
    "      Q = trial.suggest_int('Q', 0, 6)\n",
    "      m = trial.suggest_categorical('m', [6, 12])\n",
    "\n",
    "      cfg = [(p, d, q), (P, D, Q, m), t]\n",
    "      # model = SARIMAX(history, order=order, seasonal_order=sorder, trend=trend, enforce_stationarity=False,\n",
    "      #                 enforce_invertibility=False)\n",
    "    elif model_name == 'SARIMAX':\n",
    "      p = trial.suggest_int('p', 0, 5)\n",
    "      d = trial.suggest_int('d', 0, 1)\n",
    "      q = trial.suggest_int('q', 0, 5)\n",
    "      t = trial.suggest_categorical('t', ['n', 'c', 't', 'ct'])\n",
    "      P = trial.suggest_int('P', 0, 6)\n",
    "      D = trial.suggest_int('D', 0, 1)\n",
    "      Q = trial.suggest_int('Q', 0, 6)\n",
    "      m = trial.suggest_categorical('m', [6, 12])\n",
    "\n",
    "      cfg = [(p, d, q), (P, D, Q, m), t]\n",
    "    \n",
    "    mae_error = output_prediction_for_location(specific_data, model_name, location=city, feature_list=selected_features,\n",
    "                                                labels=args.labels, scaler=scaler,cfg = cfg)\n",
    "\n",
    "    return mae_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main run optimize and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-29 18:17:36,275] A new study created in memory with name: SARIMA\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è Model_name:  SARIMA\n",
      "‚≠êÔ∏è‚≠êÔ∏è City:  B√¨nh Ph∆∞·ªõc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-10-29 18:28:30,972] Trial 0 failed with parameters: {'p': 4, 'd': 1, 'q': 3, 't': 't', 'P': 2, 'D': 1, 'Q': 5, 'm': 12} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/trinhtruc/Library/Python/3.9/lib/python/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/var/folders/r_/lbw3rw192wl9sx9vtc1c2_xc0000gn/T/ipykernel_90573/3075932642.py\", line 35, in <lambda>\n",
      "    obj_func = lambda trial: objective(model_name, trial, cities[city_index])\n",
      "  File \"/var/folders/r_/lbw3rw192wl9sx9vtc1c2_xc0000gn/T/ipykernel_90573/3607572082.py\", line 38, in objective\n",
      "    mae_error = output_prediction_for_location(specific_data, model_name, location=city, feature_list=selected_features,\n",
      "  File \"/var/folders/r_/lbw3rw192wl9sx9vtc1c2_xc0000gn/T/ipykernel_90573/745625978.py\", line 12, in output_prediction_for_location\n",
      "    model, y_true, y_pred_inverse_list, mse_nstep, mae_nstep, rmse_nstep, mape_nstep = train_and_evaluate(df_full, model_name, location, feature_list, labels, scaler, cfg)\n",
      "  File \"/var/folders/r_/lbw3rw192wl9sx9vtc1c2_xc0000gn/T/ipykernel_90573/4098872063.py\", line 25, in train_and_evaluate\n",
      "    model_fit = model.fit(disp=False)\n",
      "  File \"/Users/trinhtruc/Library/Python/3.9/lib/python/site-packages/statsmodels/tsa/statespace/mlemodel.py\", line 704, in fit\n",
      "    mlefit = super(MLEModel, self).fit(start_params, method=method,\n",
      "  File \"/Users/trinhtruc/Library/Python/3.9/lib/python/site-packages/statsmodels/base/model.py\", line 566, in fit\n",
      "    xopt, retvals, optim_settings = optimizer._fit(f, score, start_params,\n",
      "  File \"/Users/trinhtruc/Library/Python/3.9/lib/python/site-packages/statsmodels/base/optimizer.py\", line 242, in _fit\n",
      "    xopt, retvals = func(objective, gradient, start_params, fargs, kwargs,\n",
      "  File \"/Users/trinhtruc/Library/Python/3.9/lib/python/site-packages/statsmodels/base/optimizer.py\", line 659, in _fit_lbfgs\n",
      "    retvals = optimize.fmin_l_bfgs_b(func, start_params, maxiter=maxiter,\n",
      "  File \"/Users/trinhtruc/Library/Python/3.9/lib/python/site-packages/scipy/optimize/_lbfgsb_py.py\", line 199, in fmin_l_bfgs_b\n",
      "    res = _minimize_lbfgsb(fun, x0, args=args, jac=jac, bounds=bounds,\n",
      "  File \"/Users/trinhtruc/Library/Python/3.9/lib/python/site-packages/scipy/optimize/_lbfgsb_py.py\", line 365, in _minimize_lbfgsb\n",
      "    f, g = func_and_grad(x)\n",
      "  File \"/Users/trinhtruc/Library/Python/3.9/lib/python/site-packages/scipy/optimize/_differentiable_functions.py\", line 286, in fun_and_grad\n",
      "    self._update_grad()\n",
      "  File \"/Users/trinhtruc/Library/Python/3.9/lib/python/site-packages/scipy/optimize/_differentiable_functions.py\", line 256, in _update_grad\n",
      "    self._update_grad_impl()\n",
      "  File \"/Users/trinhtruc/Library/Python/3.9/lib/python/site-packages/scipy/optimize/_differentiable_functions.py\", line 173, in update_grad\n",
      "    self.g = approx_derivative(fun_wrapped, self.x, f0=self.f,\n",
      "  File \"/Users/trinhtruc/Library/Python/3.9/lib/python/site-packages/scipy/optimize/_numdiff.py\", line 505, in approx_derivative\n",
      "    return _dense_difference(fun_wrapped, x0, f0, h,\n",
      "  File \"/Users/trinhtruc/Library/Python/3.9/lib/python/site-packages/scipy/optimize/_numdiff.py\", line 576, in _dense_difference\n",
      "    df = fun(x) - f0\n",
      "  File \"/Users/trinhtruc/Library/Python/3.9/lib/python/site-packages/scipy/optimize/_numdiff.py\", line 456, in fun_wrapped\n",
      "    f = np.atleast_1d(fun(x, *args, **kwargs))\n",
      "  File \"/Users/trinhtruc/Library/Python/3.9/lib/python/site-packages/scipy/optimize/_differentiable_functions.py\", line 137, in fun_wrapped\n",
      "    fx = fun(np.copy(x), *args)\n",
      "  File \"/Users/trinhtruc/Library/Python/3.9/lib/python/site-packages/statsmodels/base/model.py\", line 534, in f\n",
      "    return -self.loglike(params, *args) / nobs\n",
      "  File \"/Users/trinhtruc/Library/Python/3.9/lib/python/site-packages/statsmodels/tsa/statespace/mlemodel.py\", line 939, in loglike\n",
      "    loglike = self.ssm.loglike(complex_step=complex_step, **kwargs)\n",
      "  File \"/Users/trinhtruc/Library/Python/3.9/lib/python/site-packages/statsmodels/tsa/statespace/kalman_filter.py\", line 1001, in loglike\n",
      "    kfilter = self._filter(**kwargs)\n",
      "  File \"/Users/trinhtruc/Library/Python/3.9/lib/python/site-packages/statsmodels/tsa/statespace/kalman_filter.py\", line 924, in _filter\n",
      "    kfilter()\n",
      "KeyboardInterrupt\n",
      "[W 2023-10-29 18:28:31,320] Trial 0 failed with value None.\n",
      "[I 2023-10-29 18:28:31,321] A new study created in memory with name: SARIMA\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚≠êÔ∏è‚≠êÔ∏è City:  An Giang\n"
     ]
    }
   ],
   "source": [
    "#########################\n",
    "# Main cell for optimize ML algorithm\n",
    "#########################\n",
    "# Hai thu·∫≠t to√°n n√†y ch∆∞a ch·∫°y ƒëc nhe, n√™n ƒë·ª´ng truy·ªÅn v√¥ m·∫£ng ƒë·ªÉ n√≥ ch·∫°y nho√©!\n",
    "# \"PoissonRegressor\"\n",
    "# \"SVMRBF\"\n",
    "model_name_list = [\n",
    "    \"SARIMA\"\n",
    "    #  \"RandomForest\",\n",
    "    #  \"LinearRegressionModel\",\n",
    "    #  \"LightGBMModel\",\n",
    "    #  \"CatBoostModel\",\n",
    "    #  \"XGBModel\",\n",
    "    # \"SVMRBF\",\n",
    "    # \"PoissonRegressor\"\n",
    "    \n",
    "]\n",
    "\n",
    "\n",
    "# L∆∞u th√¥ng tin traceback study v√† error city trong qu√° tr√¨nh optimize\n",
    "l_study_city ={}\n",
    "l_errCity =[]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  for model_name in model_name_list: \n",
    "    print(\"‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è Model_name: \",model_name)\n",
    "    best_param = pd.DataFrame()\n",
    "    for city_index in range(len(cities)):\n",
    "      print(\"‚≠êÔ∏è‚≠êÔ∏è City: \",cities[city_index])\n",
    "      # Use Tree-structured Parzen Estimator sampler to minimise RMSE\n",
    "      sampler = optuna.samplers.TPESampler()\n",
    "      study = optuna.create_study(sampler=sampler, direction='minimize', study_name = model_name)\n",
    "      # truy·ªÅn multiple param v√†o trong bi·∫øn trial\n",
    "      obj_func = lambda trial: objective(model_name, trial, cities[city_index])\n",
    "      try:\n",
    "        # Optimise over 100 trials\n",
    "        study.optimize(obj_func, n_trials=args.ntry, n_jobs=args.njob)\n",
    "\n",
    "        # Print results\n",
    "        print(\"Study statistics for : \")\n",
    "        print(\"  Number of finished trials: \", len(study.trials))\n",
    "        print(\"Best trial of city: \",cities[city_index])\n",
    "\n",
    "        best_trial = study.best_trial\n",
    "        print(\"üåàüåàüåàüåàüåà\")\n",
    "        print(best_trial.params)\n",
    "        print(\"üçì:\",best_trial.params['p'])\n",
    "        print(\"üçì:\",best_trial.params['p'])\n",
    "        # l∆∞u best param v√†o trong bi·∫øn to√†n c·ª•c\n",
    "\n",
    "        if model_name == \"SARIMA\":\n",
    "            one_city_param = pd.DataFrame({\n",
    "                              'City':  [cities[city_index]],\n",
    "                              'Alg_name': 'SARIMA',\n",
    "                              'Best_value': best_trial.value,\n",
    "                              'n_try_opt': args.ntry,\n",
    "                              'p':best_trial.params['p'],\n",
    "                              'd':best_trial.params['d'],\n",
    "                              'q':best_trial.params['q'],\n",
    "                              't':best_trial.params['t'],\n",
    "                              'P':best_trial.params['P'],\n",
    "                              'D':best_trial.params['D'],\n",
    "                              'Q':best_trial.params['Q'],\n",
    "                              'm':best_trial.params['m']\n",
    "                          })\n",
    "            print(\"üî•üî•üî•üî•üî•üî•\")\n",
    "            display(one_city_param)\n",
    "        # elif model_name == 'XGBModel':\n",
    "        #   one_city_param = pd.DataFrame({\n",
    "        #                       'City':  cities[city_index],\n",
    "        #                       'Alg_name': 'XGBModel',\n",
    "        #                       'Best_value': best_trial.value,\n",
    "        #                       'n_try_opt': args.ntry,\n",
    "        #                       'lags' : lags_by_nstep,\n",
    "        #                       'lags_past_covariates': [lags_past_covariates_in_str],\n",
    "        #                       'output_chunk_length': 1,\n",
    "        #                       'random_state':best_trial.params['random_state'],\n",
    "        #                       'likelihood': best_trial.params['likelihood'],\n",
    "        #                       })\n",
    "        \n",
    "        folder_path = f'opt_results/opt_res_ml_26102023/{model_name}/'\n",
    "        file_path = folder_path+ f'261023_DF_opt_hyperparam_{model_name}_multi-nstep.xlsx'\n",
    "        if(os.path.isfile(file_path)):\n",
    "            print(\"üê∏\")\n",
    "            with pd.ExcelWriter(file_path,mode=\"a\",engine=\"openpyxl\",if_sheet_exists=\"overlay\") as writer:\n",
    "                one_city_param.to_excel(writer, header=None, startrow=city_index+1,index=False)\n",
    "        else:\n",
    "            print(\"üêπ\")\n",
    "            if(not (os.path.isdir(folder_path))):\n",
    "              os.mkdir(folder_path)\n",
    "            with pd.ExcelWriter(file_path,engine=\"openpyxl\") as writer:\n",
    "                one_city_param.to_excel(writer, startrow=city_index,index=False)\n",
    "      except Exception as e:# c√≥ error th√¨ l∆∞u v√†o l_errCity ƒë·ªÉ check l·∫°i sau \n",
    "        l_errCity.append(cities[city_index])\n",
    "        print(\"V√ÉI √í C√ì L·ªñI!!!\")\n",
    "        print(e)\n",
    "        # send_to_telegram(\"TEST ARIMA error!\" )\n",
    "        # send_to_telegram(f'T·ªânh b·ªã l·ªói trong qu√° tr√¨nh optimize b·∫±ng model {model_name}: {cities[city_index]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send_to_telegram(\"TEST ARIMA done!\" )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
